{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "259f05a9",
   "metadata": {},
   "source": [
    "# Notebook 1 \u2014 HyperPod + NeMo Capability Map (Executable)\n",
    "\n",
    "\n",
    "What to prove :\n",
    "- The Space can see and write to the hyperpod shared filessytem **FSx for Lustre**.\n",
    "- A HyperPod training pod can mount that same FSx path and write back to it.\n",
    "- The cluster has the add-ons we rely on (Training Operator + Task Governance/Kueue).\n",
    "- The **customised images** for this cluster is discoverable and used for jobs.\n",
    "\n",
    "## 0) Inputs, toggles, and fail-fast checks\n",
    "\n",
    "**Do this first:**\n",
    "- In SMUS, open the HyperPod connection panel.\n",
    "- Copy the **HyperPod cluster name** and the **project S3 bucket/path**.\n",
    "- Paste them into `HYPERPOD_CLUSTER_NAME` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec65ff78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:01.946148Z",
     "iopub.status.busy": "2025-12-31T19:24:01.945820Z",
     "iopub.status.idle": "2025-12-31T19:24:04.361139Z",
     "shell.execute_reply": "2025-12-31T19:24:04.360255Z",
     "shell.execute_reply.started": "2025-12-31T19:24:01.946128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq sagemaker-hyperpod==3.5.0 datasets fsspec==2023.12.2 transformers huggingface-hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc2edbd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:04.362262Z",
     "iopub.status.busy": "2025-12-31T19:24:04.362027Z",
     "iopub.status.idle": "2025-12-31T19:24:04.375257Z",
     "shell.execute_reply": "2025-12-31T19:24:04.374271Z",
     "shell.execute_reply.started": "2025-12-31T19:24:04.362237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "  HYPERPOD_CLUSTER_NAME = nemo-hyperpod-drdwirm8n64vqf\n",
      "Derived:\n",
      "  REPOS_DIR            = /home/sagemaker-user/smus-repos\n",
      "Toggles:\n",
      "{'RUN_VALIDATE_ENV': True, 'RUN_FSX_CHECKS': False, 'RUN_KUEUE_TOPOLOGY': True, 'RUN_KUBECTL_CONTEXT': True}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "def require(cond, msg):\n",
    "    if not cond:\n",
    "        raise ValueError(msg)\n",
    "\n",
    "AWS_ACCOUNT_ID_RE = re.compile(r'\\b\\d{12}\\b')\n",
    "AWS_ARN_RE = re.compile(r'arn:aws:[^\\s\\\"\\']+')\n",
    "\n",
    "HF_ACCESS_TOKEN_RE = re.compile(r'(recipes\\.run\\.hf_access_token=)(\\S+)')\n",
    "HF_ENV_TOKEN_RE = re.compile(r'(\\+env_vars\\.HF_TOKEN=)(\\S+)')\n",
    "HF_BARE_TOKEN_RE = re.compile(r'(\\bHF_TOKEN=)(\\S+)')\n",
    "\n",
    "def rdct_sens(value):\n",
    "    if value is None:\n",
    "        return value\n",
    "    text = rdct_aws(value)\n",
    "    text = HF_ACCESS_TOKEN_RE.sub(r'\\1<HF_TOKEN>', text)\n",
    "    text = HF_ENV_TOKEN_RE.sub(r'\\1<HF_TOKEN>', text)\n",
    "    text = HF_BARE_TOKEN_RE.sub(r'\\1<HF_TOKEN>', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def rdct_aws(value):\n",
    "    if value is None:\n",
    "        return value\n",
    "    text = str(value)\n",
    "    text = AWS_ARN_RE.sub('<ARN>', text)\n",
    "    text = AWS_ACCOUNT_ID_RE.sub('<ACCOUNT_ID>', text)\n",
    "    return text\n",
    "\n",
    "import boto3\n",
    "# --- USER INPUTS (paste from HyperPod connection panel) ---\n",
    "REGION = ''  # paste from HyperPod connection panel (optional; auto-detect if empty)\n",
    "HYPERPOD_CLUSTER_NAME = os.environ.get('HYPERPOD_CLUSTER_NAME', 'nemo-hyperpod-drdwirm8n64vqf').strip()  # e.g., nemo-hyperpod-xxxx\n",
    "\n",
    "# --- REGION AUTO-DETECT (if empty) ---\n",
    "if not REGION:\n",
    "    REGION = (os.environ.get('AWS_REGION') or os.environ.get('AWS_DEFAULT_REGION') or '').strip()\n",
    "if not REGION:\n",
    "    try:\n",
    "        REGION = boto3.Session().region_name or ''\n",
    "    except Exception:\n",
    "        REGION = ''\n",
    "\n",
    "# --- DERIVED DEFAULTS ---\n",
    "RUN_ID = datetime.utcnow().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:6]\n",
    "REPOS_DIR = str(Path.home() / 'smus-repos')\n",
    "\n",
    "# --- SECTION TOGGLES (set False to skip) ---\n",
    "RUN_VALIDATE_ENV = True\n",
    "RUN_FSX_CHECKS = False\n",
    "RUN_KUEUE_TOPOLOGY = True\n",
    "RUN_KUBECTL_CONTEXT = True\n",
    "\n",
    "# --- FAIL-FAST VALIDATION ---\n",
    "require(REGION.strip(), 'Missing REGION. Paste it from HyperPod connection panel or set AWS_REGION.')\n",
    "require(HYPERPOD_CLUSTER_NAME.strip(), 'Missing HYPERPOD_CLUSTER_NAME. Paste it from HyperPod connection panel.')\n",
    "\n",
    "print('Inputs:')\n",
    "print(f'  HYPERPOD_CLUSTER_NAME = {rdct_aws(HYPERPOD_CLUSTER_NAME)}')\n",
    "print('Derived:')\n",
    "\n",
    "print(f'  REPOS_DIR            = {REPOS_DIR}')\n",
    "print('Toggles:')\n",
    "print({\n",
    "    'RUN_VALIDATE_ENV': RUN_VALIDATE_ENV,\n",
    "    'RUN_FSX_CHECKS': RUN_FSX_CHECKS,\n",
    "    'RUN_KUEUE_TOPOLOGY': RUN_KUEUE_TOPOLOGY,\n",
    "    'RUN_KUBECTL_CONTEXT': RUN_KUBECTL_CONTEXT,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a385a96",
   "metadata": {},
   "source": [
    "## 1) Base imports + notebook utilities\n",
    "\n",
    "Before we touch the cluster, we set up a few tiny helpers used throughout the notebook:\n",
    "\n",
    "- `print_header(...)` makes logs readable when you scroll.\n",
    "- `run(...)` executes shell commands and surfaces stdout/stderr on failure.\n",
    "- `ensure_dir(...)` creates local folders (for repo clones, temp files, etc.).\n",
    "\n",
    "Just some plumbing for the rest of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c220481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:04.377663Z",
     "iopub.status.busy": "2025-12-31T19:24:04.377441Z",
     "iopub.status.idle": "2025-12-31T19:24:04.395645Z",
     "shell.execute_reply": "2025-12-31T19:24:04.394655Z",
     "shell.execute_reply.started": "2025-12-31T19:24:04.377637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment summary\n",
      "===================\n",
      "cwd: /home/sagemaker-user/bobber/notebooks\n",
      "user: sagemaker-user\n",
      "python: Python 3.11.11\n",
      "REPOS_DIR ready: /home/sagemaker-user/smus-repos\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import textwrap\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def print_header(title: str):\n",
    "    bar = '=' * len(title)\n",
    "    print(f'\\n{title}\\n{bar}')\n",
    "\n",
    "def run(cmd, check=True, capture=True):\n",
    "    \"\"\"Run a shell command and return (returncode, stdout).\"\"\"\n",
    "    result = subprocess.run(\n",
    "        cmd,\n",
    "        shell=True,\n",
    "        check=False,\n",
    "        text=True,\n",
    "        stdout=subprocess.PIPE if capture else None,\n",
    "        stderr=subprocess.STDOUT if capture else None,\n",
    "    )\n",
    "    if check and result.returncode != 0:\n",
    "        raise RuntimeError(f'Command failed ({result.returncode}): {cmd}\\n{result.stdout}')\n",
    "    return result.returncode, (result.stdout or '').strip()\n",
    "\n",
    "def get_hyp_job_status(job_name, namespace):\n",
    "    if not job_name:\n",
    "        return None, []\n",
    "    cmd = f\"kubectl get hyp-pytorch-job {job_name} -n {namespace} -o json\"\n",
    "    rc, out = run(cmd, check=False)\n",
    "    if rc != 0 or not out:\n",
    "        return None, []\n",
    "    try:\n",
    "        data = json.loads(out)\n",
    "    except Exception:\n",
    "        return None, []\n",
    "    status = data.get('status', {}) or {}\n",
    "    phase = status.get('phase') or status.get('jobStatus') or status.get('status')\n",
    "    conds = [(c.get('type'), c.get('status')) for c in status.get('conditions', []) or []]\n",
    "    return phase, conds\n",
    "\n",
    "def _parse_hyp_list_line(line, namespace):\n",
    "    if not line or not line.strip():\n",
    "        return None\n",
    "    if line.strip().startswith('---') or line.strip().startswith('NAME'):\n",
    "        return None\n",
    "    if namespace and namespace in line:\n",
    "        idx = line.find(namespace)\n",
    "        name = line[:idx].strip()\n",
    "        rest = line[idx + len(namespace):].strip()\n",
    "        tokens = rest.split()\n",
    "        status = tokens[0] if tokens else None\n",
    "        age = tokens[1] if len(tokens) > 1 else None\n",
    "        return name, status, age, line\n",
    "    parts = line.split()\n",
    "    if not parts:\n",
    "        return None\n",
    "    name = parts[0]\n",
    "    status = None\n",
    "    age = None\n",
    "    if namespace and len(parts) > 2 and parts[1] == namespace:\n",
    "        status = parts[2]\n",
    "        age = parts[3] if len(parts) > 3 else None\n",
    "    else:\n",
    "        status = parts[2] if len(parts) > 2 else (parts[1] if len(parts) > 1 else None)\n",
    "        age = parts[-1] if len(parts) > 1 else None\n",
    "    return name, status, age, line\n",
    "\n",
    "def get_hyp_list_status(job_name, namespace):\n",
    "    if not job_name:\n",
    "        return None, None, None\n",
    "    cmd = f\"hyp list hyp-pytorch-job -n {namespace}\"\n",
    "    try:\n",
    "        result = subprocess.run(['bash', '-lc', cmd], text=True, capture_output=True)\n",
    "    except Exception:\n",
    "        return None, None, None\n",
    "    out = result.stdout or ''\n",
    "    if result.returncode != 0 or not out:\n",
    "        return None, None, None\n",
    "    for line in out.splitlines():\n",
    "        parsed = _parse_hyp_list_line(line, namespace)\n",
    "        if not parsed:\n",
    "            continue\n",
    "        name, status, age, raw = parsed\n",
    "        if name == job_name:\n",
    "            return status, age, raw\n",
    "    return None, None, None\n",
    "def ensure_dir(path: str | Path):\n",
    "    p = Path(path)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "print_header('Environment summary')\n",
    "print('cwd:', os.getcwd())\n",
    "print('user:', os.environ.get('USER', 'unknown'))\n",
    "print('python:', run('python --version')[1])\n",
    "\n",
    "# Ensure local repo workspace exists\n",
    "try:\n",
    "    REPOS_DIR\n",
    "except NameError:\n",
    "    REPOS_DIR = str(Path.home() / 'smus-repos')\n",
    "    print(f'Repos dir not set yet; defaulting to {REPOS_DIR}')\n",
    "\n",
    "ensure_dir(REPOS_DIR)\n",
    "print('REPOS_DIR ready:', REPOS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebe065c",
   "metadata": {},
   "source": [
    "## 2) FSx for Lustre detection (shared storage: Space path <-> pod path)\n",
    "\n",
    "HyperPod needs a **shared, POSIX filesystem** so *every* training pod can **read the same data** and **write to the same checkpoint/artifact directory** across nodes (critical for restarts/resume). ([AWS Documentation][1])\n",
    "This is especially important for **NeMo/Megatron distributed checkpointing**: NeMo\u2019s *Fully Parallel Saving* has each data-parallel rank write its checkpoint shard **independently to shared storage** (many concurrent writers). ([NVIDIA Docs][2])\n",
    "\n",
    "We use **FSx for Lustre** because it\u2019s a fully managed, **POSIX-compliant parallel filesystem** optimized for low-latency / high-throughput GPU workloads, and it integrates with SageMaker HyperPod (and can link to S3 as a data repository). ([Amazon Web Services, Inc.][3])\n",
    "\n",
    "In this project, we also mount the same FSx into the **Studio Space**, so the *same* filesystem shows up under different paths:\n",
    "\n",
    "* **Space:** `~/custom-file-systems/fsx_lustre/<fs-id>` (or `/mnt/...`)\n",
    "* **HyperPod pods:** a fixed mount like `/fsx/...` (often via a PVC) with `subPath=<fs-id>`\n",
    "\n",
    "This cell detects the FSx mount **as the Space sees it**, then derives the values we plug into pod specs:\n",
    "`FSX_SPACE_ROOT`, `FSX_SPACE_REAL`, `FSX_FILE_SYSTEM_ID (fs-...)`, `FSX_POD_PREFIX`, `FSX_PVC_SUBPATH`.\n",
    "\n",
    "**Success looks like:** at least one Lustre mount is found, the resolved FSx path exists, and the derived ID starts with `fs-`.\n",
    "\n",
    "[1]: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-lifecycle-best-practices-slurm-slurm-setup-with-fsx.html \"Mounting Amazon FSx for Lustre to a HyperPod cluster - Amazon SageMaker AI\"\n",
    "[2]: https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/checkpoints/dist_ckpt.html \"NeMo Distributed Checkpoint User Guide \u2014 NVIDIA NeMo Framework User Guide\"\n",
    "[3]: https://aws.amazon.com/fsx/lustre/ \"Amazon FSx for Lustre | Cloud File Storage Integrated with S3 | AWS\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60755b65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:04.398181Z",
     "iopub.status.busy": "2025-12-31T19:24:04.397910Z",
     "iopub.status.idle": "2025-12-31T19:24:04.413208Z",
     "shell.execute_reply": "2025-12-31T19:24:04.412475Z",
     "shell.execute_reply.started": "2025-12-31T19:24:04.398162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lustre mounts (from /proc/mounts):\n",
      "[\n",
      "  {\n",
      "    \"source\": \"10.38.246.180@tcp:/gu7rfamv/fs-03b1953d09801303c\",\n",
      "    \"mount\": \"/mnt/custom-file-systems/fsx_lustre/fs-03b1953d09801303c\",\n",
      "    \"fstype\": \"lustre\"\n",
      "  }\n",
      "]\n",
      "FSx Lustre candidates (as seen in the Space):\n",
      "/home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c\n",
      "/mnt/custom-file-systems/fsx_lustre/fs-03b1953d09801303c\n",
      "FSx path (Space-visible): /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c\n",
      "FSx path (resolved):      /mnt/custom-file-systems/fsx_lustre/fs-03b1953d09801303c\n",
      "FSX_FILE_SYSTEM_ID=fs-03b1953d09801303c\n",
      "FSX_FILE_SYSTEM_PATH=/fs-03b1953d09801303c\n",
      "FSX_POD_PREFIX=/fsx\n",
      "FSX_PVC_SUBPATH=fs-03b1953d09801303c\n",
      "FSX_BASE_DIR (intended): /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke\n",
      "Writable now? True (root), True (parent)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def detect_lustre_mounts():\n",
    "    mounts = []\n",
    "    with open('/proc/mounts', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            src, mnt, fstype = parts[:3]\n",
    "            if fstype.lower() == 'lustre':\n",
    "                mounts.append({\"source\": src, \"mount\": mnt, \"fstype\": fstype})\n",
    "    return mounts\n",
    "\n",
    "def find_fsx_lustre_space_paths():\n",
    "    bases = [\n",
    "        Path.home() / 'custom-file-systems' / 'fsx_lustre',\n",
    "        Path('/home/sagemaker-user/custom-file-systems/fsx_lustre'),\n",
    "        Path('/mnt/custom-file-systems/fsx_lustre'),\n",
    "    ]\n",
    "    candidates = []\n",
    "    for base in bases:\n",
    "        if not base.exists():\n",
    "            continue\n",
    "        for p in base.iterdir():\n",
    "            if p.is_dir() and re.match(r'^fs-[0-9a-f]+$', p.name):\n",
    "                candidates.append(p)\n",
    "    # Deduplicate while keeping order\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for p in candidates:\n",
    "        k = str(p)\n",
    "        if k in seen:\n",
    "            continue\n",
    "        seen.add(k)\n",
    "        uniq.append(p)\n",
    "    return uniq\n",
    "\n",
    "lustre_mounts = detect_lustre_mounts()\n",
    "print('Lustre mounts (from /proc/mounts):')\n",
    "print(json.dumps(lustre_mounts, indent=2) if lustre_mounts else '(none)')\n",
    "\n",
    "fsx_candidates = find_fsx_lustre_space_paths()\n",
    "print('FSx Lustre candidates (as seen in the Space):')\n",
    "print('\\n'.join(str(p) for p in fsx_candidates) if fsx_candidates else '(none)')\n",
    "\n",
    "FSX_ID_HINT = (os.environ.get('FSX_ID') or os.environ.get('FSX_FILE_SYSTEM_ID') or '').strip()\n",
    "chosen_space_path = None\n",
    "if FSX_ID_HINT:\n",
    "    for p in fsx_candidates:\n",
    "        if p.name == FSX_ID_HINT:\n",
    "            chosen_space_path = p\n",
    "            break\n",
    "if not chosen_space_path and fsx_candidates:\n",
    "    home_base = str(Path.home())\n",
    "    chosen_space_path = next((p for p in fsx_candidates if str(p).startswith(home_base)), fsx_candidates[0])\n",
    "\n",
    "if not chosen_space_path:\n",
    "    require(lustre_mounts, 'No Lustre mounts detected and no FSx custom-file-systems path found. Is FSx attached?')\n",
    "    chosen = next((m for m in lustre_mounts if 'fsx' in (m['source'].lower() + ' ' + m['mount'].lower())), lustre_mounts[0])\n",
    "    FSX_SPACE_ROOT = Path(chosen['mount']).resolve()\n",
    "    FSX_SPACE_REAL = FSX_SPACE_ROOT\n",
    "else:\n",
    "    FSX_SPACE_ROOT = chosen_space_path\n",
    "    FSX_SPACE_REAL = Path(os.path.realpath(FSX_SPACE_ROOT)).resolve()\n",
    "\n",
    "require(FSX_SPACE_ROOT.exists(), f'FSx Space path does not exist: {FSX_SPACE_ROOT}')\n",
    "print(f'FSx path (Space-visible): {FSX_SPACE_ROOT}')\n",
    "print(f'FSx path (resolved):      {FSX_SPACE_REAL}')\n",
    "\n",
    "FSX_FILE_SYSTEM_ID = FSX_SPACE_ROOT.name if FSX_SPACE_ROOT.name.startswith('fs-') else ''\n",
    "if not FSX_FILE_SYSTEM_ID and chosen_space_path:\n",
    "    FSX_FILE_SYSTEM_ID = chosen_space_path.name\n",
    "print(f\"FSX_FILE_SYSTEM_ID={FSX_FILE_SYSTEM_ID or '(unknown)'}\")\n",
    "\n",
    "FSX_FILE_SYSTEM_PATH = os.environ.get('FSX_FILE_SYSTEM_PATH', '').strip() or f'/{FSX_FILE_SYSTEM_ID}'\n",
    "if FSX_FILE_SYSTEM_PATH and not FSX_FILE_SYSTEM_PATH.startswith('/'):\n",
    "    FSX_FILE_SYSTEM_PATH = f'/{FSX_FILE_SYSTEM_PATH}'\n",
    "print(f'FSX_FILE_SYSTEM_PATH={FSX_FILE_SYSTEM_PATH}')\n",
    "\n",
    "FSX_POD_PREFIX = '/fsx'\n",
    "FSX_PVC_SUBPATH = os.environ.get('FSX_PVC_SUBPATH', '').strip() or FSX_FILE_SYSTEM_PATH.lstrip('/')\n",
    "require(FSX_PVC_SUBPATH, 'Could not determine FSX_PVC_SUBPATH; set env var FSX_PVC_SUBPATH')\n",
    "print(f'FSX_POD_PREFIX={FSX_POD_PREFIX}')\n",
    "print(f'FSX_PVC_SUBPATH={FSX_PVC_SUBPATH}')\n",
    "\n",
    "# Ensure /fsx/<subpath> exists in the Space for local staging (pods still use /fsx)\n",
    "pod_fsx_root = Path(FSX_POD_PREFIX) / FSX_PVC_SUBPATH\n",
    "if not pod_fsx_root.exists():\n",
    "    try:\n",
    "        import subprocess\n",
    "        subprocess.run(['sudo', 'mkdir', '-p', FSX_POD_PREFIX], check=True)\n",
    "        subprocess.run(['sudo', 'ln', '-s', str(FSX_SPACE_REAL), str(pod_fsx_root)], check=True)\n",
    "    except Exception as e:\n",
    "        print(f'WARNING: could not create local alias {pod_fsx_root} -> {FSX_SPACE_REAL}: {e}')\n",
    "require(pod_fsx_root.exists(), f'Local pod FSx path not available: {pod_fsx_root}. You may need to create a /fsx symlink with sudo.')\n",
    "\n",
    "FSX_BASE_DIRNAME = 'smus-nemo-smoke'\n",
    "FSX_BASE_DIR = FSX_SPACE_ROOT / FSX_BASE_DIRNAME\n",
    "FSX_RUN_DIR = None\n",
    "\n",
    "print(f'FSX_BASE_DIR (intended): {FSX_BASE_DIR}')\n",
    "print(f'Writable now? {os.access(FSX_SPACE_ROOT, os.W_OK)} (root), {os.access(FSX_BASE_DIR.parent, os.W_OK)} (parent)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b262b",
   "metadata": {},
   "source": [
    "## 3) Custom container discovery (from SSM)\n",
    "\n",
    "In this project we build a LLMFT llama recipe based training image and publish it to **ECR** (for example, adding EFA + AWS-OFI-NCCL bits for high-performance multi-node training).\n",
    "\n",
    "We store the current image URI in SSM at:\n",
    "\n",
    "`/sagemaker/hyperpod/<cluster-name>/llmft-container-uri`\n",
    "\n",
    "This cell looks up that value and sets `LLMFT_IMAGE_CUSTOM`, which is the image we pass into `HyperPodPytorchJob` specs.\n",
    "\n",
    "**Override option:** if you\u2019re testing a custom image, set the `LLMFT_IMAGE_CUSTOM` environment variable and this section will use it instead. Dockerfile for image is in llmft-container-pipeline/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37087a35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:04.416049Z",
     "iopub.status.busy": "2025-12-31T19:24:04.415827Z",
     "iopub.status.idle": "2025-12-31T19:24:04.735248Z",
     "shell.execute_reply": "2025-12-31T19:24:04.734466Z",
     "shell.execute_reply.started": "2025-12-31T19:24:04.416021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found NeMo container SSM parameters:\n",
      "/sagemaker/hyperpod/nemo-hyperpod-drdwirm8n64vqf/nemo-container-uri\n",
      "Using NeMo image: <ACCOUNT_ID>.dkr.ecr.us-east-1.amazonaws.com/nemo-framework-hyperpod:25.04-eks\n",
      "NEMO_IMAGE_SOURCE (SSM): /sagemaker/hyperpod/nemo-hyperpod-drdwirm8n64vqf/nemo-container-uri\n",
      "Found LLMFT container SSM parameters:\n",
      "/sagemaker/hyperpod/nemo-hyperpod-drdwirm8n64vqf/llmft-container-uri\n",
      "Using LLMFT image: <ACCOUNT_ID>.dkr.ecr.us-east-1.amazonaws.com/hyperpod-recipes-llmft-custom:llmft-v1.0.0-llama-custom\n",
      "LLMFT_IMAGE_CUSTOM_SOURCE (SSM): /sagemaker/hyperpod/nemo-hyperpod-drdwirm8n64vqf/llmft-container-uri\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# --- Discover the NeMo container URI from SSM (best-effort) ---\n",
    "# Stored at: /sagemaker/hyperpod/<cluster-name>/nemo-container-uri\n",
    "ssm = boto3.client('ssm', region_name=REGION)\n",
    "\n",
    "nemo_param_names = []\n",
    "paginator = ssm.get_paginator('get_parameters_by_path')\n",
    "for page in paginator.paginate(Path='/sagemaker/hyperpod/', Recursive=True, WithDecryption=False):\n",
    "    for p in page.get('Parameters', []):\n",
    "        name = p.get('Name', '')\n",
    "        if name.endswith('/nemo-container-uri'):\n",
    "            nemo_param_names.append(name)\n",
    "\n",
    "nemo_param_names = sorted(set(nemo_param_names))\n",
    "print('Found NeMo container SSM parameters:')\n",
    "print('\\n'.join(nemo_param_names) if nemo_param_names else '(none found under /sagemaker/hyperpod/)')\n",
    "\n",
    "NEMO_IMAGE = os.environ.get('NEMO_IMAGE', '').strip()\n",
    "NEMO_IMAGE_SOURCE = ''\n",
    "\n",
    "if not NEMO_IMAGE:\n",
    "    if len(nemo_param_names) == 1:\n",
    "        NEMO_IMAGE_SOURCE = nemo_param_names[0]\n",
    "        NEMO_IMAGE = ssm.get_parameter(Name=NEMO_IMAGE_SOURCE)['Parameter']['Value'].strip()\n",
    "    elif len(nemo_param_names) > 1:\n",
    "        raise ValueError(\n",
    "            'Multiple nemo-container-uri parameters found. Set NEMO_IMAGE env var or reduce to one. '\n",
    "            + str(nemo_param_names)\n",
    "        )\n",
    "\n",
    "require(NEMO_IMAGE, 'Could not determine NeMo container image. Set env var NEMO_IMAGE or ensure SSM has */nemo-container-uri.')\n",
    "print(f'Using NeMo image: {rdct_aws(NEMO_IMAGE)}')\n",
    "if NEMO_IMAGE_SOURCE:\n",
    "    print(f'NEMO_IMAGE_SOURCE (SSM): {NEMO_IMAGE_SOURCE}')\n",
    "\n",
    "# --- Discover the LLMFT custom container URI from SSM (best-effort) ---\n",
    "# Stored at: /sagemaker/hyperpod/<cluster-name>/llmft-container-uri\n",
    "\n",
    "llmft_param_names = []\n",
    "paginator = ssm.get_paginator('get_parameters_by_path')\n",
    "for page in paginator.paginate(Path='/sagemaker/hyperpod/', Recursive=True, WithDecryption=False):\n",
    "    for p in page.get('Parameters', []):\n",
    "        name = p.get('Name', '')\n",
    "        if name.endswith('/llmft-container-uri'):\n",
    "            llmft_param_names.append(name)\n",
    "\n",
    "llmft_param_names = sorted(set(llmft_param_names))\n",
    "print('Found LLMFT container SSM parameters:')\n",
    "print('\\n'.join(llmft_param_names) if llmft_param_names else '(none found under /sagemaker/hyperpod/)')\n",
    "\n",
    "LLMFT_IMAGE_CUSTOM = os.environ.get('LLMFT_IMAGE_CUSTOM', '').strip()\n",
    "LLMFT_IMAGE_CUSTOM_SOURCE = ''\n",
    "\n",
    "if not LLMFT_IMAGE_CUSTOM:\n",
    "    if len(llmft_param_names) == 1:\n",
    "        LLMFT_IMAGE_CUSTOM_SOURCE = llmft_param_names[0]\n",
    "        LLMFT_IMAGE_CUSTOM = ssm.get_parameter(Name=LLMFT_IMAGE_CUSTOM_SOURCE)['Parameter']['Value'].strip()\n",
    "    elif len(llmft_param_names) > 1:\n",
    "        raise ValueError(\n",
    "            'Multiple llmft-container-uri parameters found. Set LLMFT_IMAGE_CUSTOM env var or reduce to one. '\n",
    "            + str(llmft_param_names)\n",
    "        )\n",
    "\n",
    "require(LLMFT_IMAGE_CUSTOM, 'Could not determine LLMFT container image. Set env var LLMFT_IMAGE_CUSTOM or ensure SSM has */llmft-container-uri.')\n",
    "print(f'Using LLMFT image: {rdct_aws(LLMFT_IMAGE_CUSTOM)}')\n",
    "if LLMFT_IMAGE_CUSTOM_SOURCE:\n",
    "    print(f'LLMFT_IMAGE_CUSTOM_SOURCE (SSM): {LLMFT_IMAGE_CUSTOM_SOURCE}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hf-token-md",
   "metadata": {},
   "source": [
    "## 3b) Hugging Face access token (Secrets Manager)\n",
    "\n",
    "Some models/datasets (for example, Llama) require an authenticated Hugging Face token.\n",
    "\n",
    "This notebook reads the token from AWS Secrets Manager (`nemo-container-build/hf-access-token`) and sets standard environment variables used by Hugging Face libraries.\n",
    "\n",
    "We also point Hugging Face caches at FSx so downloads persist across runs.\n",
    "\n",
    "**Security note:** the token is never printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hf-token-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:04.737769Z",
     "iopub.status.busy": "2025-12-31T19:24:04.737561Z",
     "iopub.status.idle": "2025-12-31T19:24:04.854387Z",
     "shell.execute_reply": "2025-12-31T19:24:04.853388Z",
     "shell.execute_reply.started": "2025-12-31T19:24:04.737749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hugging Face token + cache config\n",
      "=================================\n",
      "Resolved HF token from Secrets Manager: nemo-container-build/hf-access-token (length=37)\n",
      "HF_HOME: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/hf-home\n",
      "HF_HUB_CACHE: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/hf-home/hub\n",
      "HF_DATASETS_CACHE: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/hf-home/datasets\n",
      "HF_ASSETS_CACHE: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/hf-home/assets\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "print_header('Hugging Face token + cache config')\n",
    "\n",
    "HF_SECRET_ID = os.environ.get('HF_SECRET_ID', 'nemo-container-build/hf-access-token').strip()\n",
    "secrets = boto3.client('secretsmanager', region_name=REGION)\n",
    "\n",
    "hf_token = ''\n",
    "try:\n",
    "    resp = secrets.get_secret_value(SecretId=HF_SECRET_ID)\n",
    "    secret_str = resp.get('SecretString')\n",
    "    if not secret_str and resp.get('SecretBinary'):\n",
    "        secret_str = base64.b64decode(resp['SecretBinary']).decode('utf-8')\n",
    "    secret_str = (secret_str or '').strip()\n",
    "    if secret_str:\n",
    "        try:\n",
    "            data = json.loads(secret_str)\n",
    "            hf_token = (\n",
    "                data.get('token')\n",
    "                or data.get('hf_token')\n",
    "                or data.get('HUGGING_FACE_HUB_TOKEN')\n",
    "                or data.get('HF_TOKEN')\n",
    "                or ''\n",
    "            ).strip()\n",
    "        except Exception:\n",
    "            hf_token = secret_str\n",
    "except Exception as e:\n",
    "    print(f'WARNING: Could not read HF token secret ({HF_SECRET_ID}): {e}')\n",
    "\n",
    "if hf_token:\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "    print(f'Resolved HF token from Secrets Manager: {HF_SECRET_ID} (length={len(hf_token)})')\n",
    "else:\n",
    "    print('HF token not set. Public models/datasets will still work, but gated repos will fail.')\n",
    "\n",
    "# Put Hugging Face caches on FSx so downloads persist (Space + jobs share the same filesystem).\n",
    "HF_HOME_DIR = FSX_BASE_DIR / 'hf-home'\n",
    "HF_HUB_CACHE_DIR = HF_HOME_DIR / 'hub'\n",
    "HF_DATASETS_CACHE_DIR = HF_HOME_DIR / 'datasets'\n",
    "HF_ASSETS_CACHE_DIR = HF_HOME_DIR / 'assets'\n",
    "for d in [HF_HOME_DIR, HF_HUB_CACHE_DIR, HF_DATASETS_CACHE_DIR, HF_ASSETS_CACHE_DIR]:\n",
    "    ensure_dir(d)\n",
    "\n",
    "os.environ['HF_HOME'] = str(HF_HOME_DIR)\n",
    "os.environ['HF_HUB_CACHE'] = str(HF_HUB_CACHE_DIR)\n",
    "os.environ['HF_DATASETS_CACHE'] = str(HF_DATASETS_CACHE_DIR)\n",
    "os.environ['HF_ASSETS_CACHE'] = str(HF_ASSETS_CACHE_DIR)\n",
    "# Optional compatibility for older Transformers docs/code: keep this aligned with HF_HUB_CACHE\n",
    "os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')\n",
    "\n",
    "print('HF_HOME:', os.environ['HF_HOME'])\n",
    "print('HF_HUB_CACHE:', os.environ['HF_HUB_CACHE'])\n",
    "print('HF_DATASETS_CACHE:', os.environ['HF_DATASETS_CACHE'])\n",
    "print('HF_ASSETS_CACHE:', os.environ['HF_ASSETS_CACHE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c30eed",
   "metadata": {},
   "source": [
    "## 4) Platform validation (HyperPod + EKS add-ons)\n",
    "\n",
    "Now we sanity-check the *platform* before we spend time running workloads.\n",
    "\n",
    "This section:\n",
    "1. Describes the SageMaker HyperPod cluster and confirms it\u2019s using an **EKS orchestrator**.\n",
    "2. Extracts the underlying EKS cluster name and lists installed add-ons.\n",
    "3. Sets guardrails we use later:\n",
    "   - `HAS_TRAINING_OPERATOR` means the HyperPod **training operator** add-on is installed (it provides the `HyperPodPytorchJob` Kubernetes API + controller that turns a job spec into real training pods, and adds resiliency features like surgical recovery and hang monitoring).\n",
    "   - `HAS_TASK_GOV` means HyperPod **task governance** is installed (it integrates with Kueue for admission/priority/preemption; when it\u2019s present, Kueue queue/priority labels and topology hints are meaningful).\n",
    "4. (Optional) Updates your local `kubectl` context so we can inspect Kubernetes objects when troubleshooting.\n",
    "\n",
    "**Note:** some kubectl builds don\u2019t support `kubectl version --client --short`; that warning is harmless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e478fa5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:04.857675Z",
     "iopub.status.busy": "2025-12-31T19:24:04.857344Z",
     "iopub.status.idle": "2025-12-31T19:24:07.370727Z",
     "shell.execute_reply": "2025-12-31T19:24:07.369768Z",
     "shell.execute_reply.started": "2025-12-31T19:24:04.857633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HyperPod cluster\n",
      "================\n",
      "ClusterStatus: InService\n",
      "OrchestratorType: EKS\n",
      "\n",
      "EKS add-ons\n",
      "===========\n",
      "Add-ons: ['amazon-sagemaker-hyperpod-observability', 'amazon-sagemaker-hyperpod-taskgovernance', 'amazon-sagemaker-hyperpod-training-operator', 'aws-fsx-csi-driver', 'cert-manager', 'coredns', 'eks-pod-identity-agent', 'kube-proxy', 'vpc-cni']\n",
      "HAS_TRAINING_OPERATOR: True\n",
      "HAS_TASK_GOV: True\n",
      "TRAINING_OPERATOR_VERSION: v1.2.0-eksbuild.1\n",
      "TASK_GOV_VERSION: v1.3.1-eksbuild.1\n",
      "\n",
      "kubectl context\n",
      "===============\n",
      "Updated context <ARN> in /home/sagemaker-user/.kube/config\n",
      "Downloading kubectl: https://dl.k8s.io/release/v1.33.7/bin/linux/amd64/kubectl\n",
      "kubectl installed at /home/sagemaker-user/.local/bin/kubectl\n",
      "kubectl version: Client Version: v1.33.7\n",
      "Kustomize Version: v5.6.0\n",
      "Current kubectl context: <ARN>\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "sm = boto3.client('sagemaker', region_name=REGION)\n",
    "eks = boto3.client('eks', region_name=REGION)\n",
    "\n",
    "print_header('HyperPod cluster')\n",
    "cluster_desc = sm.describe_cluster(ClusterName=HYPERPOD_CLUSTER_NAME)\n",
    "CLUSTER_STATUS = cluster_desc.get('ClusterStatus')\n",
    "ORCHESTRATOR = cluster_desc.get('Orchestrator', {})\n",
    "ORCHESTRATOR_TYPE = 'EKS' if ORCHESTRATOR.get('Eks') else None\n",
    "EKS_CLUSTER_ARN = ORCHESTRATOR.get('Eks', {}).get('ClusterArn', '')\n",
    "EKS_CLUSTER_NAME = EKS_CLUSTER_ARN.split('/')[-1] if EKS_CLUSTER_ARN else ''\n",
    "print('ClusterStatus:', CLUSTER_STATUS)\n",
    "print('OrchestratorType:', ORCHESTRATOR_TYPE)\n",
    "\n",
    "require(ORCHESTRATOR_TYPE == 'EKS', f'Expected EKS orchestrator, got: {ORCHESTRATOR}')\n",
    "require(EKS_CLUSTER_NAME, 'Could not derive EKS cluster name from Orchestrator.Eks.ClusterArn')\n",
    "\n",
    "print_header('EKS add-ons')\n",
    "addons = eks.list_addons(clusterName=EKS_CLUSTER_NAME).get('addons', [])\n",
    "print('Add-ons:', addons)\n",
    "\n",
    "HAS_TRAINING_OPERATOR = 'amazon-sagemaker-hyperpod-training-operator' in addons\n",
    "HAS_TASK_GOV = 'amazon-sagemaker-hyperpod-taskgovernance' in addons\n",
    "print('HAS_TRAINING_OPERATOR:', HAS_TRAINING_OPERATOR)\n",
    "print('HAS_TASK_GOV:', HAS_TASK_GOV)\n",
    "\n",
    "TRAINING_OPERATOR_VERSION = None\n",
    "TASK_GOV_VERSION = None\n",
    "if HAS_TRAINING_OPERATOR:\n",
    "    TRAINING_OPERATOR_VERSION = eks.describe_addon(\n",
    "        clusterName=EKS_CLUSTER_NAME,\n",
    "        addonName='amazon-sagemaker-hyperpod-training-operator'\n",
    "    )['addon'].get('addonVersion')\n",
    "if HAS_TASK_GOV:\n",
    "    TASK_GOV_VERSION = eks.describe_addon(\n",
    "        clusterName=EKS_CLUSTER_NAME,\n",
    "        addonName='amazon-sagemaker-hyperpod-taskgovernance'\n",
    "    )['addon'].get('addonVersion')\n",
    "\n",
    "print('TRAINING_OPERATOR_VERSION:', TRAINING_OPERATOR_VERSION)\n",
    "print('TASK_GOV_VERSION:', TASK_GOV_VERSION)\n",
    "\n",
    "if not HAS_TRAINING_OPERATOR:\n",
    "    print('WARNING: Training operator not found. Recipe-based HyperPod jobs will be skipped.')\n",
    "if not HAS_TASK_GOV:\n",
    "    print('WARNING: Task governance not found. Kueue/topology scheduling will be skipped.')\n",
    "\n",
    "\n",
    "print_header('kubectl context')\n",
    "import shutil\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "import platform\n",
    "\n",
    "if RUN_KUBECTL_CONTEXT:\n",
    "    # kubeconfig update does not require kubectl\n",
    "    cmd = f\"aws eks update-kubeconfig --name {EKS_CLUSTER_NAME} --region {REGION}\"\n",
    "    rc, out = run(cmd, check=False)\n",
    "    print(rdct_aws(out))\n",
    "    if rc != 0:\n",
    "        raise RuntimeError('Failed to update kubeconfig. Check AWS credentials and permissions.')\n",
    "\n",
    "    if shutil.which('kubectl'):\n",
    "        print('kubectl already available:', run('kubectl version --client --short', check=False)[1])\n",
    "    else:\n",
    "        require(EKS_CLUSTER_NAME, 'Could not resolve EKS cluster name for kubectl install')\n",
    "        eks = boto3.client('eks', region_name=REGION)\n",
    "        k8s_version = eks.describe_cluster(name=EKS_CLUSTER_NAME)['cluster']['version']  # e.g., '1.30'\n",
    "        major_minor = '.'.join(k8s_version.split('.')[:2])\n",
    "\n",
    "        # Prefer EKS-hosted kubectl if path is provided; otherwise derive upstream version\n",
    "        eks_s3_path = os.environ.get('KUBECTL_EKS_S3_PATH', '').strip()\n",
    "        upstream_version = ''\n",
    "        if not eks_s3_path:\n",
    "            try:\n",
    "                with urlopen(f'https://dl.k8s.io/release/stable-{major_minor}.txt') as r:\n",
    "                    upstream_version = r.read().decode().strip()\n",
    "            except Exception as e:\n",
    "                print('WARN: Could not resolve upstream kubectl version:', e)\n",
    "\n",
    "        arch = platform.machine().lower()\n",
    "        arch = 'amd64' if arch in ('x86_64', 'amd64') else 'arm64' if arch in ('aarch64', 'arm64') else 'amd64'\n",
    "        os_name = 'linux'\n",
    "\n",
    "        url = ''\n",
    "        if eks_s3_path:\n",
    "            s3_region = os.environ.get('KUBECTL_S3_REGION', '').strip() or REGION\n",
    "            url = f'https://s3.{s3_region}.amazonaws.com/amazon-eks/{eks_s3_path}/bin/{os_name}/{arch}/kubectl'\n",
    "        elif upstream_version:\n",
    "            url = f'https://dl.k8s.io/release/{upstream_version}/bin/{os_name}/{arch}/kubectl'\n",
    "        else:\n",
    "            print('Could not determine kubectl download URL; skipping install.')\n",
    "\n",
    "        if url:\n",
    "            dest_dir = Path.home() / '.local' / 'bin'\n",
    "            dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "            dest = dest_dir / 'kubectl'\n",
    "            print('Downloading kubectl:', url)\n",
    "            urlretrieve(url, dest)\n",
    "            dest.chmod(0o755)\n",
    "            os.environ['PATH'] = f'{dest_dir}:' + os.environ.get('PATH', '')\n",
    "            print('kubectl installed at', dest)\n",
    "            print('kubectl version:', run('kubectl version --client', check=False)[1])\n",
    "\n",
    "    if shutil.which('kubectl'):\n",
    "        _, ctx = run('kubectl config current-context', check=False)\n",
    "        print('Current kubectl context:', rdct_aws(ctx))\n",
    "else:\n",
    "    print('Skipping kubectl setup (RUN_KUBECTL_CONTEXT=False).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a8c9d3",
   "metadata": {},
   "source": [
    "## 5) Storage & scheduling validation (FSx + real HyperPod pod)\n",
    "\n",
    "We do two writes to the same FSx location:\n",
    "- Write from the Space (local FSx mount) \u2192 proves the Space has read/write access.\n",
    "- Write from a HyperPod training pod \u2192 proves pods mount the same filesystem and can write back.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d85a16",
   "metadata": {},
   "source": [
    "### 5a) FSx local sanity check (Space \u2192 FSx)\n",
    "\n",
    "We create a run-specific directory on FSx and write a small \u201csentinel\u201d file from the Space.\n",
    "\n",
    "Pay attention to:\n",
    "- `FSX_BASE_DIR`: where we write (under FSx)\n",
    "- `RUN_ID`: part of the filename, so reruns don\u2019t overwrite earlier runs\n",
    "\n",
    "**Success looks like:** the cell prints a file path under `.../smus-nemo-smoke/` and reads back the timestamp it just wrote. You should also be able to see the file in the Jupyter file browser under the home symlink to FSx (for example `~/custom-file-systems/fsx_lustre/<fs-id>/smus-nemo-smoke/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a535bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:07.374772Z",
     "iopub.status.busy": "2025-12-31T19:24:07.374475Z",
     "iopub.status.idle": "2025-12-31T19:24:07.379285Z",
     "shell.execute_reply": "2025-12-31T19:24:07.378532Z",
     "shell.execute_reply.started": "2025-12-31T19:24:07.374743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping FSx local sanity check (RUN_FSX_CHECKS=False).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Local FSx sanity check ---\n",
    "if RUN_FSX_CHECKS:\n",
    "    print_header('FSx local sanity check')\n",
    "    ensure_dir(FSX_BASE_DIR)\n",
    "    sentinel = FSX_BASE_DIR / f'space_sentinel_{RUN_ID}.txt'\n",
    "    sentinel.write_text(f'space-write {datetime.utcnow().isoformat()}\\n')\n",
    "    print('Wrote:', sentinel)\n",
    "    print('Read:', sentinel.read_text().strip())\n",
    "else:\n",
    "    print('Skipping FSx local sanity check (RUN_FSX_CHECKS=False).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef95e7c",
   "metadata": {},
   "source": [
    "### 5b) HyperPod SDK + FSx PVC (pinned + predictable)\n",
    "\n",
    "Before submitting anything, we do two practical checks:\n",
    "\n",
    "1. The FSx PersistentVolumeClaim exists in your user namespace (default: `fsx-claim`).\n",
    "2. The notebook can import the pinned HyperPod SDK (`sagemaker-hyperpod==3.5.0`) using the stable module layout:\n",
    "   `from sagemaker.hyperpod.training import HyperPodPytorchJob`\n",
    "\n",
    "**Parameters you might tweak:**\n",
    "- `HYPERPOD_NAMESPACE` (defaults to `hyperpod-ns-datascientist1`)\n",
    "- `FSX_PVC_NAME` (defaults to `fsx-claim`)\n",
    "\n",
    "**Success looks like:** `kubectl get pvc ...` shows `Bound`, and the import cell runs without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff23515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:07.381678Z",
     "iopub.status.busy": "2025-12-31T19:24:07.381429Z",
     "iopub.status.idle": "2025-12-31T19:24:07.385354Z",
     "shell.execute_reply": "2025-12-31T19:24:07.384686Z",
     "shell.execute_reply.started": "2025-12-31T19:24:07.381640Z"
    }
   },
   "outputs": [],
   "source": [
    "namespace = os.environ.get('HYPERPOD_NAMESPACE', '').strip() or 'hyperpod-ns-datascientist1'\n",
    "os.environ['HYPERPOD_NAMESPACE'] = namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074d3273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:07.386441Z",
     "iopub.status.busy": "2025-12-31T19:24:07.386040Z",
     "iopub.status.idle": "2025-12-31T19:24:08.206992Z",
     "shell.execute_reply": "2025-12-31T19:24:08.206009Z",
     "shell.execute_reply.started": "2025-12-31T19:24:07.386414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        STATUS   VOLUME                              CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE\n",
      "fsx-claim   Bound    fsx-pv-hyperpod-ns-datascientist1   1200Gi     RWX                           <unset>                 2d12h\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pvc -n $HYPERPOD_NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c807194a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:08.208262Z",
     "iopub.status.busy": "2025-12-31T19:24:08.208009Z",
     "iopub.status.idle": "2025-12-31T19:24:08.213173Z",
     "shell.execute_reply": "2025-12-31T19:24:08.212402Z",
     "shell.execute_reply.started": "2025-12-31T19:24:08.208237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping FSx pod test (RUN_FSX_CHECKS=False).\n"
     ]
    }
   ],
   "source": [
    "# --- HyperPod pod write/read check (Python SDK) ---\n",
    "RUN_FSX_POD_TEST = RUN_FSX_CHECKS and HAS_TRAINING_OPERATOR\n",
    "if RUN_FSX_POD_TEST:\n",
    "    print_header('FSx pod write test (HyperPodPytorchJob)')\n",
    "    try:\n",
    "        from sagemaker.hyperpod.training.hyperpod_pytorch_job import HyperPodPytorchJob\n",
    "        from sagemaker.hyperpod.training.config.hyperpod_pytorch_job_unified_config import (\n",
    "            ReplicaSpec, Template, Spec, Containers, Resources, RunPolicy\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    import json\n",
    "    import shutil\n",
    "else:\n",
    "    if not RUN_FSX_CHECKS:\n",
    "        print('Skipping FSx pod test (RUN_FSX_CHECKS=False).')\n",
    "    elif not HAS_TRAINING_OPERATOR:\n",
    "        print('Skipping FSx pod test (training operator not available).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4235cd91",
   "metadata": {},
   "source": [
    "### 5c) Kueue/task governance discovery (queue, priority, topology)\n",
    "\n",
    "If Task Governance is installed, HyperPod uses **Kueue** to decide whether your job is admitted and *where* it can run.\n",
    "\n",
    "This cell auto-discovers (via `kubectl`) some defaults for this namespace:\n",
    "- LocalQueue name (used for admission)\n",
    "- Priority class (used to sort work)\n",
    "- Topology label (optional) for topology-aware placement\n",
    "\n",
    "**Overrides (optional):**\n",
    "- `KUEUE_QUEUE_NAME`\n",
    "- `KUEUE_PRIORITY_CLASS`\n",
    "- `KUEUE_TOPOLOGY_LABEL`\n",
    "- `KUEUE_TOPOLOGY_MODE` = `preferred` or `required`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef762ffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:08.215674Z",
     "iopub.status.busy": "2025-12-31T19:24:08.215492Z",
     "iopub.status.idle": "2025-12-31T19:24:08.236294Z",
     "shell.execute_reply": "2025-12-31T19:24:08.235424Z",
     "shell.execute_reply.started": "2025-12-31T19:24:08.215658Z"
    }
   },
   "outputs": [],
   "source": [
    "if RUN_FSX_POD_TEST:\n",
    "    print_header('Kueue discovery (namespace-scoped)')\n",
    "\n",
    "    # --- Env overrides (optional) ---\n",
    "    KUEUE_QUEUE_NAME = os.environ.get('KUEUE_QUEUE_NAME', '').strip()\n",
    "    KUEUE_PRIORITY_CLASS = os.environ.get('KUEUE_PRIORITY_CLASS', 'interactive-priority').strip()\n",
    "    KUEUE_TOPOLOGY_LABEL = os.environ.get('KUEUE_TOPOLOGY_LABEL', '').strip()\n",
    "    KUEUE_TOPOLOGY_MODE = (os.environ.get('KUEUE_TOPOLOGY_MODE', '') or 'preferred').strip().lower()\n",
    "    if KUEUE_TOPOLOGY_MODE not in ['preferred', 'required']:\n",
    "        KUEUE_TOPOLOGY_MODE = 'preferred'\n",
    "\n",
    "    DEFAULT_KUEUE_PRIORITY_CLASS = os.environ.get('DEFAULT_KUEUE_PRIORITY_CLASS', 'training-priority').strip() or 'training-priority'\n",
    "    job_namespace = os.environ.get('HYPERPOD_NAMESPACE', '').strip() or 'hyperpod-ns-datascientist1'\n",
    "\n",
    "    def _extract_json(blob):\n",
    "        if not blob:\n",
    "            return None\n",
    "        for idx, ch in enumerate(blob):\n",
    "            if ch in '{[':\n",
    "                try:\n",
    "                    return json.loads(blob[idx:])\n",
    "                except Exception:\n",
    "                    return None\n",
    "        return None\n",
    "\n",
    "    def _kubectl_json(cmd, label=None):\n",
    "        if not shutil.which('kubectl'):\n",
    "            return None\n",
    "        rc, out = run(cmd, check=False)\n",
    "        if rc != 0 or not out:\n",
    "            if label:\n",
    "                print(f\"{label} not visible: {out or '(no output)'}\")\n",
    "            return None\n",
    "        data = _extract_json(out)\n",
    "        if data is None and label:\n",
    "            print(f\"{label} not visible (non-JSON output): {out[:200]}\")\n",
    "        return data\n",
    "\n",
    "    def _can_i(resource, namespace=None):\n",
    "        cmd = f\"kubectl auth can-i list {resource}\"\n",
    "        if namespace:\n",
    "            cmd += f\" -n {namespace}\"\n",
    "        rc, out = run(cmd, check=False)\n",
    "        if rc != 0:\n",
    "            return False\n",
    "        last = out.strip().splitlines()[-1].strip().lower() if out else ''\n",
    "        return last == 'yes'\n",
    "\n",
    "    def _list_localqueues(ns):\n",
    "        if not _can_i('localqueues.kueue.x-k8s.io', namespace=ns):\n",
    "            return []\n",
    "        data = _kubectl_json(f\"kubectl get localqueue -n {ns} -o json\", label=f\"LocalQueues in {ns}\")\n",
    "        if not data:\n",
    "            return []\n",
    "        items = data.get('items', [])\n",
    "        return [i.get('metadata', {}).get('name', '') for i in items if i.get('metadata', {}).get('name', '')]\n",
    "\n",
    "    def _list_workloadpriorityclasses():\n",
    "        if not _can_i('workloadpriorityclasses.kueue.x-k8s.io'):\n",
    "            return []\n",
    "        data = _kubectl_json('kubectl get workloadpriorityclass -o json', label='WorkloadPriorityClasses')\n",
    "        if not data:\n",
    "            return []\n",
    "        items = data.get('items', [])\n",
    "        result = []\n",
    "        for item in items:\n",
    "            name = item.get('metadata', {}).get('name', '')\n",
    "            value = item.get('value') or 0\n",
    "            if name:\n",
    "                result.append({'name': name, 'value': value})\n",
    "        return result\n",
    "\n",
    "    def _list_priorityclasses():\n",
    "        if not _can_i('priorityclasses.scheduling.k8s.io'):\n",
    "            return []\n",
    "        data = _kubectl_json('kubectl get priorityclass -o json', label='PriorityClasses')\n",
    "        if not data:\n",
    "            return []\n",
    "        items = data.get('items', [])\n",
    "        result = []\n",
    "        for item in items:\n",
    "            name = item.get('metadata', {}).get('name', '')\n",
    "            value = item.get('value') or 0\n",
    "            if name:\n",
    "                result.append({'name': name, 'value': value})\n",
    "        return result\n",
    "\n",
    "    def _list_topology_labels():\n",
    "        if not _can_i('nodes'):\n",
    "            return {}\n",
    "        data = _kubectl_json('kubectl get nodes -o json', label='Topology labels')\n",
    "        if not data:\n",
    "            return {}\n",
    "        labels_map = {}\n",
    "        for node in data.get('items', []):\n",
    "            labels = node.get('metadata', {}).get('labels', {}) or {}\n",
    "            for key, value in labels.items():\n",
    "                if key.startswith('topology.') or key.startswith('topology.k8s.aws/'):\n",
    "                    labels_map.setdefault(key, set()).add(value)\n",
    "        return {k: sorted(v) for k, v in labels_map.items()}\n",
    "\n",
    "    def _auto_localqueue(localqueues):\n",
    "        if not localqueues:\n",
    "            return ''\n",
    "        expected = f\"{job_namespace}-localqueue\"\n",
    "        for name in localqueues:\n",
    "            if name == expected:\n",
    "                return name\n",
    "        return localqueues[0]\n",
    "\n",
    "    def _auto_workload_priority(items):\n",
    "        if not items:\n",
    "            return ''\n",
    "        def score(item):\n",
    "            name = item.get('name', '')\n",
    "            value = item.get('value') or 0\n",
    "            return (1 if 'high' in name else 0, value)\n",
    "        items = sorted(items, key=score, reverse=True)\n",
    "        return items[0].get('name', '')\n",
    "\n",
    "    def _auto_priority_class(items):\n",
    "        if not items:\n",
    "            return ''\n",
    "        candidates = [i for i in items if not i.get('name', '').startswith('system-')]\n",
    "        if not candidates:\n",
    "            candidates = items\n",
    "        candidates = sorted(candidates, key=lambda i: i.get('value') or 0, reverse=True)\n",
    "        return candidates[0].get('name', '')\n",
    "\n",
    "    def _auto_topology_label(labels_map):\n",
    "        if not labels_map:\n",
    "            return ''\n",
    "        preferred_order = [\n",
    "            'topology.kubernetes.io/zone',\n",
    "            'topology.k8s.aws/zone-id',\n",
    "            'topology.k8s.aws/network-node-layer-3',\n",
    "            'topology.k8s.aws/network-node-layer-2',\n",
    "            'topology.k8s.aws/network-node-layer-1',\n",
    "        ]\n",
    "        for key in preferred_order:\n",
    "            if key in labels_map:\n",
    "                return key\n",
    "        for key in sorted(labels_map.keys()):\n",
    "            if key.startswith('topology.k8s.aws/'):\n",
    "                return key\n",
    "        return ''\n",
    "\n",
    "    if HAS_TASK_GOV:\n",
    "        print('Using namespace:', job_namespace)\n",
    "        localqueues = _list_localqueues(job_namespace)\n",
    "        workload_priorities = _list_workloadpriorityclasses()\n",
    "        priority_classes = _list_priorityclasses()\n",
    "        topology_labels = _list_topology_labels()\n",
    "\n",
    "        print(f\"LocalQueues in {job_namespace}:\", localqueues or '(none visible)')\n",
    "        if workload_priorities:\n",
    "            print('WorkloadPriorityClasses:', [(i['name'], i['value']) for i in workload_priorities])\n",
    "        else:\n",
    "            print('WorkloadPriorityClasses: (none visible)')\n",
    "        if priority_classes:\n",
    "            print('PriorityClasses:', [(i['name'], i['value']) for i in priority_classes])\n",
    "        else:\n",
    "            print('PriorityClasses: (none visible)')\n",
    "        if topology_labels:\n",
    "            print('Topology labels (key -> values):')\n",
    "            for key in sorted(topology_labels.keys()):\n",
    "                print(f\"  {key}: {topology_labels[key]}\")\n",
    "        else:\n",
    "            print('Topology labels: (none visible)')\n",
    "\n",
    "        print_header('Kueue defaults (selected for this run)')\n",
    "        if not KUEUE_QUEUE_NAME:\n",
    "            KUEUE_QUEUE_NAME = _auto_localqueue(localqueues)\n",
    "        if not KUEUE_PRIORITY_CLASS:\n",
    "            KUEUE_PRIORITY_CLASS = _auto_workload_priority(workload_priorities) or _auto_priority_class(priority_classes) or DEFAULT_KUEUE_PRIORITY_CLASS\n",
    "        if RUN_KUEUE_TOPOLOGY and not KUEUE_TOPOLOGY_LABEL:\n",
    "            KUEUE_TOPOLOGY_LABEL = _auto_topology_label(topology_labels)\n",
    "\n",
    "        print('Kueue queue:', KUEUE_QUEUE_NAME or '(not set)')\n",
    "        print('Kueue priority class:', KUEUE_PRIORITY_CLASS or '(not set)')\n",
    "        if RUN_KUEUE_TOPOLOGY:\n",
    "            print('Kueue topology label:', KUEUE_TOPOLOGY_LABEL or '(not set)')\n",
    "    else:\n",
    "        print('Task governance not enabled; skipping Kueue discovery.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab2fe63",
   "metadata": {},
   "source": [
    "### 5d) Build a minimal HyperPod job spec (with labels/annotations)\n",
    "\n",
    "Here we assemble the smallest possible \u201creal\u201d HyperPod job:\n",
    "- 1 replica, 1 process (`nprocPerNode=1`)\n",
    "- Mounts the FSx PVC at `/fsx`\n",
    "- Requests 1 GPU (to land on training nodes)\n",
    "- Runs a short bash command that writes `pod_sentinel_<RUN_ID>.txt` into FSx\n",
    "\n",
    "When Task Governance is enabled, we also attach:\n",
    "- `kueue.x-k8s.io/queue-name`\n",
    "- `kueue.x-k8s.io/priority-class`\n",
    "- `kueue.x-k8s.io/podset-(preferred|required)-topology` (if configured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3f8557f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:08.239079Z",
     "iopub.status.busy": "2025-12-31T19:24:08.238816Z",
     "iopub.status.idle": "2025-12-31T19:24:08.247540Z",
     "shell.execute_reply": "2025-12-31T19:24:08.246696Z",
     "shell.execute_reply.started": "2025-12-31T19:24:08.239053Z"
    }
   },
   "outputs": [],
   "source": [
    "if RUN_FSX_POD_TEST:\n",
    "    pod_labels = {}\n",
    "    pod_annotations = {}\n",
    "    job_labels = {}\n",
    "    job_annotations = {}\n",
    "    if HAS_TASK_GOV and KUEUE_QUEUE_NAME:\n",
    "        pod_labels['kueue.x-k8s.io/queue-name'] = KUEUE_QUEUE_NAME\n",
    "        job_labels['kueue.x-k8s.io/queue-name'] = KUEUE_QUEUE_NAME\n",
    "    if HAS_TASK_GOV and KUEUE_PRIORITY_CLASS:\n",
    "        pod_labels['kueue.x-k8s.io/priority-class'] = KUEUE_PRIORITY_CLASS\n",
    "        job_labels['kueue.x-k8s.io/priority-class'] = KUEUE_PRIORITY_CLASS\n",
    "    if HAS_TASK_GOV and RUN_KUEUE_TOPOLOGY and KUEUE_TOPOLOGY_LABEL:\n",
    "        key = 'kueue.x-k8s.io/podset-preferred-topology' if KUEUE_TOPOLOGY_MODE == 'preferred' else 'kueue.x-k8s.io/podset-required-topology'\n",
    "        pod_annotations[key] = KUEUE_TOPOLOGY_LABEL\n",
    "        job_annotations[key] = KUEUE_TOPOLOGY_LABEL\n",
    "\n",
    "    FSX_TEST_GPU = '1'  # request GPU to force scheduling on training nodes\n",
    "    FSX_PVC_NAME = os.environ.get('FSX_PVC_NAME', 'fsx-claim')\n",
    "    fsx_pod_path = f\"{FSX_POD_PREFIX}/{FSX_PVC_SUBPATH}/{FSX_BASE_DIRNAME}\"\n",
    "    sentinel_name = f'pod_sentinel_{RUN_ID}.txt'\n",
    "\n",
    "    inner_cmd = (\n",
    "        f'mkdir -p {fsx_pod_path}; '\n",
    "        f'echo pod-write $(hostname) $(date -u +%Y-%m-%dT%H:%M:%SZ) > {fsx_pod_path}/{sentinel_name}; '\n",
    "        f'ls -l {fsx_pod_path}/{sentinel_name}'\n",
    "    )\n",
    "\n",
    "    cmd = (\n",
    "        'set -euo pipefail; '\n",
    "        'if ! command -v hyperpodrun >/dev/null 2>&1; then '\n",
    "        '  echo \"hyperpodrun not found; installing hyperpod-elastic-agent\"; '\n",
    "        '  python -m pip install -q hyperpod-elastic-agent; '\n",
    "        'fi; '\n",
    "        f'hyperpodrun --nnodes 1 --nproc-per-node 1 --rdzv-backend hyperpod --no-python /bin/bash -lc \\\"{inner_cmd}\\\"'\n",
    "    )\n",
    "\n",
    "    replica_specs = [\n",
    "        ReplicaSpec(\n",
    "            name='pod',\n",
    "            replicas=1,\n",
    "            template=Template(\n",
    "                metadata={\n",
    "                    'labels': pod_labels if pod_labels else None,\n",
    "                    'annotations': pod_annotations if pod_annotations else None,\n",
    "                },\n",
    "                spec=Spec(\n",
    "                    containers=[\n",
    "                        Containers(\n",
    "                            name='fsx-test',\n",
    "                            image=LLMFT_IMAGE_CUSTOM,\n",
    "                            image_pull_policy='Always',\n",
    "                            resources=Resources(\n",
    "                                requests={'nvidia.com/gpu': FSX_TEST_GPU},\n",
    "                                limits={'nvidia.com/gpu': FSX_TEST_GPU},\n",
    "                            ),\n",
    "                            command=['bash', '-lc'],\n",
    "                            args=[cmd],\n",
    "                            volume_mounts=[{'name': 'fsx-claim', 'mount_path': FSX_POD_PREFIX}],\n",
    "                        )\n",
    "                    ],\n",
    "                    volumes=[{'name': 'fsx-claim', 'persistent_volume_claim': {'claim_name': FSX_PVC_NAME}}],\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    job_name = f'fsx-test-{RUN_ID}'\n",
    "\n",
    "    metadata = {\n",
    "        'name': job_name,\n",
    "        'namespace': job_namespace or None,\n",
    "        'labels': job_labels if job_labels else None,\n",
    "        'annotations': job_annotations if job_annotations else None,\n",
    "    }\n",
    "\n",
    "    job_kwargs = dict(\n",
    "        metadata=metadata,\n",
    "        nproc_per_node='1',\n",
    "        replica_specs=replica_specs,\n",
    "        run_policy=RunPolicy(clean_pod_policy='OnlyComplete'),\n",
    "    )\n",
    "    model_fields = getattr(HyperPodPytorchJob, 'model_fields', None) or getattr(HyperPodPytorchJob, '__fields__', {})\n",
    "\n",
    "\n",
    "    pytorch_job = HyperPodPytorchJob(**job_kwargs)\n",
    "    print_header('FSx test job spec (computed)')\n",
    "    try:\n",
    "        job_spec = pytorch_job.model_dump(exclude_none=True)\n",
    "    except Exception:\n",
    "        job_spec = job_kwargs\n",
    "    print(rdct_aws(json.dumps(job_spec, indent=2, default=str)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb2111",
   "metadata": {},
   "source": [
    "### 5e) Submit, poll, and verify on FSx\n",
    "\n",
    "We submit the job via the Python SDK, poll until it completes, then verify the sentinel file is visible from the Space FSx mount.\n",
    "\n",
    "What you might see while polling:\n",
    "- `Suspended`: waiting for Kueue admission / scheduling (common when the cluster is busy)\n",
    "- `Completed`: success\n",
    "- `Faulted`: something went wrong (check pod logs / events)\n",
    "\n",
    "**Success looks like:** the notebook prints `Pod wrote file: .../pod_sentinel_<RUN_ID>.txt` and shows its contents (hostname + timestamp). You should also see the pod written file in the mounted fsx file system folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81075c04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:08.249957Z",
     "iopub.status.busy": "2025-12-31T19:24:08.249599Z",
     "iopub.status.idle": "2025-12-31T19:24:08.253730Z",
     "shell.execute_reply": "2025-12-31T19:24:08.252931Z",
     "shell.execute_reply.started": "2025-12-31T19:24:08.249931Z"
    }
   },
   "outputs": [],
   "source": [
    "if RUN_FSX_POD_TEST:\n",
    "    if 'pytorch_job' not in globals():\n",
    "        pytorch_job = HyperPodPytorchJob(**job_kwargs)\n",
    "    print('Submitting job:', job_name)\n",
    "    pytorch_job.create()\n",
    "\n",
    "    os.environ['FSX_TEST_JOB_NAME'] = job_name\n",
    "    os.environ['HYP_JOB_NAME'] = job_name\n",
    "    os.environ['FSX_TEST_SENTINEL_PATH'] = str(FSX_BASE_DIR / sentinel_name)\n",
    "    os.environ['FSX_TEST_POD_PATH'] = str(fsx_pod_path)\n",
    "    print('FSX_TEST_JOB_NAME set:', job_name)\n",
    "    print('Note: first run may take ~10 minutes due to initial image pull.')\n",
    "    print('Next: run the status cell below to check progress and locate the sentinel file.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "801cd8ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:08.254487Z",
     "iopub.status.busy": "2025-12-31T19:24:08.254298Z",
     "iopub.status.idle": "2025-12-31T19:24:08.261228Z",
     "shell.execute_reply": "2025-12-31T19:24:08.260374Z",
     "shell.execute_reply.started": "2025-12-31T19:24:08.254473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSX_TEST_JOB_NAME not set. Run the submission cell first.\n"
     ]
    }
   ],
   "source": [
    "# --- FSx test job status (async) ---\n",
    "job_name = os.environ.get('FSX_TEST_JOB_NAME', '').strip()\n",
    "if not job_name:\n",
    "    print('FSX_TEST_JOB_NAME not set. Run the submission cell first.')\n",
    "else:\n",
    "    os.environ['FSX_TEST_JOB_NAME'] = job_name\n",
    "    !hyp list hyp-pytorch-job -n ${HYPERPOD_NAMESPACE} | grep -F -- \"${FSX_TEST_JOB_NAME}\" || echo '(job not found)'\n",
    "    list_status, list_age, _line = get_hyp_list_status(job_name, namespace)\n",
    "    phase, conds = get_hyp_job_status(job_name, namespace)\n",
    "    phase = phase or list_status\n",
    "    print('Job:', job_name)\n",
    "    print('Phase:', phase if phase is not None else '(unavailable)')\n",
    "    if list_age:\n",
    "        print('Age:', list_age)\n",
    "    if conds:\n",
    "        print('Conditions:', conds)\n",
    "    if phase == 'Completed':\n",
    "        sentinel_path = os.environ.get('FSX_TEST_SENTINEL_PATH', '').strip()\n",
    "        if sentinel_path:\n",
    "            print('Job completed. Check the shared FSx folder for:', sentinel_path)\n",
    "        else:\n",
    "            print('Job completed. Check the shared FSx folder for the sentinel file.')\n",
    "    elif phase in {'Failed', 'Faulted'}:\n",
    "        print('Job failed. Re-run this cell or use hyp describe for details.')\n",
    "    elif phase is None:\n",
    "        print('Status unavailable (kubectl not ready?). You can use hyp describe in a terminal if needed (may show env vars).')\n",
    "    else:\n",
    "        print('Job still running. Re-run this cell in a few minutes.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7312abb",
   "metadata": {},
   "source": [
    "# Section 7 \u2014  Training with HyperPod Recipes (Primary Path)\n",
    "\n",
    "This section is the **training** for this notebook. We use **HyperPod Recipes** end\u2011to\u2011end to show:\n",
    "\n",
    "- **SFT LoRA** (multi\u2011node + elastic training)\n",
    "- **DPO** (preference alignment)\n",
    "- **Checkpointless training** (fast recovery)\n",
    "- **Kueue + topology\u2011aware scheduling**\n",
    "- **Cluster\u2011based inference at multiple stages** (base \u2192 SFT \u2192 DPO)\n",
    "\n",
    "We will dig into the recipes and hyperpod adapter for nemo afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92f274eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:08.263615Z",
     "iopub.status.busy": "2025-12-31T19:24:08.263373Z",
     "iopub.status.idle": "2025-12-31T19:24:08.996552Z",
     "shell.execute_reply": "2025-12-31T19:24:08.995748Z",
     "shell.execute_reply.started": "2025-12-31T19:24:08.263596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 7 setup (Recipes primary)\n",
      "================================="
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 7 toggles:\n",
      "{'RUN_RECIPE_SFT': True, 'RUN_RECIPE_DPO': True, 'RUN_CHECKPOINTLESS': True, 'RUN_INFER_BASELINE': True, 'RUN_INFER_POST_SFT': True, 'RUN_INFER_POST_DPO': True}\n",
      "Kueue/Topology:\n",
      "{'HYPERPOD_NAMESPACE': 'hyperpod-ns-datascientist1', 'KUEUE_QUEUE_NAME': 'hyperpod-ns-datascientist1-localqueue', 'KUEUE_PRIORITY_CLASS_TRAIN': 'training-priority', 'KUEUE_PRIORITY_CLASS_INFER': 'inference-priority', 'KUEUE_TOPOLOGY_LABEL_TRAIN': 'topology.k8s.aws/network-node-layer-3', 'KUEUE_TOPOLOGY_LABEL_INFER': 'topology.kubernetes.io/zone', 'KUEUE_TOPOLOGY_MODE_TRAIN': 'preferred', 'KUEUE_TOPOLOGY_MODE_INFER': 'preferred'}\n",
      "Instance type: p4d.24xlarge count: 3\n",
      "GPUs per node: 8\n",
      "LLMFT nodes: 1 (elastic: True 1-16)\n",
      "Checkpointless nodes: 2\n",
      "POD_FSX_ROOT: /fsx/fs-03b1953d09801303c/smus-nemo-smoke\n",
      "LLMFT_IMAGE: 327873000638.dkr.ecr.us-east-1.amazonaws.com/hyperpod-recipes:llmft-v1.0.0-llama\n",
      "CKPTLESS_IMAGE: 839249767557.dkr.ecr.us-west-2.amazonaws.com/hyperpod-checkpointless-training:v1.0.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 7a: Shared config + toggles (Recipes primary)\n",
    "# =============================================================================\n",
    "\n",
    "print_header('Section 7 setup (Recipes primary)')\n",
    "\n",
    "# --- Section 7 toggles ---\n",
    "RUN_RECIPE_SFT = bool(str(os.environ.get('RUN_RECIPE_SFT', 'true')).lower() in {'1','true','yes'})\n",
    "RUN_RECIPE_DPO = bool(str(os.environ.get('RUN_RECIPE_DPO', 'true')).lower() in {'1','true','yes'})\n",
    "RUN_CHECKPOINTLESS_RAW = os.environ.get('RUN_CHECKPOINTLESS', 'auto').strip().lower()\n",
    "if RUN_CHECKPOINTLESS_RAW in {'1','true','yes'}:\n",
    "    RUN_CHECKPOINTLESS = True\n",
    "elif RUN_CHECKPOINTLESS_RAW in {'0','false','no'}:\n",
    "    RUN_CHECKPOINTLESS = False\n",
    "else:\n",
    "    RUN_CHECKPOINTLESS = None  # auto\n",
    "\n",
    "RUN_INFER_BASELINE = bool(str(os.environ.get('RUN_INFER_BASELINE', 'true')).lower() in {'1','true','yes'})\n",
    "RUN_INFER_POST_SFT = bool(str(os.environ.get('RUN_INFER_POST_SFT', 'true')).lower() in {'1','true','yes'})\n",
    "RUN_INFER_POST_DPO = bool(str(os.environ.get('RUN_INFER_POST_DPO', 'true')).lower() in {'1','true','yes'})\n",
    "\n",
    "# --- Namespace / Kueue / Topology ---\n",
    "HYPERPOD_NAMESPACE = os.environ.get('HYPERPOD_NAMESPACE', 'hyperpod-ns-datascientist1').strip()\n",
    "KUEUE_QUEUE_NAME = os.environ.get('KUEUE_QUEUE_NAME', '').strip() or f\"{HYPERPOD_NAMESPACE}-localqueue\"\n",
    "KUEUE_PRIORITY_CLASS_TRAIN = os.environ.get('KUEUE_PRIORITY_CLASS', 'training-priority').strip()\n",
    "KUEUE_PRIORITY_CLASS_INFER = os.environ.get('KUEUE_PRIORITY_CLASS_INFER', 'inference-priority').strip()\n",
    "\n",
    "KUEUE_TOPOLOGY_LABEL_TRAIN = os.environ.get('KUEUE_TOPOLOGY_LABEL_TRAIN', 'topology.k8s.aws/network-node-layer-3').strip()\n",
    "KUEUE_TOPOLOGY_LABEL_INFER = os.environ.get('KUEUE_TOPOLOGY_LABEL_INFER', 'topology.kubernetes.io/zone').strip()\n",
    "KUEUE_TOPOLOGY_MODE_TRAIN = (os.environ.get('KUEUE_TOPOLOGY_MODE_TRAIN', 'preferred') or 'preferred').strip().lower()\n",
    "KUEUE_TOPOLOGY_MODE_INFER = (os.environ.get('KUEUE_TOPOLOGY_MODE_INFER', 'preferred') or 'preferred').strip().lower()\n",
    "\n",
    "if KUEUE_TOPOLOGY_MODE_TRAIN not in {'required', 'preferred'}:\n",
    "    KUEUE_TOPOLOGY_MODE_TRAIN = 'preferred'\n",
    "if KUEUE_TOPOLOGY_MODE_INFER not in {'required', 'preferred'}:\n",
    "    KUEUE_TOPOLOGY_MODE_INFER = 'preferred'\n",
    "\n",
    "# --- FSx pod-visible root ---\n",
    "POD_FSX_ROOT = f\"{FSX_POD_PREFIX}/{FSX_PVC_SUBPATH}/{FSX_BASE_DIRNAME}\"\n",
    "\n",
    "# --- Hugging Face cache paths (pod-visible) ---\n",
    "POD_HF_HOME = f\"{POD_FSX_ROOT}/hf-home\"\n",
    "POD_HF_HUB_CACHE = f\"{POD_HF_HOME}/hub\"\n",
    "POD_HF_DATASETS_CACHE = f\"{POD_HF_HOME}/datasets\"\n",
    "POD_HF_ASSETS_CACHE = f\"{POD_HF_HOME}/assets\"\n",
    "\n",
    "HF_ENV = [\n",
    "    {'name': 'HF_TOKEN', 'value': os.environ.get('HF_TOKEN','')},\n",
    "    {'name': 'HF_HOME', 'value': POD_HF_HOME},\n",
    "    {'name': 'HF_HUB_CACHE', 'value': POD_HF_HUB_CACHE},\n",
    "    {'name': 'HF_DATASETS_CACHE', 'value': POD_HF_DATASETS_CACHE},\n",
    "    {'name': 'HF_ASSETS_CACHE', 'value': POD_HF_ASSETS_CACHE},\n",
    "    {'name': 'TOKENIZERS_PARALLELISM', 'value': 'false'},\n",
    "]\n",
    "\n",
    "SECTION7_DIR = FSX_BASE_DIR / 'section7'\n",
    "RECIPE_WORK_DIR = SECTION7_DIR / 'recipes'\n",
    "DATA_WORK_DIR = SECTION7_DIR / 'data'\n",
    "INFER_WORK_DIR = SECTION7_DIR / 'inference'\n",
    "\n",
    "for d in [SECTION7_DIR, RECIPE_WORK_DIR, DATA_WORK_DIR, INFER_WORK_DIR]:\n",
    "    ensure_dir(d)\n",
    "\n",
    "# --- Instance type discovery (required by recipes launcher) ---\n",
    "import json as _json\n",
    "import shutil as _shutil\n",
    "\n",
    "def _find_recipes_src():\n",
    "    env_val = os.environ.get('RECIPES_SRC', '').strip() or os.environ.get('HYPERPOD_RECIPES_ROOT', '').strip()\n",
    "    if env_val:\n",
    "        p = Path(env_val).expanduser().resolve()\n",
    "        if p.exists():\n",
    "            return p\n",
    "        raise FileNotFoundError(f'RECIPES_SRC not found: {p}')\n",
    "    candidates = []\n",
    "    for base in [Path.cwd(), *Path.cwd().parents]:\n",
    "        cand = base / 'sagemaker-hyperpod-recipes'\n",
    "        candidates.append(cand)\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "    raise FileNotFoundError(\n",
    "        'Missing repo: searched ' + ', '.join(str(c) for c in candidates) +\n",
    "        '. Set RECIPES_SRC to the local clone path.'\n",
    "    )\n",
    "\n",
    "RECIPES_SRC = _find_recipes_src()\n",
    "\n",
    "GPU_PER_NODE_MAP = {\n",
    "    'p5.48xlarge': 8,\n",
    "    'p5e.48xlarge': 8,\n",
    "    'p5en.48xlarge': 8,\n",
    "    'p4d.24xlarge': 8,\n",
    "    'p4de.24xlarge': 8,\n",
    "    'g5.48xlarge': 8,\n",
    "    'g5.12xlarge': 4,\n",
    "}\n",
    "\n",
    "def _detect_instance_type_and_count():\n",
    "    env_val = os.environ.get('HYPERPOD_INSTANCE_TYPE', '').strip()\n",
    "    if env_val:\n",
    "        return env_val.replace('ml.', ''), None\n",
    "    if _shutil.which('kubectl'):\n",
    "        rc, out = run('kubectl get nodes -o json', check=False)\n",
    "        if rc == 0 and out:\n",
    "            try:\n",
    "                data = _json.loads(out)\n",
    "                items = data.get('items', [])\n",
    "                itype = None\n",
    "                count = 0\n",
    "                for n in items:\n",
    "                    labels = n.get('metadata', {}).get('labels', {})\n",
    "                    cur = labels.get('node.kubernetes.io/instance-type') or labels.get('beta.kubernetes.io/instance-type')\n",
    "                    if cur:\n",
    "                        cur = cur.replace('ml.', '')\n",
    "                        if itype is None:\n",
    "                            itype = cur\n",
    "                        if cur == itype:\n",
    "                            count += 1\n",
    "                return itype or 'p5.48xlarge', count or None\n",
    "            except Exception:\n",
    "                pass\n",
    "    return 'p5.48xlarge', None\n",
    "\n",
    "INSTANCE_TYPE, INSTANCE_TYPE_COUNT = _detect_instance_type_and_count()\n",
    "GPUS_PER_NODE = int(os.environ.get('GPUS_PER_NODE', '0')) or GPU_PER_NODE_MAP.get(INSTANCE_TYPE, 8)\n",
    "\n",
    "# --- Recipe IDs (primary path) ---\n",
    "RECIPE_SFT_ID = os.environ.get(\n",
    "    'RECIPE_SFT_ID',\n",
    "    'fine-tuning/llama/llmft_llama3_1_8b_instruct_seq4k_gpu_sft_lora'\n",
    ").strip()\n",
    "RECIPE_DPO_ID = os.environ.get(\n",
    "    'RECIPE_DPO_ID',\n",
    "    'fine-tuning/llama/llmft_llama3_1_8b_instruct_seq4k_gpu_dpo'\n",
    ").strip()\n",
    "RECIPE_CKPTLESS_ID = os.environ.get(\n",
    "    'RECIPE_CKPTLESS_ID',\n",
    "    'fine-tuning/llama/checkpointless_llama3_70b_lora'\n",
    ").strip()\n",
    "\n",
    "# --- Elastic settings for SFT/DPO ---\n",
    "LLMFT_ELASTIC = bool(str(os.environ.get('LLMFT_ELASTIC', 'true')).lower() in {'1','true','yes'})\n",
    "LLMFT_ELASTIC_MIN = int(os.environ.get('LLMFT_ELASTIC_MIN', '1'))\n",
    "LLMFT_ELASTIC_MAX = int(os.environ.get('LLMFT_ELASTIC_MAX', '16'))\n",
    "LLMFT_NUM_NODES = int(os.environ.get('LLMFT_NUM_NODES', str(LLMFT_ELASTIC_MIN)))\n",
    "\n",
    "# Enforce at least 1 node; set >=2 for multi-node-only runs\n",
    "require(LLMFT_NUM_NODES >= 1, 'LLMFT_NUM_NODES must be >=1; set >=2 for multi-node-only runs.')\n",
    "\n",
    "# --- Checkpointless settings ---\n",
    "CKPTLESS_NUM_NODES = int(os.environ.get('CKPTLESS_NUM_NODES', '2'))\n",
    "require(CKPTLESS_NUM_NODES >= 2, 'CKPTLESS_NUM_NODES must be >=2 for checkpointless peer recovery.')\n",
    "CKPTLESS_MAX_STEPS = int(os.environ.get('CKPTLESS_MAX_STEPS', '10'))\n",
    "CHECKPOINTLESS_MODEL_NAME_OR_PATH = os.environ.get(\n",
    "    'CHECKPOINTLESS_MODEL_NAME_OR_PATH',\n",
    "    'meta-llama/Llama-3-70B-Instruct'\n",
    ").strip()\n",
    "\n",
    "# --- Base model for inference (no shortcuts) ---\n",
    "BASE_MODEL_ID = os.environ.get('BASE_MODEL_ID', 'meta-llama/Llama-3.1-8B-Instruct').strip()\n",
    "\n",
    "# --- Run directories (pod-visible) ---\n",
    "POD_RESULTS_DIR = f\"{POD_FSX_ROOT}/section7/recipes/results\"\n",
    "POD_SFT_TRAIN_DIR = f\"{POD_RESULTS_DIR}/sft-{RUN_ID}\"\n",
    "POD_DPO_TRAIN_DIR = f\"{POD_RESULTS_DIR}/dpo-{RUN_ID}\"\n",
    "POD_CKPTLESS_DIR = f\"{POD_RESULTS_DIR}/checkpointless-{RUN_ID}\"\n",
    "\n",
    "POD_INFER_DIR = f\"{POD_FSX_ROOT}/section7/inference\"\n",
    "\n",
    "# --- Dataset paths (pod-visible) ---\n",
    "POD_SFT_DATA_TRAIN_DIR = f\"{POD_FSX_ROOT}/section7/data/llmft_sft/train\"\n",
    "POD_SFT_DATA_VAL_DIR = f\"{POD_FSX_ROOT}/section7/data/llmft_sft/val\"\n",
    "POD_DPO_DATA_TRAIN_DIR = f\"{POD_FSX_ROOT}/section7/data/llmft_dpo/train\"\n",
    "POD_DPO_DATA_VAL_DIR = f\"{POD_FSX_ROOT}/section7/data/llmft_dpo/val\"\n",
    "POD_CKPTLESS_DATA_DIR = f\"{POD_FSX_ROOT}/section7/data/checkpointless_mmap\"\n",
    "\n",
    "# --- Resolve container images ---\n",
    "\n",
    "LLMFT_IMAGE = os.environ.get('RECIPES_LLMFT_IMAGE', '').strip()\n",
    "if not LLMFT_IMAGE:\n",
    "    params_path = RECIPES_SRC / 'launcher' / 'recipe_templatization' / 'llmft' / 'llmft_regional_parameters.json'\n",
    "    with open(params_path, 'r') as f:\n",
    "        params = _json.load(f)\n",
    "    llama_keys = sorted([k for k in params.keys() if k.startswith('llmft_llama')])\n",
    "    image = ''\n",
    "    for k in llama_keys:\n",
    "        region_map = params.get(k, {}).get('k8s', {}).get('container_image', {}).get('prod', {})\n",
    "        if REGION in region_map:\n",
    "            image = region_map[REGION]\n",
    "            break\n",
    "    if not image:\n",
    "        region_map = params.get('llmft', {}).get('k8s', {}).get('container_image', {}).get('prod', {})\n",
    "        image = region_map.get(REGION, '')\n",
    "    LLMFT_IMAGE = image\n",
    "\n",
    "require(LLMFT_IMAGE, 'Could not resolve LLMFT container image. Set RECIPES_LLMFT_IMAGE env var.')\n",
    "\n",
    "CKPTLESS_IMAGE = os.environ.get('RECIPES_CHECKPOINTLESS_IMAGE', '').strip()\n",
    "available_ckpt_regions = []\n",
    "if not CKPTLESS_IMAGE:\n",
    "    ckpt_params_path = RECIPES_SRC / 'launcher' / 'recipe_templatization' / 'checkpointless' / 'checkpointless_regional_parameters.json'\n",
    "    with open(ckpt_params_path, 'r') as f:\n",
    "        ckpt_params = _json.load(f)\n",
    "    region_map = ckpt_params.get('checkpointless_nemo', {}).get('k8s', {}).get('container_image', {}).get('prod', {})\n",
    "    available_ckpt_regions = sorted(region_map.keys())\n",
    "    CKPTLESS_IMAGE = region_map.get('us-west-2', '')\n",
    "\n",
    "if RUN_CHECKPOINTLESS is None:\n",
    "    RUN_CHECKPOINTLESS = bool(CKPTLESS_IMAGE)\n",
    "    if not RUN_CHECKPOINTLESS:\n",
    "        print(\n",
    "            f'Checkpointless disabled: no image for region {REGION}. '\n",
    "            f'Available regions: {available_ckpt_regions}. '\n",
    "            'Set RECIPES_CHECKPOINTLESS_IMAGE or switch region.'\n",
    "        )\n",
    "elif RUN_CHECKPOINTLESS and not CKPTLESS_IMAGE:\n",
    "    raise ValueError(\n",
    "        f'No checkpointless container found for region {REGION}. '\n",
    "        f'Available regions: {available_ckpt_regions}. '\n",
    "        'Set RECIPES_CHECKPOINTLESS_IMAGE or switch region.'\n",
    "    )\n",
    "\n",
    "print('Section 7 toggles:')\n",
    "print({\n",
    "    'RUN_RECIPE_SFT': RUN_RECIPE_SFT,\n",
    "    'RUN_RECIPE_DPO': RUN_RECIPE_DPO,\n",
    "    'RUN_CHECKPOINTLESS': RUN_CHECKPOINTLESS,\n",
    "    'RUN_INFER_BASELINE': RUN_INFER_BASELINE,\n",
    "    'RUN_INFER_POST_SFT': RUN_INFER_POST_SFT,\n",
    "    'RUN_INFER_POST_DPO': RUN_INFER_POST_DPO,\n",
    "})\n",
    "print('Kueue/Topology:')\n",
    "print({\n",
    "    'HYPERPOD_NAMESPACE': HYPERPOD_NAMESPACE,\n",
    "    'KUEUE_QUEUE_NAME': KUEUE_QUEUE_NAME,\n",
    "    'KUEUE_PRIORITY_CLASS_TRAIN': KUEUE_PRIORITY_CLASS_TRAIN,\n",
    "    'KUEUE_PRIORITY_CLASS_INFER': KUEUE_PRIORITY_CLASS_INFER,\n",
    "    'KUEUE_TOPOLOGY_LABEL_TRAIN': KUEUE_TOPOLOGY_LABEL_TRAIN,\n",
    "    'KUEUE_TOPOLOGY_LABEL_INFER': KUEUE_TOPOLOGY_LABEL_INFER,\n",
    "    'KUEUE_TOPOLOGY_MODE_TRAIN': KUEUE_TOPOLOGY_MODE_TRAIN,\n",
    "    'KUEUE_TOPOLOGY_MODE_INFER': KUEUE_TOPOLOGY_MODE_INFER,\n",
    "})\n",
    "print('Instance type:', INSTANCE_TYPE, 'count:', INSTANCE_TYPE_COUNT)\n",
    "print('GPUs per node:', GPUS_PER_NODE)\n",
    "print('LLMFT nodes:', LLMFT_NUM_NODES, '(elastic:', LLMFT_ELASTIC, f'{LLMFT_ELASTIC_MIN}-{LLMFT_ELASTIC_MAX})')\n",
    "print('Checkpointless nodes:', CKPTLESS_NUM_NODES)\n",
    "print('POD_FSX_ROOT:', POD_FSX_ROOT)\n",
    "print('LLMFT_IMAGE:', LLMFT_IMAGE)\n",
    "print('CKPTLESS_IMAGE:', CKPTLESS_IMAGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bdd8254",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:08.997680Z",
     "iopub.status.busy": "2025-12-31T19:24:08.997393Z",
     "iopub.status.idle": "2025-12-31T19:24:16.914421Z",
     "shell.execute_reply": "2025-12-31T19:24:16.913464Z",
     "shell.execute_reply.started": "2025-12-31T19:24:08.997646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sync recipes repo to FSx + toolchain\n",
      "====================================\n",
      "RECIPES_FSX_DIR: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section7/recipes/sagemaker-hyperpod-recipes\n",
      "helm: v3.14.4+g81c902a\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 7b: Sync recipes repo to FSx + toolchain\n",
    "# =============================================================================\n",
    "\n",
    "print_header('Sync recipes repo to FSx + toolchain')\n",
    "\n",
    "RECIPES_FSX_DIR = RECIPE_WORK_DIR / 'sagemaker-hyperpod-recipes'\n",
    "\n",
    "# Copy recipes repo to FSx so pods can access the same code paths\n",
    "import shutil\n",
    "\n",
    "def _sync_tree(src: Path, dst: Path):\n",
    "    if not src.exists():\n",
    "        raise FileNotFoundError(f\"Missing repo: {src}\")\n",
    "    ensure_dir(dst)\n",
    "    if shutil.which('rsync'):\n",
    "        run(f\"rsync -a --delete --exclude='.git' {src}/ {dst}/\")\n",
    "    else:\n",
    "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "\n",
    "_sync_tree(RECIPES_SRC, RECIPES_FSX_DIR)\n",
    "\n",
    "print('RECIPES_FSX_DIR:', RECIPES_FSX_DIR)\n",
    "os.environ['HYPERPOD_RECIPES_ROOT'] = str(RECIPES_FSX_DIR)\n",
    "\n",
    "# Create/ensure venv for recipes launcher deps\n",
    "VENV_DIR = Path('.venv').resolve()\n",
    "VENV_PY = VENV_DIR / 'bin' / 'python'\n",
    "VENV_PIP = VENV_DIR / 'bin' / 'pip'\n",
    "if not VENV_PY.exists():\n",
    "    run('python -m venv .venv')\n",
    "\n",
    "# Install recipes deps into venv\n",
    "run(f\"{VENV_PIP} install -q -e {RECIPES_SRC}\")\n",
    "\n",
    "# Ensure helm is available (recipes launcher uses helm template)\n",
    "dest_dir = Path.home() / '.local' / 'bin'\n",
    "if str(dest_dir) not in os.environ.get('PATH', '').split(':'):\n",
    "    os.environ['PATH'] = f\"{dest_dir}:\" + os.environ.get('PATH', '')\n",
    "\n",
    "if not shutil.which('helm'):\n",
    "    helm_version = os.environ.get('HELM_VERSION', 'v3.14.4')\n",
    "    arch = 'amd64'\n",
    "    os_name = 'linux'\n",
    "    url = f\"https://get.helm.sh/helm-{helm_version}-{os_name}-{arch}.tar.gz\"\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tar_path = dest_dir / 'helm.tgz'\n",
    "    run(f\"curl -sSL {url} -o {tar_path}\")\n",
    "    run(f\"tar -xzf {tar_path} -C {dest_dir}\")\n",
    "    helm_bin = dest_dir / f\"{os_name}-{arch}\" / 'helm'\n",
    "    if helm_bin.exists():\n",
    "        run(f\"mv {helm_bin} {dest_dir / 'helm'}\")\n",
    "        run(f\"rm -rf {dest_dir / (os_name + '-' + arch)}\")\n",
    "    (dest_dir / 'helm').chmod(0o755)\n",
    "\n",
    "print('helm:', run('helm version --short', check=False)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a199792c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:16.915537Z",
     "iopub.status.busy": "2025-12-31T19:24:16.915234Z",
     "iopub.status.idle": "2025-12-31T19:24:29.035651Z",
     "shell.execute_reply": "2025-12-31T19:24:29.034744Z",
     "shell.execute_reply.started": "2025-12-31T19:24:16.915509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prepare datasets (SFT + DPO + checkpointless raw text)\n",
      "======================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT Train dir: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section7/data/llmft_sft/train rows: 2000\n",
      "SFT Val dir: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section7/data/llmft_sft/val rows: 200\n",
      "SFT Sample (tokenized): {'input_ids': [128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 15546, 1051, 279, 2911, 315, 279, 28812, 12471, 339, 7997, 10888, 11, 279, 5234, 6342, 315, 279, 5629, 11258, 304, 279, 4101, 362, 19508, 315, 20534, 323, 6785, 30, 128009, 128006, 78191, 128007, 271, 45030, 339, 279, 23251, 804, 11, 3842, 279, 18787, 11, 46092, 315, 279, 650, 1572, 11, 32866, 315, 279, 94672, 37080, 11, 435, 3746, 279, 59979, 11, 47809, 18787, 729, 71, 823, 11, 5340, 12490, 279, 24008, 11, 6385, 86452, 315, 279, 27206, 11, 426, 1105, 279, 15996, 261, 11, 3061, 683, 82, 279, 13585, 11, 2947, 285, 279, 48185, 11, 16344, 315, 279, 3816, 11940, 11, 4072, 18499, 18374, 27687, 11, 11035, 276, 7573, 12, 6670, 128009], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 45030, 339, 279, 23251, 804, 11, 3842, 279, 18787, 11, 46092, 315, 279, 650, 1572, 11, 32866, 315, 279, 94672, 37080, 11, 435, 3746, 279, 59979, 11, 47809, 18787, 729, 71, 823, 11, 5340, 12490, 279, 24008, 11, 6385, 86452, 315, 279, 27206, 11, 426, 1105, 279, 15996, 261, 11, 3061, 683, 82, 279, 13585, 11, 2947, 285, 279, 48185, 11, 16344, 315, 279, 3816, 11940, 11, 4072, 18499, 18374, 27687, 11, 11035, 276, 7573, 12, 6670, 128009]}\n",
      "DPO Train dir: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section7/data/llmft_dpo/train rows: 1993\n",
      "DPO Val dir: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section7/data/llmft_dpo/val rows: 200\n",
      "DPO Sample (tokenized): {'prompt_input_ids': [128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 10445, 1550, 7917, 13517, 16343, 3871, 311, 1893, 2324, 30, 128009, 128006, 78191, 128007, 271], 'chosen_input_ids': [18433, 872, 4382, 6956, 1198, 26333, 1198, 16681, 291, 304, 4040, 5627, 13, 220, 1628, 1606, 315, 11742, 11618, 16239, 33969, 323, 23963, 11, 3738, 13124, 315, 26333, 649, 3240, 311, 659, 12, 8629, 553, 1139, 8294, 14726, 11, 1093, 39654, 1481, 13382, 87352, 13, 220, 1628, 433, 753, 505, 1884, 87352, 430, 2324, 9778, 22763, 13], 'rejected_input_ids': [21526, 16343, 1606, 814, 8935, 505, 23915, 11, 2533, 814, 649, 617, 2753, 10937, 369, 5070, 555, 3318, 3871, 13]}\n",
      "Checkpointless raw text: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section7/data/checkpointless_raw/train.txt\n",
      "Checkpointless mapping file: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section7/data/checkpointless_raw/mapping.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 7c: Prepare datasets (SFT + DPO) and raw text for checkpointless\n",
    "# =============================================================================\n",
    "\n",
    "print_header('Prepare datasets (SFT + DPO + checkpointless raw text)')\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer\n",
    "import json as _json\n",
    "import re as _re\n",
    "\n",
    "LLMFT_SFT_DIR = DATA_WORK_DIR / 'llmft_sft'\n",
    "LLMFT_DPO_DIR = DATA_WORK_DIR / 'llmft_dpo'\n",
    "LLMFT_SFT_TRAIN_DIR = LLMFT_SFT_DIR / 'train'\n",
    "LLMFT_SFT_VAL_DIR = LLMFT_SFT_DIR / 'val'\n",
    "LLMFT_DPO_TRAIN_DIR = LLMFT_DPO_DIR / 'train'\n",
    "LLMFT_DPO_VAL_DIR = LLMFT_DPO_DIR / 'val'\n",
    "CKPTLESS_RAW_DIR = DATA_WORK_DIR / 'checkpointless_raw'\n",
    "\n",
    "for d in [\n",
    "    LLMFT_SFT_DIR,\n",
    "    LLMFT_DPO_DIR,\n",
    "    LLMFT_SFT_TRAIN_DIR,\n",
    "    LLMFT_SFT_VAL_DIR,\n",
    "    LLMFT_DPO_TRAIN_DIR,\n",
    "    LLMFT_DPO_VAL_DIR,\n",
    "    CKPTLESS_RAW_DIR,\n",
    "]:\n",
    "    ensure_dir(d)\n",
    "\n",
    "\n",
    "def _clear_json(dir_path):\n",
    "    for p in dir_path.glob('*.json'):\n",
    "        p.unlink()\n",
    "\n",
    "\n",
    "_clear_json(LLMFT_SFT_TRAIN_DIR)\n",
    "_clear_json(LLMFT_SFT_VAL_DIR)\n",
    "_clear_json(LLMFT_DPO_TRAIN_DIR)\n",
    "_clear_json(LLMFT_DPO_VAL_DIR)\n",
    "\n",
    "# --- Tokenizer setup ---\n",
    "TOKENIZER_ID = os.environ.get('TOKENIZER_ID', BASE_MODEL_ID).strip() or BASE_MODEL_ID\n",
    "HF_TOKEN = os.environ.get('HF_TOKEN') or None\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_ID, token=HF_TOKEN)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# Use right padding during training (collator will pad dynamically)\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "SFT_MAX_LEN = int(os.environ.get('SFT_MAX_LEN', '4096'))\n",
    "DPO_MAX_PROMPT_LEN = int(os.environ.get('DPO_MAX_PROMPT_LEN', '2048'))\n",
    "DPO_MAX_COMPLETION_LEN = int(os.environ.get('DPO_MAX_COMPLETION_LEN', '2048'))\n",
    "\n",
    "\n",
    "def _apply_chat_template(messages, add_generation_prompt):\n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "        )\n",
    "    # Fallback for tokenizers without chat templates\n",
    "    parts = []\n",
    "    for msg in messages:\n",
    "        role = msg.get('role', 'user')\n",
    "        prefix = 'User: ' if role == 'user' else 'Assistant: '\n",
    "        parts.append(prefix + msg.get('content', ''))\n",
    "    if add_generation_prompt:\n",
    "        parts.append('Assistant: ')\n",
    "    return '\\n\\n'.join(parts)\n",
    "\n",
    "\n",
    "def _tokenize(text, max_len=None, truncation_side='left'):\n",
    "    tokenizer.truncation_side = truncation_side\n",
    "    if max_len:\n",
    "        return tokenizer(text, add_special_tokens=False, truncation=True, max_length=max_len).input_ids\n",
    "    return tokenizer(text, add_special_tokens=False).input_ids\n",
    "\n",
    "\n",
    "# --- SFT dataset: Dolly -> tokenized JSON ---\n",
    "TRAIN_SAMPLES = int(os.environ.get('TRAIN_SAMPLES', '2000'))\n",
    "VAL_SAMPLES = int(os.environ.get('VAL_SAMPLES', '200'))\n",
    "SEED = int(os.environ.get('DATASET_SEED', '42'))\n",
    "\n",
    "SFT_DATASET = os.environ.get('SFT_DATASET', 'databricks/databricks-dolly-15k')\n",
    "SFT_DATAFILE = os.environ.get('SFT_DATAFILE', 'databricks-dolly-15k.jsonl')\n",
    "\n",
    "try:\n",
    "    raw = load_dataset(SFT_DATASET, split='train')\n",
    "except Exception as e:\n",
    "    print(f'WARNING: load_dataset failed for {SFT_DATASET} ({type(e).__name__}: {e})')\n",
    "    print('Falling back to direct JSONL download via huggingface_hub...')\n",
    "    data_path = hf_hub_download(\n",
    "        repo_id=SFT_DATASET,\n",
    "        filename=SFT_DATAFILE,\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "    raw = load_dataset('json', data_files=data_path, split='train')\n",
    "raw = raw.shuffle(seed=SEED)\n",
    "\n",
    "train_end = min(TRAIN_SAMPLES, len(raw))\n",
    "val_end = min(TRAIN_SAMPLES + VAL_SAMPLES, len(raw))\n",
    "\n",
    "train_raw = raw.select(range(0, train_end))\n",
    "val_raw = raw.select(range(train_end, val_end))\n",
    "\n",
    "\n",
    "def to_messages(example):\n",
    "    instruction = example.get('instruction', '')\n",
    "    context = example.get('context', '') or ''\n",
    "    response = example.get('response', '')\n",
    "    if context:\n",
    "        user_text = f\"{instruction}\\n\\nContext: {context}\"\n",
    "    else:\n",
    "        user_text = instruction\n",
    "    return {\n",
    "        'messages': [\n",
    "            {'role': 'user', 'content': user_text},\n",
    "            {'role': 'assistant', 'content': response},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "train_messages = train_raw.map(to_messages, remove_columns=train_raw.column_names)\n",
    "val_messages = val_raw.map(to_messages, remove_columns=val_raw.column_names)\n",
    "\n",
    "\n",
    "def _build_sft_record(example):\n",
    "    messages = example['messages']\n",
    "    prompt_text = _apply_chat_template(messages[:-1], add_generation_prompt=True)\n",
    "    full_text = _apply_chat_template(messages, add_generation_prompt=False)\n",
    "\n",
    "    prompt_ids = _tokenize(prompt_text, max_len=SFT_MAX_LEN, truncation_side='left')\n",
    "    full_ids = _tokenize(full_text, max_len=SFT_MAX_LEN, truncation_side='left')\n",
    "\n",
    "    labels = list(full_ids)\n",
    "    prompt_len = min(len(prompt_ids), len(labels))\n",
    "    for i in range(prompt_len):\n",
    "        labels[i] = -100\n",
    "\n",
    "    return {\n",
    "        'input_ids': list(full_ids),\n",
    "        'attention_mask': [1] * len(full_ids),\n",
    "        'labels': labels,\n",
    "    }\n",
    "\n",
    "\n",
    "train_sft = [\n",
    "    _build_sft_record(ex) for ex in train_messages\n",
    "]\n",
    "val_sft = [\n",
    "    _build_sft_record(ex) for ex in val_messages\n",
    "]\n",
    "\n",
    "SFT_TRAIN_JSON = LLMFT_SFT_TRAIN_DIR / 'train.json'\n",
    "SFT_VAL_JSON = LLMFT_SFT_VAL_DIR / 'val.json'\n",
    "\n",
    "with open(SFT_TRAIN_JSON, 'w') as f:\n",
    "    for ex in train_sft:\n",
    "        f.write(_json.dumps(ex) + '\\n')\n",
    "\n",
    "with open(SFT_VAL_JSON, 'w') as f:\n",
    "    for ex in val_sft:\n",
    "        f.write(_json.dumps(ex) + '\\n')\n",
    "\n",
    "print('SFT Train dir:', LLMFT_SFT_TRAIN_DIR, 'rows:', len(train_sft))\n",
    "print('SFT Val dir:', LLMFT_SFT_VAL_DIR, 'rows:', len(val_sft))\n",
    "print('SFT Sample (tokenized):', train_sft[0])\n",
    "\n",
    "# --- DPO dataset: Anthropic HH -> tokenized JSON ---\n",
    "DPO_DATASET = os.environ.get('DPO_DATASET', 'Anthropic/hh-rlhf')\n",
    "DPO_TRAIN_SAMPLES = int(os.environ.get('DPO_TRAIN_SAMPLES', '2000'))\n",
    "DPO_VAL_SAMPLES = int(os.environ.get('DPO_VAL_SAMPLES', '200'))\n",
    "\n",
    "raw_dpo = load_dataset(DPO_DATASET)\n",
    "train_split = raw_dpo['train']\n",
    "val_split = raw_dpo.get('test') or raw_dpo.get('validation') or raw_dpo['train']\n",
    "\n",
    "train_split = train_split.shuffle(seed=SEED)\n",
    "val_split = val_split.shuffle(seed=SEED)\n",
    "\n",
    "train_split = train_split.select(range(0, min(DPO_TRAIN_SAMPLES, len(train_split))))\n",
    "val_split = val_split.select(range(0, min(DPO_VAL_SAMPLES, len(val_split))))\n",
    "\n",
    "START_PROMPT_FORMAT = \"User: {body}\\n\\nAssistant: {response}\"\n",
    "PROMPT_CONTINUATION_FORMAT = \"{text}\\n\\nUser: {body}\\n\\nAssistant: {response}\"\n",
    "\n",
    "\n",
    "def _convert_anthropic(text):\n",
    "    split_string = text.split(\"\\n\\nHuman: \")\n",
    "    string_to_use = \"\"\n",
    "    prompt_string_to_use = \"\"\n",
    "    for item in split_string:\n",
    "        if len(item) == 0:\n",
    "            continue\n",
    "        output = item.split(\"\\n\\nAssistant: \")\n",
    "        if len(output) != 2:\n",
    "            return None\n",
    "        body, response = output\n",
    "        if len(string_to_use) == 0:\n",
    "            prompt_string_to_use = START_PROMPT_FORMAT.format(body=body, response=\"\")\n",
    "            string_to_use = START_PROMPT_FORMAT.format(body=body, response=response)\n",
    "        else:\n",
    "            prompt_string_to_use = PROMPT_CONTINUATION_FORMAT.format(text=string_to_use, body=body, response=\"\")\n",
    "            string_to_use = PROMPT_CONTINUATION_FORMAT.format(text=string_to_use, body=body, response=response)\n",
    "    return string_to_use, prompt_string_to_use.rstrip()\n",
    "\n",
    "\n",
    "def to_dpo(example):\n",
    "    # Prefer explicit prompt/chosen/rejected if present\n",
    "    if all(k in example for k in ('prompt', 'chosen', 'rejected')):\n",
    "        return {\n",
    "            'prompt': example['prompt'],\n",
    "            'chosen': example['chosen'],\n",
    "            'rejected': example['rejected'],\n",
    "        }\n",
    "    # Anthropic HH fallback (chosen/rejected are full conversations)\n",
    "    c = _convert_anthropic(example.get('chosen', ''))\n",
    "    r = _convert_anthropic(example.get('rejected', ''))\n",
    "    if c is None or r is None:\n",
    "        return {'prompt': None, 'chosen': None, 'rejected': None}\n",
    "    chosen_text, chosen_prompt = c\n",
    "    rejected_text, rejected_prompt = r\n",
    "    if chosen_prompt != rejected_prompt:\n",
    "        return {'prompt': None, 'chosen': None, 'rejected': None}\n",
    "    return {\n",
    "        'prompt': chosen_prompt,\n",
    "        'chosen': chosen_text,\n",
    "        'rejected': rejected_text,\n",
    "    }\n",
    "\n",
    "\n",
    "def _filter_none(ds):\n",
    "    return ds.filter(lambda x: x['prompt'] is not None, desc='filter-none')\n",
    "\n",
    "\n",
    "train_dpo = train_split.map(to_dpo, remove_columns=train_split.column_names)\n",
    "val_dpo = val_split.map(to_dpo, remove_columns=val_split.column_names)\n",
    "train_dpo = _filter_none(train_dpo)\n",
    "val_dpo = _filter_none(val_dpo)\n",
    "\n",
    "_UA_RE = _re.compile(r'(User|Assistant):')\n",
    "\n",
    "\n",
    "def _parse_dialogue(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    text = str(text).replace('\\r\\n', '\\n').strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    matches = list(_UA_RE.finditer(text))\n",
    "    if not matches:\n",
    "        return []\n",
    "    messages = []\n",
    "    for idx, match in enumerate(matches):\n",
    "        role = match.group(1).lower()\n",
    "        start = match.end()\n",
    "        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(text)\n",
    "        content = text[start:end].strip()\n",
    "        messages.append({'role': role, 'content': content})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def _strip_empty_tail(messages):\n",
    "    while messages and messages[-1]['role'] == 'assistant' and not messages[-1]['content']:\n",
    "        messages = messages[:-1]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def _extract_last_assistant(text):\n",
    "    messages = _parse_dialogue(text)\n",
    "    if messages:\n",
    "        for msg in reversed(messages):\n",
    "            if msg['role'] == 'assistant':\n",
    "                return msg.get('content', '').strip()\n",
    "    return str(text).strip()\n",
    "\n",
    "\n",
    "def _build_prompt_messages(prompt_text):\n",
    "    messages = _parse_dialogue(prompt_text)\n",
    "    if messages:\n",
    "        return _strip_empty_tail(messages)\n",
    "    return [{'role': 'user', 'content': str(prompt_text).strip()}]\n",
    "\n",
    "\n",
    "def _build_dpo_record(prompt_text, chosen_text, rejected_text):\n",
    "    prompt_messages = _build_prompt_messages(prompt_text)\n",
    "    chosen_resp = _extract_last_assistant(chosen_text)\n",
    "    rejected_resp = _extract_last_assistant(rejected_text)\n",
    "    if not chosen_resp or not rejected_resp:\n",
    "        return None\n",
    "\n",
    "    prompt_text_rendered = _apply_chat_template(prompt_messages, add_generation_prompt=True)\n",
    "    prompt_ids = _tokenize(prompt_text_rendered, max_len=DPO_MAX_PROMPT_LEN, truncation_side='left')\n",
    "    chosen_ids = _tokenize(chosen_resp, max_len=DPO_MAX_COMPLETION_LEN, truncation_side='right')\n",
    "    rejected_ids = _tokenize(rejected_resp, max_len=DPO_MAX_COMPLETION_LEN, truncation_side='right')\n",
    "\n",
    "    if not prompt_ids or not chosen_ids or not rejected_ids:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'prompt_input_ids': list(prompt_ids),\n",
    "        'chosen_input_ids': list(chosen_ids),\n",
    "        'rejected_input_ids': list(rejected_ids),\n",
    "    }\n",
    "\n",
    "\n",
    "train_dpo_tok = []\n",
    "for ex in train_dpo:\n",
    "    rec = _build_dpo_record(ex['prompt'], ex['chosen'], ex['rejected'])\n",
    "    if rec is not None:\n",
    "        train_dpo_tok.append(rec)\n",
    "\n",
    "val_dpo_tok = []\n",
    "for ex in val_dpo:\n",
    "    rec = _build_dpo_record(ex['prompt'], ex['chosen'], ex['rejected'])\n",
    "    if rec is not None:\n",
    "        val_dpo_tok.append(rec)\n",
    "\n",
    "DPO_TRAIN_JSON = LLMFT_DPO_TRAIN_DIR / 'train.json'\n",
    "DPO_VAL_JSON = LLMFT_DPO_VAL_DIR / 'val.json'\n",
    "\n",
    "with open(DPO_TRAIN_JSON, 'w') as f:\n",
    "    for ex in train_dpo_tok:\n",
    "        f.write(_json.dumps(ex) + '\\n')\n",
    "\n",
    "with open(DPO_VAL_JSON, 'w') as f:\n",
    "    for ex in val_dpo_tok:\n",
    "        f.write(_json.dumps(ex) + '\\n')\n",
    "\n",
    "print('DPO Train dir:', LLMFT_DPO_TRAIN_DIR, 'rows:', len(train_dpo_tok))\n",
    "print('DPO Val dir:', LLMFT_DPO_VAL_DIR, 'rows:', len(val_dpo_tok))\n",
    "print('DPO Sample (tokenized):', train_dpo_tok[0])\n",
    "\n",
    "# --- Raw text for checkpointless preprocessing ---\n",
    "CKPTLESS_RAW_TEXT = CKPTLESS_RAW_DIR / 'train.txt'\n",
    "with open(CKPTLESS_RAW_TEXT, 'w') as f:\n",
    "    for ex in train_messages:\n",
    "        msgs = ex['messages']\n",
    "        user = msgs[0]['content'] if msgs else ''\n",
    "        assistant = msgs[1]['content'] if len(msgs) > 1 else ''\n",
    "        f.write(f\"User: {user}\\nAssistant: {assistant}\\n\\n\")\n",
    "\n",
    "# Mapping file (one line per worker) for custom dataprep\n",
    "CKPTLESS_MAPPING = CKPTLESS_RAW_DIR / 'mapping.txt'\n",
    "CKPTLESS_MAPPING.write_text(str(CKPTLESS_RAW_TEXT) + '\\n')\n",
    "\n",
    "print('Checkpointless raw text:', CKPTLESS_RAW_TEXT)\n",
    "print('Checkpointless mapping file:', CKPTLESS_MAPPING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43607a9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:29.037304Z",
     "iopub.status.busy": "2025-12-31T19:24:29.036682Z",
     "iopub.status.idle": "2025-12-31T19:24:29.892316Z",
     "shell.execute_reply": "2025-12-31T19:24:29.891504Z",
     "shell.execute_reply.started": "2025-12-31T19:24:29.037280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configure recipes K8s settings + helpers\n",
      "========================================\n",
      "Inference script: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section7/inference/run_inference.py\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 7d: Configure recipes K8s settings + helpers\n",
    "# =============================================================================\n",
    "\n",
    "print_header('Configure recipes K8s settings + helpers')\n",
    "\n",
    "import time\n",
    "import shlex\n",
    "\n",
    "cfg_script = f\"\"\"\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "k8s_cfg_path = Path(r\"{RECIPES_FSX_DIR}\") / 'recipes_collection' / 'cluster' / 'k8s.yaml'\n",
    "with open(k8s_cfg_path, 'r') as f:\n",
    "    k8s_cfg = yaml.safe_load(f)\n",
    "\n",
    "k8s_cfg['namespace'] = r\"{HYPERPOD_NAMESPACE}\"\n",
    "if {HAS_TASK_GOV}:\n",
    "    if not isinstance(k8s_cfg.get('custom_labels'), dict):\n",
    "        k8s_cfg['custom_labels'] = dict()\n",
    "    k8s_cfg['custom_labels']['kueue.x-k8s.io/priority-class'] = r\"{KUEUE_PRIORITY_CLASS_TRAIN}\"\n",
    "k8s_cfg['queue_name'] = r\"{KUEUE_QUEUE_NAME}\" if {HAS_TASK_GOV} else None\n",
    "k8s_cfg['annotations'] = (\n",
    "    {{'kueue.x-k8s.io/podset-{KUEUE_TOPOLOGY_MODE_TRAIN}-topology': r\"{KUEUE_TOPOLOGY_LABEL_TRAIN}\"}}\n",
    "    if {HAS_TASK_GOV} and r\"{KUEUE_TOPOLOGY_LABEL_TRAIN}\" else None\n",
    ")\n",
    "\n",
    "k8s_cfg['persistent_volume_claims'] = [\n",
    "    {{'claimName': 'fsx-claim', 'mountPath': r\"{FSX_POD_PREFIX}\"}}\n",
    "]\n",
    "\n",
    "k8s_cfg['label_selector'] = {{\n",
    "    'required': {{\n",
    "        'sagemaker.amazonaws.com/node-health-status': ['Schedulable']\n",
    "    }}\n",
    "}}\n",
    "\n",
    "k8s_cfg['cleanPodPolicy'] = 'OnlyComplete'\n",
    "\n",
    "with open(k8s_cfg_path, 'w') as f:\n",
    "    yaml.safe_dump(k8s_cfg, f, sort_keys=False)\n",
    "\n",
    "print('Updated k8s.yaml:', k8s_cfg_path)\n",
    "\"\"\"\n",
    "\n",
    "script_path = SECTION7_DIR / f'update_k8s_{RUN_ID}.py'\n",
    "script_path.write_text(cfg_script)\n",
    "run(f\"{VENV_PY} {script_path}\")\n",
    "\n",
    "# --- Helpers for cluster jobs (inference, dataprep) ---\n",
    "if HAS_TRAINING_OPERATOR:\n",
    "    from sagemaker.hyperpod.training.hyperpod_pytorch_job import HyperPodPytorchJob\n",
    "    from sagemaker.hyperpod.training.config.hyperpod_pytorch_job_unified_config import (\n",
    "        ReplicaSpec, Template, Spec, Containers, Resources, RunPolicy\n",
    "    )\n",
    "from sagemaker.hyperpod.common.config.metadata import Metadata\n",
    "\n",
    "FSX_PVC_NAME = os.environ.get('FSX_PVC_NAME', 'fsx-claim').strip()\n",
    "\n",
    "def submit_simple_job(name, image, command, gpu='1', labels=None, annotations=None, env=None, use_hyperpodrun=True, nnodes=1, nproc_per_node=1):\n",
    "    require(HAS_TRAINING_OPERATOR, 'Training operator not available; cannot submit job.')\n",
    "    env = env or []\n",
    "\n",
    "    # Use hyperpodrun so the elastic agent can mark the job complete\n",
    "    cmd = command\n",
    "    if use_hyperpodrun:\n",
    "        # umask is applied in cmd_parts above\n",
    "        cmd_parts = [\n",
    "            'set -euo pipefail; ',\n",
    "            'export HOME=/tmp; ',\n",
    "            'mkdir -p /tmp/hp-rdzv; ',\n",
    "            'if ! command -v hyperpodrun >/dev/null 2>&1; then ',\n",
    "            '  echo \"hyperpodrun not found; installing hyperpod-elastic-agent\"; ',\n",
    "            '  python -m pip install -q hyperpod-elastic-agent; ',\n",
    "            'fi; ',\n",
    "            f'hyperpodrun --nnodes {nnodes} --nproc-per-node {nproc_per_node} ',\n",
    "            '--rdzv-conf resource_config_dir=/tmp/hp-rdzv ',\n",
    "            '--rdzv-backend hyperpod --no-python /bin/bash -lc ',\n",
    "        ]\n",
    "        cmd = ''.join(cmd_parts) + shlex.quote(command)\n",
    "\n",
    "    container = Containers(\n",
    "        name=name,\n",
    "        image=image,\n",
    "        image_pull_policy='Always',\n",
    "        command=['bash', '-lc'],\n",
    "        args=[cmd],\n",
    "        resources=Resources(requests={'nvidia.com/gpu': gpu}, limits={'nvidia.com/gpu': gpu}),\n",
    "        volume_mounts=[\n",
    "            {'name': 'fsx-claim', 'mount_path': FSX_POD_PREFIX},\n",
    "            {'name': 'dshm', 'mount_path': '/dev/shm'},\n",
    "        ],\n",
    "        env=env,\n",
    "    )\n",
    "\n",
    "    replica_spec = ReplicaSpec(\n",
    "        name='worker',\n",
    "        replicas=1,\n",
    "        template=Template(\n",
    "            metadata={'labels': labels or None, 'annotations': annotations or None},\n",
    "            spec=Spec(\n",
    "                containers=[container],\n",
    "                volumes=[\n",
    "                    {'name': 'fsx-claim', 'persistent_volume_claim': {'claim_name': FSX_PVC_NAME}},\n",
    "                    {'name': 'dshm', 'empty_dir': {'medium': 'Memory', 'size_limit': '128Gi'}},\n",
    "                ],\n",
    "                node_selector={'sagemaker.amazonaws.com/node-health-status': 'Schedulable'},\n",
    "                    ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    job = HyperPodPytorchJob(\n",
    "        metadata=Metadata(name=name, namespace=HYPERPOD_NAMESPACE, labels=labels or None, annotations=annotations or None),\n",
    "        nproc_per_node=str(nproc_per_node),\n",
    "        replica_specs=[replica_spec],\n",
    "        run_policy=RunPolicy(clean_pod_policy='OnlyComplete'),\n",
    "    )\n",
    "    job.create()\n",
    "    return job\n",
    "# Inference script (runs inside cluster)\n",
    "INFER_SCRIPT = INFER_WORK_DIR / 'run_inference.py'\n",
    "INFER_SCRIPT.write_text(\"\"\"import json, os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "BASE_MODEL = os.environ.get('BASE_MODEL')\n",
    "TRAINING_DIR = os.environ.get('TRAINING_DIR', '').strip()\n",
    "OUTPUT_PATH = os.environ.get('OUTPUT_PATH')\n",
    "PROMPTS = json.loads(os.environ.get('PROMPTS_JSON', '[]'))\n",
    "MAX_NEW_TOKENS = int(os.environ.get('MAX_NEW_TOKENS', '128'))\n",
    "\n",
    "if not BASE_MODEL:\n",
    "    raise ValueError('BASE_MODEL not set')\n",
    "if not OUTPUT_PATH:\n",
    "    raise ValueError('OUTPUT_PATH not set')\n",
    "\n",
    "# Find adapter or merged model in training dir\n",
    "adapter_dir = None\n",
    "merged_dir = None\n",
    "if TRAINING_DIR:\n",
    "    root = Path(TRAINING_DIR)\n",
    "    if root.exists():\n",
    "        for p in root.rglob('adapter_config.json'):\n",
    "            adapter_dir = p.parent\n",
    "            break\n",
    "        if adapter_dir is None:\n",
    "            for p in root.rglob('config.json'):\n",
    "                if any(p.parent.glob('*.safetensors')) or (p.parent / 'pytorch_model.bin').exists():\n",
    "                    merged_dir = p.parent\n",
    "                    break\n",
    "\n",
    "if adapter_dir:\n",
    "    from peft import PeftModel\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.bfloat16, device_map='auto')\n",
    "    model = PeftModel.from_pretrained(model, str(adapter_dir))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "elif merged_dir:\n",
    "    model = AutoModelForCausalLM.from_pretrained(str(merged_dir), torch_dtype=torch.bfloat16, device_map='auto')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(merged_dir), use_fast=True)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.bfloat16, device_map='auto')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def format_prompt(text):\n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        msgs = [{'role': 'user', 'content': text}]\n",
    "        return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "results = []\n",
    "for p in PROMPTS:\n",
    "    prompt = format_prompt(p)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    results.append({'prompt': p, 'output': text})\n",
    "\n",
    "Path(OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(OUTPUT_PATH, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print('Wrote', OUTPUT_PATH)\n",
    "\"\"\")\n",
    "\n",
    "print('Inference script:', INFER_SCRIPT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d468fed",
   "metadata": {},
   "source": [
    "### Section 7d.2 \u2014 Prefetch base model to FSx cache\n",
    "\n",
    "This downloads the base model into the **FSx Hugging Face cache** so cluster pods reuse it\n",
    "instead of re-downloading from the Hub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1b2397f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:29.894960Z",
     "iopub.status.busy": "2025-12-31T19:24:29.894736Z",
     "iopub.status.idle": "2025-12-31T19:24:31.425597Z",
     "shell.execute_reply": "2025-12-31T19:24:31.424856Z",
     "shell.execute_reply.started": "2025-12-31T19:24:29.894942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prefetch base model to FSx cache\n",
      "================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefetching model: meta-llama/Llama-3.1-8B-Instruct\n",
      "Cache dir: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/hf-home/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465adb8c74cb436a84f522a9f4b5a0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefetch complete.\n"
     ]
    }
   ],
   "source": [
    "print_header('Prefetch base model to FSx cache')\n",
    "\n",
    "PREFETCH_BASE_MODEL = bool(str(os.environ.get('PREFETCH_BASE_MODEL', 'true')).lower() in {'1','true','yes'})\n",
    "if PREFETCH_BASE_MODEL:\n",
    "    run('python -m pip install -q huggingface_hub transformers')\n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    model_id = os.environ.get('PREFETCH_MODEL_ID', BASE_MODEL_ID).strip()\n",
    "    revision = os.environ.get('PREFETCH_MODEL_REVISION', '').strip() or None\n",
    "\n",
    "    # Prefer the HF cache already configured earlier in the notebook\n",
    "    cache_dir = os.environ.get('HF_HUB_CACHE', '') or str(HF_HUB_CACHE_DIR)\n",
    "\n",
    "    require(os.environ.get('HF_TOKEN','').strip(), 'HF_TOKEN not set; required to prefetch gated model.')\n",
    "    print('Prefetching model:', model_id)\n",
    "    print('Cache dir:', cache_dir)\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        revision=revision,\n",
    "        cache_dir=cache_dir,\n",
    "        token=os.environ.get('HF_TOKEN') or None,\n",
    "        local_dir_use_symlinks=False,\n",
    "    )\n",
    "    print('Prefetch complete.')\n",
    "else:\n",
    "    print('Skipping prefetch (PREFETCH_BASE_MODEL=False).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "002c84f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:31.426748Z",
     "iopub.status.busy": "2025-12-31T19:24:31.426492Z",
     "iopub.status.idle": "2025-12-31T19:24:32.140567Z",
     "shell.execute_reply": "2025-12-31T19:24:32.139767Z",
     "shell.execute_reply.started": "2025-12-31T19:24:31.426719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline inference (base model)\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully submitted HyperPodPytorchJob 'infer-base-20260104-192842-98ffab'!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted baseline inference job: infer-base-20260104-192842-98ffab\n",
      "Note: first run may take ~10 minutes due to initial image pull.\n",
      "Next: run the status cell below to check progress and then open the shared folder.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 7e: Baseline inference + SFT LoRA (multi-node + elastic)\n",
    "# =============================================================================\n",
    "\n",
    "PROMPTS = [\n",
    "    \"Summarize the paragraph below in exactly 2 bullet points:\\nHyperPod schedules GPU pods using topology hints so colocated nodes share faster links and lower latency. It also reduces noisy-neighbor traffic by avoiding oversubscribed network paths during distributed training. Operators can trade strict placement for faster queue times depending on cluster load.\",\n",
    "    \"Extract the fields and output ONLY valid JSON with keys: name, company, role.\\nText: 'Dr. Mina Park joined Acme Robotics as Head of Research in 2024.'\",\n",
    "    \"Classify the sentiment as Positive, Neutral, or Negative. Output ONLY the label.\\nText: 'The deployment finished on time, but the logs were noisy and confusing.'\",\n",
    "    \"Rewrite as a short, professional email (3 sentences max):\\n'hey team we broke the build again pls fix asap'\",\n",
    "    \"Give a step-by-step list (numbered) to debug a failing Kubernetes job with no logs.\",\n",
    "]\n",
    "PROMPTS_JSON = _json.dumps(PROMPTS)\n",
    "\n",
    "# --- Baseline inference (base model) ---\n",
    "if RUN_INFER_BASELINE and HAS_TRAINING_OPERATOR:\n",
    "    print_header('Baseline inference (base model)')\n",
    "    infer_name = f\"infer-base-{RUN_ID}\"\n",
    "    out_path = f\"{POD_INFER_DIR}/baseline-{RUN_ID}.json\"\n",
    "\n",
    "    cmd = f\"python {POD_INFER_DIR}/run_inference.py\"\n",
    "    job = submit_simple_job(\n",
    "        name=infer_name,\n",
    "        image=LLMFT_IMAGE_CUSTOM,\n",
    "        command=cmd,\n",
    "        gpu='1',\n",
    "        labels={\n",
    "            'kueue.x-k8s.io/queue-name': KUEUE_QUEUE_NAME,\n",
    "            'kueue.x-k8s.io/priority-class': KUEUE_PRIORITY_CLASS_INFER,\n",
    "        } if HAS_TASK_GOV else None,\n",
    "        annotations={\n",
    "            f'kueue.x-k8s.io/podset-{KUEUE_TOPOLOGY_MODE_INFER}-topology': KUEUE_TOPOLOGY_LABEL_INFER,\n",
    "        } if HAS_TASK_GOV else None,\n",
    "        env=HF_ENV + [\n",
    "            {'name': 'BASE_MODEL', 'value': BASE_MODEL_ID},\n",
    "            {'name': 'TRAINING_DIR', 'value': ''},\n",
    "            {'name': 'OUTPUT_PATH', 'value': out_path},\n",
    "            {'name': 'PROMPTS_JSON', 'value': PROMPTS_JSON},\n",
    "        ],\n",
    "    )\n",
    "    os.environ['INFER_BASE_JOB_NAME'] = infer_name\n",
    "    os.environ['INFER_BASE_OUTPUT_PATH'] = out_path\n",
    "    os.environ['HYP_JOB_NAME'] = infer_name\n",
    "    print('Submitted baseline inference job:', infer_name)\n",
    "    print('Note: first run may take ~10 minutes due to initial image pull.')\n",
    "    print('Next: run the status cell below to check progress and then open the shared folder.')\n",
    "else:\n",
    "    if not RUN_INFER_BASELINE:\n",
    "        print('Skipping baseline inference (RUN_INFER_BASELINE=False).')\n",
    "    elif not HAS_TRAINING_OPERATOR:\n",
    "        print('Skipping baseline inference (training operator not available).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "477f4726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:29:26.021542Z",
     "iopub.status.busy": "2025-12-31T19:29:26.021174Z",
     "iopub.status.idle": "2025-12-31T19:29:35.006364Z",
     "shell.execute_reply": "2025-12-31T19:29:35.005399Z",
     "shell.execute_reply.started": "2025-12-31T19:29:26.021482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infer-base-20260104-192842-98ffabhyperpod-ns-datascientist1Created        0m             \n",
      "Job: infer-base-20260104-192842-98ffab\n",
      "Phase: Running\n",
      "Age: 0m\n",
      "Job still running. Re-run this cell in a few minutes.\n"
     ]
    }
   ],
   "source": [
    "# --- Baseline inference job status (async) ---\n",
    "job_name = os.environ.get('INFER_BASE_JOB_NAME', '').strip()\n",
    "out_path = os.environ.get('INFER_BASE_OUTPUT_PATH', '').strip()\n",
    "if not RUN_INFER_BASELINE:\n",
    "    print('Baseline inference not run; no job to check.')\n",
    "elif not job_name:\n",
    "    print('INFER_BASE_JOB_NAME not set. Run the submission cell first.')\n",
    "else:\n",
    "    namespace = os.environ.get('HYPERPOD_NAMESPACE', '').strip() or 'hyperpod-ns-datascientist1'\n",
    "    os.environ['HYPERPOD_NAMESPACE'] = namespace\n",
    "    os.environ['INFER_BASE_JOB_NAME'] = job_name\n",
    "    !hyp list hyp-pytorch-job -n ${HYPERPOD_NAMESPACE} | grep -F -- \"${INFER_BASE_JOB_NAME}\" || echo '(job not found)'\n",
    "    list_status, list_age, _line = get_hyp_list_status(job_name, namespace)\n",
    "    phase, conds = get_hyp_job_status(job_name, namespace)\n",
    "    phase = phase or list_status\n",
    "    print('Job:', job_name)\n",
    "    print('Phase:', phase if phase is not None else '(unavailable)')\n",
    "    if list_age:\n",
    "        print('Age:', list_age)\n",
    "    if conds:\n",
    "        print('Conditions:', conds)\n",
    "    if phase == 'Completed':\n",
    "        if out_path:\n",
    "            try:\n",
    "                results_path = FSX_SPACE_ROOT / Path(out_path).relative_to(POD_FSX_ROOT)\n",
    "            except Exception:\n",
    "                results_path = None\n",
    "            if results_path:\n",
    "                print('Job completed. Check the shared folder for:', results_path)\n",
    "            else:\n",
    "                print('Job completed. Check the shared folder for the output path:', out_path)\n",
    "        else:\n",
    "            print('Job completed. Check the shared folder for the inference output JSON.')\n",
    "    elif phase in {'Failed', 'Faulted'}:\n",
    "        print('Job failed. Re-run this cell or use hyp describe for details.')\n",
    "    elif phase is None:\n",
    "        print('Status unavailable (kubectl not ready?). You can use hyp describe in a terminal if needed (may show env vars).')\n",
    "    else:\n",
    "        print('Job still running. Re-run this cell in a few minutes.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ad776eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:41.063355Z",
     "iopub.status.busy": "2025-12-31T19:24:41.063040Z",
     "iopub.status.idle": "2025-12-31T19:24:43.792865Z",
     "shell.execute_reply": "2025-12-31T19:24:43.792008Z",
     "shell.execute_reply.started": "2025-12-31T19:24:41.063324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submit SFT LoRA recipe (multi-node + elastic)\n",
      "=============================================\n",
      "Recipe: fine-tuning/llama/llmft_llama3_1_8b_instruct_seq4k_gpu_sft_lora\n",
      "Command: /home/sagemaker-user/bobber/notebooks/.venv/bin/python -m launcher recipes=fine-tuning/llama/llmft_llama3_1_8b_instruct_seq4k_gpu_sft_lora base_results_dir=/fsx/fs-03b1953d09801303c/smus-nemo-smoke/section7/recipes/results container=<ACCOUNT_ID>.dkr.ecr.us-east-1.amazonaws.com/hyperpod-recipes-llmft-custom:llmft-v1.0.0-llama-custom cluster=k8s cluster_type=k8s instance_type=p4d.24xlarge recipes.run.name=llmft-sft-20260104-192842-98ffab recipes.run.hf_access_token=<HF_TOKEN> recipes.trainer.num_nodes=1 recipes.trainer.devices=8 recipes.elastic_policy.is_elastic=true recipes.elastic_policy.min_nodes=1 recipes.elastic_policy.max_nodes=16 recipes.training_config.model_config.model_save_name=Meta-Llama-3.1-8B-Instruct-SFT recipes.training_config.training_args.training_dir=/fsx/fs-03b1953d09801303c/smus-nemo-smoke/section7/recipes/results/sft-20260104-192842-98ffab +recipes.pre_script='[\"JOB_DATA_DIR=\\\"/fsx/fs-03b1953d09801303c/smus-nemo-smoke/section7/recipes/results/sft-20260104-192842-98ffab/data\\\"\", \"mkdir -p \\\"$JOB_DATA_DIR\\\"\", \"ln -sfn \\\"$JOB_DATA_DIR\\\" /data\"]' recipes.training_config.training_args.max_epochs=1 recipes.training_config.datasets.train_data.name=llmft-train recipes.training_config.datasets.train_data.file_path=/fsx/fs-03b1953d09801303c/smus-nemo-smoke/section7/data/llmft_sft/train recipes.training_config.datasets.val_data.name=llmft-val recipes.training_config.datasets.val_data.file_path=/fsx/fs-03b1953d09801303c/smus-nemo-smoke/section7/data/llmft_sft/val +env_vars.HF_TOKEN=<HF_TOKEN> +env_vars.HF_HOME=/fsx/fs-03b1953d09801303c/smus-nemo-smoke/hf-home +env_vars.HF_HUB_CACHE=/fsx/fs-03b1953d09801303c/smus-nemo-smoke/hf-home/hub +env_vars.HF_DATASETS_CACHE=/fsx/fs-03b1953d09801303c/smus-nemo-smoke/hf-home/datasets +env_vars.HF_ASSETS_CACHE=/fsx/fs-03b1953d09801303c/smus-nemo-smoke/hf-home/assets +env_vars.TOKENIZERS_PARALLELISM=false \n"
     ]
    }
   ],
   "source": [
    "# --- SFT LoRA recipe ---\n",
    "if RUN_RECIPE_SFT:\n",
    "    print_header('Submit SFT LoRA recipe (multi-node + elastic)')\n",
    "\n",
    "    SFT_RUN_NAME = f\"llmft-sft-{RUN_ID}\"\n",
    "    SFT_MODEL_SAVE_NAME = os.environ.get('SFT_MODEL_SAVE_NAME', 'Meta-Llama-3.1-8B-Instruct-SFT').strip()\n",
    "    SFT_MAX_EPOCHS = int(os.environ.get('SFT_MAX_EPOCHS', '1'))\n",
    "\n",
    "    SFT_PRE_SCRIPT = [\n",
    "        f'JOB_DATA_DIR=\"{POD_SFT_TRAIN_DIR}/data\"',\n",
    "        'mkdir -p \"$JOB_DATA_DIR\"',\n",
    "        'ln -sfn \"$JOB_DATA_DIR\" /data'\n",
    "    ]\n",
    "    SFT_PRE_SCRIPT_ARG = \"+recipes.pre_script=\" + shlex.quote(json.dumps(SFT_PRE_SCRIPT))\n",
    "\n",
    "\n",
    "    cmd = (\n",
    "        f\"{VENV_PY} -m launcher \"\n",
    "        f\"recipes={RECIPE_SFT_ID} \"\n",
    "        f\"base_results_dir={POD_RESULTS_DIR} \"\n",
    "        f\"container={LLMFT_IMAGE_CUSTOM} \"\n",
    "        f\"cluster=k8s cluster_type=k8s instance_type={INSTANCE_TYPE} \"\n",
    "        f\"recipes.run.name={SFT_RUN_NAME} \"\n",
    "        f\"recipes.run.hf_access_token={os.environ.get('HF_TOKEN','')} \"\n",
    "        f\"recipes.trainer.num_nodes={LLMFT_NUM_NODES} recipes.trainer.devices={GPUS_PER_NODE} \"\n",
    "        f\"recipes.elastic_policy.is_elastic={str(LLMFT_ELASTIC).lower()} \"\n",
    "        f\"recipes.elastic_policy.min_nodes={LLMFT_ELASTIC_MIN} \"\n",
    "        f\"recipes.elastic_policy.max_nodes={LLMFT_ELASTIC_MAX} \"\n",
    "        f\"recipes.training_config.model_config.model_save_name={SFT_MODEL_SAVE_NAME} \"\n",
    "        f\"recipes.training_config.training_args.training_dir={POD_SFT_TRAIN_DIR} \"\n",
    "        f\"{SFT_PRE_SCRIPT_ARG} \"\n",
    "        f\"recipes.training_config.training_args.max_epochs={SFT_MAX_EPOCHS} \"\n",
    "        f\"recipes.training_config.datasets.train_data.name=llmft-train \"\n",
    "        f\"recipes.training_config.datasets.train_data.file_path={POD_SFT_DATA_TRAIN_DIR} \"\n",
    "        f\"recipes.training_config.datasets.val_data.name=llmft-val \"\n",
    "        f\"recipes.training_config.datasets.val_data.file_path={POD_SFT_DATA_VAL_DIR} \"\n",
    "        f\"+env_vars.HF_TOKEN={os.environ.get('HF_TOKEN','')} \"\n",
    "        f\"+env_vars.HF_HOME={POD_HF_HOME} \"\n",
    "        f\"+env_vars.HF_HUB_CACHE={POD_HF_HUB_CACHE} \"\n",
    "        f\"+env_vars.HF_DATASETS_CACHE={POD_HF_DATASETS_CACHE} \"\n",
    "        f\"+env_vars.HF_ASSETS_CACHE={POD_HF_ASSETS_CACHE} \"\n",
    "        f\"+env_vars.TOKENIZERS_PARALLELISM=false \"\n",
    "    )\n",
    "\n",
    "    print('Recipe:', RECIPE_SFT_ID)\n",
    "    print('Command:', rdct_sens(cmd))\n",
    "    run(cmd)\n",
    "    # Persist the run name so the status cell can resolve the actual job name (suffix added by recipes)\n",
    "    os.environ['SFT_RUN_NAME'] = SFT_RUN_NAME\n",
    "    os.environ['SFT_JOB_NAME'] = ''  # clear any stale value so status cell can re-resolve\n",
    "else:\n",
    "    print('Skipping SFT recipe submission (RUN_RECIPE_SFT=False).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a823ebd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:24:43.793965Z",
     "iopub.status.busy": "2025-12-31T19:24:43.793698Z",
     "iopub.status.idle": "2025-12-31T19:24:43.801846Z",
     "shell.execute_reply": "2025-12-31T19:24:43.800968Z",
     "shell.execute_reply.started": "2025-12-31T19:24:43.793948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SFT recipe status (async)\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "# --- SFT recipe job status (async) ---\n",
    "print_header('SFT recipe status (async)')\n",
    "\n",
    "job_name = os.environ.get('SFT_JOB_NAME', '').strip()\n",
    "job_prefix = os.environ.get('SFT_RUN_NAME', '').strip()\n",
    "\n",
    "# If SFT_RUN_NAME not set, treat SFT_JOB_NAME as a prefix\n",
    "if not job_prefix and job_name:\n",
    "    job_prefix = job_name\n",
    "# Fallback to RUN_ID-derived prefix\n",
    "if not job_prefix:\n",
    "    try:\n",
    "        job_prefix = f\"llmft-sft-{RUN_ID}\"\n",
    "    except Exception:\n",
    "        job_prefix = ''\n",
    "\n",
    "def _parse_age_to_seconds(age):\n",
    "    if not age:\n",
    "        return None\n",
    "    age = age.strip()\n",
    "    total = 0\n",
    "    num = ''\n",
    "    for ch in age:\n",
    "        if ch.isdigit():\n",
    "            num += ch\n",
    "            continue\n",
    "        if not num:\n",
    "            continue\n",
    "        val = int(num)\n",
    "        num = ''\n",
    "        if ch == 's':\n",
    "            total += val\n",
    "        elif ch == 'm':\n",
    "            total += val * 60\n",
    "        elif ch == 'h':\n",
    "            total += val * 3600\n",
    "        elif ch == 'd':\n",
    "            total += val * 86400\n",
    "    return total if total > 0 else None\n",
    "\n",
    "def _find_latest_hyp_job_by_prefix_with_hyp(prefix, namespace):\n",
    "    if not prefix:\n",
    "        return None\n",
    "    rc, out_txt = run(f\"hyp list hyp-pytorch-job -n {namespace}\", check=False)\n",
    "    if rc != 0 or not out_txt:\n",
    "        return None\n",
    "    candidates = []\n",
    "    for line in out_txt.splitlines():\n",
    "        parsed = _parse_hyp_list_line(line, namespace)\n",
    "        if not parsed:\n",
    "            continue\n",
    "        name, _status, age, _raw = parsed\n",
    "        if prefix in name:\n",
    "            age_s = _parse_age_to_seconds(age)\n",
    "            candidates.append((age_s, name))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    candidates.sort(key=lambda x: (x[0] is None, x[0]))\n",
    "    return candidates[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cffe92d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:34:31.223998Z",
     "iopub.status.busy": "2025-12-31T19:34:31.223732Z",
     "iopub.status.idle": "2025-12-31T19:34:32.851379Z",
     "shell.execute_reply": "2025-12-31T19:34:32.850567Z",
     "shell.execute_reply.started": "2025-12-31T19:34:31.223977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SFT inference (guarded)\n",
      "=======================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to resolve: /fsx/fs-03b1953d09801303c/smus-nemo-smoke/section7/recipes/results/sft-20260104-192842-98ffab\n",
      "FSX_BASE_DIR: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke\n",
      "POD_FSX_ROOT: /fsx/fs-03b1953d09801303c/smus-nemo-smoke\n",
      "SFT output path not found on FSx: /fsx/fs-03b1953d09801303c/smus-nemo-smoke/section7/recipes/results/sft-20260104-192842-98ffab\n"
     ]
    }
   ],
   "source": [
    "# --- SFT inference (guarded; requires completed SFT + adapter artifacts) ---\n",
    "print_header('SFT inference (guarded)')\n",
    "\n",
    "def _resolve_fsx_path(pod_path):\n",
    "    candidates = []\n",
    "    # Primary: map POD path to FSx space via POD_FSX_ROOT\n",
    "    try:\n",
    "        candidates.append(FSX_SPACE_ROOT / Path(pod_path).relative_to(POD_FSX_ROOT))\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: map using FSX_BASE_DIRNAME in the path\n",
    "    try:\n",
    "        if 'FSX_BASE_DIRNAME' in globals() and FSX_BASE_DIRNAME and FSX_BASE_DIRNAME in pod_path:\n",
    "            rel = pod_path.split(FSX_BASE_DIRNAME, 1)[1].lstrip('/')\n",
    "            candidates.append(FSX_BASE_DIR / rel)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Return first existing candidate\n",
    "    for c in candidates:\n",
    "        if c and c.exists():\n",
    "            return c\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "if not RUN_INFER_POST_SFT:\n",
    "\n",
    "    def _resolve_fsx_path(pod_path):\n",
    "        candidates = []\n",
    "        # Primary: map POD path to FSx space via POD_FSX_ROOT\n",
    "        try:\n",
    "            candidates.append(FSX_SPACE_ROOT / Path(pod_path).relative_to(POD_FSX_ROOT))\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Fallback: map using FSX_BASE_DIRNAME in the path\n",
    "        try:\n",
    "            if 'FSX_BASE_DIRNAME' in globals() and FSX_BASE_DIRNAME and FSX_BASE_DIRNAME in pod_path:\n",
    "                rel = pod_path.split(FSX_BASE_DIRNAME, 1)[1].lstrip('/')\n",
    "                candidates.append(FSX_BASE_DIR / rel)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Return first existing candidate\n",
    "        for c in candidates:\n",
    "            if c and c.exists():\n",
    "                return c\n",
    "        return candidates[0] if candidates else None\n",
    "    print('Skipping SFT inference (RUN_INFER_POST_SFT=False).')\n",
    "else:\n",
    "    # Resolve job name if we have it\n",
    "    namespace = os.environ.get('HYPERPOD_NAMESPACE', '').strip() or 'hyperpod-ns-datascientist1'\n",
    "    os.environ['HYPERPOD_NAMESPACE'] = namespace\n",
    "\n",
    "    sft_job = os.environ.get('SFT_JOB_NAME', '').strip()\n",
    "    sft_prefix = os.environ.get('SFT_RUN_NAME', '').strip()\n",
    "    if not sft_job and sft_prefix:\n",
    "        sft_job = _find_latest_hyp_job_by_prefix_with_hyp(sft_prefix, namespace)\n",
    "    if sft_job:\n",
    "        os.environ['SFT_JOB_NAME'] = sft_job\n",
    "\n",
    "    # Check job completion if possible\n",
    "    phase = None\n",
    "    if sft_job:\n",
    "        phase, _conds = get_hyp_job_status(sft_job, namespace)\n",
    "    if phase and phase != 'Completed':\n",
    "        print(f'SFT job not completed yet (phase={phase}). Skipping inference.')\n",
    "    else:\n",
    "        # Sense-check outputs on FSx\n",
    "        out_dir = POD_SFT_TRAIN_DIR if 'POD_SFT_TRAIN_DIR' in globals() else os.environ.get('POD_SFT_TRAIN_DIR', '').strip()\n",
    "        if not out_dir:\n",
    "            print('Missing POD_SFT_TRAIN_DIR; cannot locate SFT outputs.')\n",
    "        else:\n",
    "            local_out_dir = _resolve_fsx_path(out_dir)\n",
    "            if not local_out_dir or not local_out_dir.exists():\n",
    "                print('Tried to resolve:', out_dir)\n",
    "                print('FSX_BASE_DIR:', FSX_BASE_DIR if 'FSX_BASE_DIR' in globals() else '(unset)')\n",
    "                print('POD_FSX_ROOT:', POD_FSX_ROOT if 'POD_FSX_ROOT' in globals() else '(unset)')\n",
    "                print('SFT output path not found on FSx:', out_dir)\n",
    "            else:\n",
    "                # adapter artifacts check\n",
    "                adapter_cfgs = list(local_out_dir.rglob('adapter_config.json'))\n",
    "                adapter_weights = list(local_out_dir.rglob('adapter_model*.safetensors'))\n",
    "                if not adapter_cfgs or not adapter_weights:\n",
    "                    print('SFT outputs missing adapter artifacts. Found:')\n",
    "                    print('  adapter_config.json:', len(adapter_cfgs))\n",
    "                    print('  adapter_model*.safetensors:', len(adapter_weights))\n",
    "                    print('Skipping inference until artifacts are present.')\n",
    "                else:\n",
    "                    infer_name = f\"infer-sft-{RUN_ID}\"\n",
    "                    out_path = f\"{POD_INFER_DIR}/sft-{RUN_ID}.json\"\n",
    "                    cmd = f\"python {POD_INFER_DIR}/run_inference.py\"\n",
    "                    job = submit_simple_job(\n",
    "                        name=infer_name,\n",
    "                        image=LLMFT_IMAGE_CUSTOM,\n",
    "                        command=cmd,\n",
    "                        gpu='1',\n",
    "                        labels={\n",
    "                            'kueue.x-k8s.io/queue-name': KUEUE_QUEUE_NAME,\n",
    "                            'kueue.x-k8s.io/priority-class': KUEUE_PRIORITY_CLASS_INFER,\n",
    "                        } if HAS_TASK_GOV else None,\n",
    "                        annotations={\n",
    "                            f'kueue.x-k8s.io/podset-{KUEUE_TOPOLOGY_MODE_INFER}-topology': KUEUE_TOPOLOGY_LABEL_INFER,\n",
    "                        } if HAS_TASK_GOV else None,\n",
    "                        env=HF_ENV + [\n",
    "                            {'name': 'BASE_MODEL', 'value': BASE_MODEL_ID},\n",
    "                            {'name': 'TRAINING_DIR', 'value': out_dir},\n",
    "                            {'name': 'OUTPUT_PATH', 'value': out_path},\n",
    "                            {'name': 'PROMPTS_JSON', 'value': PROMPTS_JSON},\n",
    "                            {'name': 'MAX_NEW_TOKENS', 'value': '128'},\n",
    "                        ],\n",
    "                    )\n",
    "                    os.environ['INFER_SFT_JOB_NAME'] = infer_name\n",
    "                    os.environ['INFER_SFT_OUTPUT_PATH'] = out_path\n",
    "                    os.environ['HYP_JOB_NAME'] = infer_name\n",
    "                    print('Submitted SFT inference job:', infer_name)\n",
    "                    print('Output will be written to:', out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "675cd84a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T19:45:04.350381Z",
     "iopub.status.busy": "2025-12-31T19:45:04.350127Z",
     "iopub.status.idle": "2025-12-31T19:45:04.364830Z",
     "shell.execute_reply": "2025-12-31T19:45:04.363966Z",
     "shell.execute_reply.started": "2025-12-31T19:45:04.350361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compare baseline vs SFT outputs\n",
      "===============================\n",
      "Missing output paths. Run baseline + SFT inference first.\n"
     ]
    }
   ],
   "source": [
    "# --- Compare baseline vs SFT outputs (readable) ---\n",
    "print_header('Compare baseline vs SFT outputs')\n",
    "\n",
    "base_out = os.environ.get('INFER_BASE_OUTPUT_PATH', '').strip()\n",
    "sft_out = os.environ.get('INFER_SFT_OUTPUT_PATH', '').strip()\n",
    "\n",
    "if not base_out or not sft_out:\n",
    "    print('Missing output paths. Run baseline + SFT inference first.')\n",
    "else:\n",
    "    def _local(pod_path):\n",
    "        candidates = []\n",
    "        try:\n",
    "            candidates.append(FSX_SPACE_ROOT / Path(pod_path).relative_to(POD_FSX_ROOT))\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            if \"FSX_BASE_DIRNAME\" in globals() and FSX_BASE_DIRNAME and FSX_BASE_DIRNAME in pod_path:\n",
    "                rel = pod_path.split(FSX_BASE_DIRNAME, 1)[1].lstrip(\"/\")\n",
    "                candidates.append(FSX_BASE_DIR / rel)\n",
    "        except Exception:\n",
    "            pass\n",
    "        for c in candidates:\n",
    "            if c and c.exists():\n",
    "                return c\n",
    "        return candidates[0] if candidates else None\n",
    "\n",
    "    base_path = _local(base_out)\n",
    "    sft_path = _local(sft_out)\n",
    "\n",
    "    if not base_path.exists() or not sft_path.exists():\n",
    "        print('Output file(s) not found:')\n",
    "        print('  baseline:', base_path)\n",
    "        print('  sft     :', sft_path)\n",
    "    else:\n",
    "        import json\n",
    "\n",
    "        with open(base_path) as f:\n",
    "            base_rows = json.load(f)\n",
    "        with open(sft_path) as f:\n",
    "            sft_rows = json.load(f)\n",
    "\n",
    "        def _out(row):\n",
    "            return row.get('output') or row.get('response') or row.get('text') or ''\n",
    "\n",
    "        def _clean(text):\n",
    "            if not text:\n",
    "                return ''\n",
    "            t = text.strip()\n",
    "            low = t.lower()\n",
    "            # Strip system/user preambles if they leaked into outputs\n",
    "            if 'assistant' in low:\n",
    "                idx = low.rfind('assistant')\n",
    "                t = t[idx + len('assistant'):]\n",
    "            elif 'user' in low:\n",
    "                idx = low.rfind('user')\n",
    "                t = t[idx + len('user'):]\n",
    "            t = t.replace('Cutting Knowledge Date:', 'Knowledge Date:')\n",
    "            return t.strip()\n",
    "\n",
    "        sft_map = {r.get('prompt'): _out(r) for r in sft_rows}\n",
    "\n",
    "        for i, r in enumerate(base_rows, 1):\n",
    "            prompt = r.get('prompt', '')\n",
    "            base_text = _clean(_out(r))\n",
    "            sft_text = _clean(sft_map.get(prompt, ''))\n",
    "\n",
    "            print(f\"PROMPT {i}:{prompt}\")\n",
    "            print(\"BASELINE:\" + (base_text or \"(empty)\"))\n",
    "            print(\"SFT (LoRA):\" + (sft_text or \"(empty)\"))\n",
    "            print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c79b84-5b30-43cb-91b0-388af8160f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d150d6c",
   "metadata": {},
   "source": [
    "# Section 8 \u2014 Function-Calling SFT (Qwen2.5-Coder-7B/14B-Instruct)\n",
    "\n",
    "This section builds on Section 7 and adds a dedicated function-calling SFT stage **using NeMo 2.0**.\n",
    "\n",
    "Goals:\n",
    "- Prepare XLAM function-calling data (train/val/test)\n",
    "- Prefetch the base model to FSx\n",
    "- Full fine-tune Qwen2.5-Coder-7B/14B-Instruct with NeMo 2.0\n",
    "- Produce splits for evaluation and error analysis\n",
    "\n",
    "Set env vars to control behavior:\n",
    "- FC_QWEN_SIZE (7B or 14B; case-insensitive)\n",
    "- RUN_FC_PREP, RUN_FC_SFT, RUN_FC_PREFETCH\n",
    "- RUN_FC_EVAL_BASELINE, RUN_FC_EVAL_SFT\n",
    "- RUN_FC_NEMO2_PREP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a239461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 8 setup (Function-calling SFT)\n",
      "======================================\n",
      "Section 8 toggles:\n",
      "{'RUN_FC_PREP': True, 'RUN_FC_SFT': True, 'RUN_FC_IMPORT': True, 'RUN_FC_EVAL_BASELINE': True, 'RUN_FC_EVAL_SFT': True, 'RUN_FC_NEMO2_PREP': True}\n",
      "Dataset:\n",
      "{'FC_DATASET_ID': 'Salesforce/xlam-function-calling-60k', 'FC_TRAIN_SIZE': 54000, 'FC_VAL_SIZE': 3000, 'FC_TEST_SIZE': 3000, 'FC_TOTAL_LIMIT': 0}\n",
      "Model/recipe:\n",
      "{'FC_QWEN_SIZE': '7B', 'FC_BASE_MODEL_ID': 'Qwen/Qwen2.5-Coder-7B-Instruct', 'FC_TOKENIZER_ID': 'Qwen/Qwen2.5-Coder-7B-Instruct', 'FC_RECIPE_SFT_ID': 'fine-tuning/qwen/nemo2_qwen2_5_7b_instruct_seq4k_gpu_sft_fft', 'FC_RECIPE_IMPORT_ID': 'fine-tuning/qwen/nemo2_qwen2_5_7b_instruct_import'}\n",
      "Training:\n",
      "{'FC_TRAIN_NUM_NODES': 2, 'FC_GPUS_PER_NODE': 8, 'FC_MICRO_BS': 1, 'FC_TRAIN_BS': 32, 'FC_LR': 1e-05, 'FC_MAX_EPOCHS': 2, 'FC_MAX_LEN': 8192, 'FC_WARMUP_RATIO': 0.1}\n",
      "Elastic (NeMo 2.0 SFT):\n",
      "{'FC_ELASTIC': True, 'FC_ELASTIC_MIN': 1, 'FC_ELASTIC_MAX': 4, 'FC_ELASTIC_REPLICA_INCREMENT_STEP': 1}\n",
      "NeMo 2.0 SFT:\n",
      "{'FC_NEMO2_SEQ_LEN': 8192, 'FC_NEMO2_MICRO_BS': 1, 'FC_NEMO2_GLOBAL_BS': 32, 'FC_NEMO2_MAX_STEPS': 50, 'FC_NEMO2_LR': 1e-05, 'FC_NEMO2_WARMUP_STEPS': 5, 'FC_NEMO2_VAL_CHECK_INTERVAL': 1.0, 'FC_NEMO2_LOG_EVERY_N_STEPS': 1, 'FC_NEMO2_NUM_WORKERS': 2, 'FC_NEMO2_PEFT': 'none', 'FC_NEMO2_RECOMPUTE_GRANULARITY': 'selective', 'FC_NEMO2_RECOMPUTE_METHOD': 'none', 'FC_NEMO2_RECOMPUTE_NUM_LAYERS': 0, 'FC_NEMO2_GRAD_ACCUM_FUSION': False}\n",
      "Paths:\n",
      "{'FC_DATA_TRAIN_DIR': '/home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/data/train', 'FC_DATA_VAL_DIR': '/home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/data/val', 'FC_DATA_TEST_DIR': '/home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/data/test', 'FC_NEMO2_DATA_DIR': '/home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/data_nemo2', 'FC_NEMO2_IMPORT_OUTPUT': '/home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/imports/qwen25_7b_instruct.nemo', 'POD_FC_DATA_TRAIN_DIR': '/fsx/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/data/train', 'POD_FC_NEMO2_IMPORT_OUTPUT': '/fsx/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/imports/qwen25_7b_instruct.nemo', 'POD_FC_RESULTS_DIR': '/fsx/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/results'}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8a: Shared config + toggles (Function-calling SFT)\n",
    "# =============================================================================\n",
    "\n",
    "print_header('Section 8 setup (Function-calling SFT)')\n",
    "\n",
    "import math\n",
    "\n",
    "# --- Section 8 toggles ---\n",
    "RUN_FC_PREP = bool(str(os.environ.get('RUN_FC_PREP', 'true')).lower() in {'1','true','yes'})\n",
    "RUN_FC_SFT = bool(str(os.environ.get('RUN_FC_SFT', 'true')).lower() in {'1','true','yes'})\n",
    "RUN_FC_IMPORT = bool(str(os.environ.get('RUN_FC_IMPORT', 'true')).lower() in {'1','true','yes'})\n",
    "RUN_FC_PREFETCH = bool(str(os.environ.get('RUN_FC_PREFETCH', 'true')).lower() in {'1','true','yes'})\n",
    "RUN_FC_EVAL_BASELINE = bool(str(os.environ.get('RUN_FC_EVAL_BASELINE', 'true')).lower() in {'1','true','yes'})\n",
    "RUN_FC_EVAL_SFT = bool(str(os.environ.get('RUN_FC_EVAL_SFT', 'true')).lower() in {'1','true','yes'})\n",
    "RUN_FC_NEMO2_PREP = bool(str(os.environ.get('RUN_FC_NEMO2_PREP', 'true')).lower() in {'1','true','yes'})\n",
    "\n",
    "# --- Dataset + split settings ---\n",
    "FC_DATASET_ID = os.environ.get('FC_DATASET_ID', 'Salesforce/xlam-function-calling-60k').strip()\n",
    "FC_SEED = int(os.environ.get('FC_SEED', '42'))\n",
    "\n",
    "# Size controls (0 = use full split)\n",
    "FC_TRAIN_SIZE = int(os.environ.get('FC_TRAIN_SIZE', '54000'))\n",
    "FC_VAL_SIZE = int(os.environ.get('FC_VAL_SIZE', '3000'))\n",
    "FC_TEST_SIZE = int(os.environ.get('FC_TEST_SIZE', '3000'))\n",
    "FC_TOTAL_LIMIT = int(os.environ.get('FC_TOTAL_LIMIT', '0'))  # optional cap for quick tests\n",
    "\n",
    "# --- Qwen size selector (7B/14B) ---\n",
    "FC_QWEN_SIZE = os.environ.get('FC_QWEN_SIZE', '7B').strip()\n",
    "if not FC_QWEN_SIZE:\n",
    "    FC_QWEN_SIZE = '7B'\n",
    "_fc_qwen_size_norm = FC_QWEN_SIZE.lower()\n",
    "if _fc_qwen_size_norm in {'7', '7b'}:\n",
    "    _fc_qwen_size_norm = '7b'\n",
    "elif _fc_qwen_size_norm in {'14', '14b'}:\n",
    "    _fc_qwen_size_norm = '14b'\n",
    "else:\n",
    "    raise ValueError(\"FC_QWEN_SIZE must be '7B' or '14B' (case-insensitive).\")\n",
    "FC_QWEN_SIZE = _fc_qwen_size_norm\n",
    "FC_QWEN_SIZE_LABEL = FC_QWEN_SIZE.upper()\n",
    "\n",
    "# --- Model + recipe ---\n",
    "FC_DEFAULT_MODEL_ID = f\"Qwen/Qwen2.5-Coder-{FC_QWEN_SIZE_LABEL}-Instruct\"\n",
    "FC_BASE_MODEL_ID = os.environ.get('FC_BASE_MODEL_ID', FC_DEFAULT_MODEL_ID).strip()\n",
    "FC_TOKENIZER_ID = os.environ.get('FC_TOKENIZER_ID', FC_BASE_MODEL_ID).strip()\n",
    "FC_RECIPE_SFT_ID = os.environ.get(\n",
    "    'FC_RECIPE_SFT_ID',\n",
    "    f'fine-tuning/qwen/nemo2_qwen2_5_{FC_QWEN_SIZE}_instruct_seq4k_gpu_sft_fft'\n",
    ").strip()\n",
    "\n",
    "FC_RECIPE_IMPORT_ID = os.environ.get(\n",
    "    'FC_RECIPE_IMPORT_ID',\n",
    "    f'fine-tuning/qwen/nemo2_qwen2_5_{FC_QWEN_SIZE}_instruct_import'\n",
    ").strip()\n",
    "\n",
    "# --- Prefetch settings ---\n",
    "FC_PREFETCH_MODEL_ID = os.environ.get('FC_PREFETCH_MODEL_ID', FC_BASE_MODEL_ID).strip()\n",
    "FC_PREFETCH_REVISION = os.environ.get('FC_PREFETCH_REVISION', '').strip() or None\n",
    "\n",
    "\n",
    "# --- Training hyperparams (defaults tuned for longer context) ---\n",
    "FC_MAX_LEN = int(os.environ.get('FC_MAX_LEN', '8192'))\n",
    "FC_LR = float(os.environ.get('FC_LR', '1e-5'))\n",
    "FC_MAX_EPOCHS = int(os.environ.get('FC_MAX_EPOCHS', '3'))\n",
    "FC_WARMUP_RATIO = float(os.environ.get('FC_WARMUP_RATIO', '0.03'))\n",
    "FC_EVAL_STEPS = int(os.environ.get('FC_EVAL_STEPS', '200'))\n",
    "FC_SAVE_STEPS = int(os.environ.get('FC_SAVE_STEPS', '200'))\n",
    "\n",
    "# Ensure eval steps are on for deliverables\n",
    "if FC_EVAL_STEPS <= 0:\n",
    "    FC_EVAL_STEPS = 200\n",
    "    print('FC_EVAL_STEPS was <=0; forcing to 200 for validation loss.')\n",
    "\n",
    "# --- Scale ---\n",
    "FC_TRAIN_NUM_NODES = int(os.environ.get('FC_TRAIN_NUM_NODES', '2'))\n",
    "FC_NUM_NODES =int(os.environ.get('FC_NUM_NODES', '4'))\n",
    "FC_GPUS_PER_NODE = int(os.environ.get('FC_GPUS_PER_NODE', str(GPUS_PER_NODE)))\n",
    "FC_MICRO_BS = int(os.environ.get('FC_MICRO_BS', '1'))\n",
    "FC_TRAIN_BS = int(os.environ.get('FC_TRAIN_BS', str(FC_MICRO_BS * FC_TRAIN_NUM_NODES * FC_GPUS_PER_NODE * 2)))\n",
    "\n",
    "# --- NeMo 2.0 SFT hyperparams ---\n",
    "FC_NEMO2_SEQ_LEN = int(os.environ.get('FC_NEMO2_SEQ_LEN', str(FC_MAX_LEN)))\n",
    "FC_NEMO2_MICRO_BS = int(os.environ.get('FC_NEMO2_MICRO_BS', str(FC_MICRO_BS)))\n",
    "FC_NEMO2_GLOBAL_BS = int(os.environ.get('FC_NEMO2_GLOBAL_BS', str(FC_TRAIN_BS)))\n",
    "_steps_per_epoch = math.ceil(FC_TRAIN_SIZE / max(FC_NEMO2_GLOBAL_BS, 1)) if FC_TRAIN_SIZE > 0 else 0\n",
    "FC_NEMO2_MAX_STEPS = int(os.environ.get('FC_NEMO2_MAX_STEPS', str(_steps_per_epoch * FC_MAX_EPOCHS if _steps_per_epoch else 0)))\n",
    "FC_NEMO2_WARMUP_STEPS = int(os.environ.get('FC_NEMO2_WARMUP_STEPS', str(max(1, int(FC_NEMO2_MAX_STEPS * FC_WARMUP_RATIO)) if FC_NEMO2_MAX_STEPS else 0)))\n",
    "FC_NEMO2_LR = float(os.environ.get('FC_NEMO2_LR', str(FC_LR)))\n",
    "FC_NEMO2_VAL_CHECK_INTERVAL = float(os.environ.get('FC_NEMO2_VAL_CHECK_INTERVAL', '1.0'))\n",
    "FC_NEMO2_LOG_EVERY_N_STEPS = int(os.environ.get('FC_NEMO2_LOG_EVERY_N_STEPS', '1'))\n",
    "FC_NEMO2_NUM_WORKERS = int(os.environ.get('FC_NEMO2_NUM_WORKERS', '2'))\n",
    "FC_NEMO2_PEFT = os.environ.get('FC_NEMO2_PEFT', 'none').strip()\n",
    "FC_NEMO2_SEED = int(os.environ.get('FC_NEMO2_SEED', str(FC_SEED)))\n",
    "FC_NEMO2_RECOMPUTE_GRANULARITY = os.environ.get('FC_NEMO2_RECOMPUTE_GRANULARITY', 'full').strip()\n",
    "FC_NEMO2_RECOMPUTE_METHOD = os.environ.get('FC_NEMO2_RECOMPUTE_METHOD', 'block').strip()\n",
    "if not FC_NEMO2_RECOMPUTE_METHOD:\n",
    "    FC_NEMO2_RECOMPUTE_METHOD = 'none'\n",
    "FC_NEMO2_RECOMPUTE_NUM_LAYERS = int(os.environ.get('FC_NEMO2_RECOMPUTE_NUM_LAYERS', '1'))\n",
    "FC_NEMO2_GRAD_ACCUM_FUSION = str(os.environ.get('FC_NEMO2_GRAD_ACCUM_FUSION', 'false')).strip().lower()\n",
    "FC_NEMO2_GRAD_ACCUM_FUSION = FC_NEMO2_GRAD_ACCUM_FUSION in {'1','true','yes','on'}\n",
    "\n",
    "# --- Elastic settings ---\n",
    "FC_ELASTIC = bool(str(os.environ.get('FC_ELASTIC', 'true')).lower() in {'1','true','yes'})\n",
    "FC_ELASTIC_MIN = int(os.environ.get('FC_ELASTIC_MIN', '1'))\n",
    "FC_ELASTIC_MAX = int(os.environ.get('FC_ELASTIC_MAX', str(FC_NUM_NODES)))\n",
    "if FC_ELASTIC_MAX < FC_ELASTIC_MIN:\n",
    "    FC_ELASTIC_MAX = FC_ELASTIC_MIN\n",
    "    print('FC_ELASTIC_MAX < FC_ELASTIC_MIN; forcing max = min')\n",
    "FC_ELASTIC_REPLICA_INCREMENT_STEP = int(os.environ.get('FC_ELASTIC_REPLICA_INCREMENT_STEP', '1'))\n",
    "if FC_ELASTIC and FC_TRAIN_NUM_NODES < FC_ELASTIC_MIN:\n",
    "    FC_TRAIN_NUM_NODES = FC_ELASTIC_MIN\n",
    "    print('FC_TRAIN_NUM_NODES < FC_ELASTIC_MIN; forcing nodes = min')\n",
    "if FC_ELASTIC and FC_TRAIN_NUM_NODES > FC_ELASTIC_MAX:\n",
    "    FC_TRAIN_NUM_NODES = FC_ELASTIC_MAX\n",
    "    print('FC_TRAIN_NUM_NODES > FC_ELASTIC_MAX; forcing nodes = max')\n",
    "\n",
    "# --- Directories (Space + Pod) ---\n",
    "SECTION8_DIR = FSX_BASE_DIR / 'section8_fc'\n",
    "FC_DATA_DIR = SECTION8_DIR / 'data'\n",
    "FC_RESULTS_DIR = SECTION8_DIR / 'results'\n",
    "FC_EVAL_DIR = SECTION8_DIR / 'eval'\n",
    "\n",
    "FC_DATA_TRAIN_DIR = FC_DATA_DIR / 'train'\n",
    "FC_DATA_VAL_DIR = FC_DATA_DIR / 'val'\n",
    "FC_DATA_TEST_DIR = FC_DATA_DIR / 'test'\n",
    "FC_NEMO2_DATA_DIR = SECTION8_DIR / 'data_nemo2'\n",
    "\n",
    "FC_NEMO2_IMPORT_DIR = SECTION8_DIR / 'imports'\n",
    "FC_NEMO2_IMPORT_OUTPUT = FC_NEMO2_IMPORT_DIR / f'qwen25_{FC_QWEN_SIZE}_instruct.nemo'\n",
    "\n",
    "for d in [SECTION8_DIR, FC_DATA_DIR, FC_RESULTS_DIR, FC_EVAL_DIR, FC_DATA_TRAIN_DIR, FC_DATA_VAL_DIR, FC_DATA_TEST_DIR, FC_NEMO2_DATA_DIR, FC_NEMO2_IMPORT_DIR]:\n",
    "    ensure_dir(d)\n",
    "\n",
    "POD_FC_DATA_TRAIN_DIR = f\"{POD_FSX_ROOT}/section8_fc/data/train\"\n",
    "POD_FC_DATA_VAL_DIR = f\"{POD_FSX_ROOT}/section8_fc/data/val\"\n",
    "POD_FC_DATA_TEST_DIR = f\"{POD_FSX_ROOT}/section8_fc/data/test\"\n",
    "POD_FC_NEMO2_DATA_DIR = f\"{POD_FSX_ROOT}/section8_fc/data_nemo2\"\n",
    "POD_FC_RESULTS_DIR = f\"{POD_FSX_ROOT}/section8_fc/results\"\n",
    "\n",
    "POD_FC_NEMO2_IMPORT_OUTPUT = f\"{POD_FSX_ROOT}/section8_fc/imports/qwen25_{FC_QWEN_SIZE}_instruct.nemo\"\n",
    "\n",
    "FC_RUN_NAME = f\"fc-sft-{RUN_ID}\"\n",
    "FC_IMPORT_RUN_NAME = f\"fc-import-{RUN_ID}\"\n",
    "FC_NEMO2_RUN_NAME = FC_RUN_NAME\n",
    "POD_FC_TRAIN_DIR = f\"{POD_FC_RESULTS_DIR}/{FC_RUN_NAME}\"\n",
    "POD_FC_EVAL_DIR = f\"{POD_FSX_ROOT}/section8_fc/eval\"\n",
    "\n",
    "FC_MODEL_SAVE_NAME = os.environ.get('FC_MODEL_SAVE_NAME', f'Qwen2.5-Coder-{FC_QWEN_SIZE_LABEL}-Instruct-SFT').strip()\n",
    "\n",
    "print('Section 8 toggles:')\n",
    "print({\n",
    "    'RUN_FC_PREP': RUN_FC_PREP,\n",
    "    'RUN_FC_SFT': RUN_FC_SFT,\n",
    "    'RUN_FC_IMPORT': RUN_FC_IMPORT,\n",
    "    'RUN_FC_EVAL_BASELINE': RUN_FC_EVAL_BASELINE,\n",
    "    'RUN_FC_EVAL_SFT': RUN_FC_EVAL_SFT,\n",
    "    'RUN_FC_NEMO2_PREP': RUN_FC_NEMO2_PREP,\n",
    "})\n",
    "print('Dataset:')\n",
    "print({\n",
    "    'FC_DATASET_ID': FC_DATASET_ID,\n",
    "    'FC_TRAIN_SIZE': FC_TRAIN_SIZE,\n",
    "    'FC_VAL_SIZE': FC_VAL_SIZE,\n",
    "    'FC_TEST_SIZE': FC_TEST_SIZE,\n",
    "    'FC_TOTAL_LIMIT': FC_TOTAL_LIMIT,\n",
    "})\n",
    "print('Model/recipe:')\n",
    "print({\n",
    "    'FC_QWEN_SIZE': FC_QWEN_SIZE_LABEL,\n",
    "    'FC_BASE_MODEL_ID': FC_BASE_MODEL_ID,\n",
    "    'FC_TOKENIZER_ID': FC_TOKENIZER_ID,\n",
    "    'FC_RECIPE_SFT_ID': FC_RECIPE_SFT_ID,\n",
    "    'FC_RECIPE_IMPORT_ID': FC_RECIPE_IMPORT_ID,\n",
    "})\n",
    "print('Training:')\n",
    "print({\n",
    "    'FC_TRAIN_NUM_NODES': FC_TRAIN_NUM_NODES,\n",
    "    'FC_GPUS_PER_NODE': FC_GPUS_PER_NODE,\n",
    "    'FC_MICRO_BS': FC_MICRO_BS,\n",
    "    'FC_TRAIN_BS': FC_TRAIN_BS,\n",
    "    'FC_LR': FC_LR,\n",
    "    'FC_MAX_EPOCHS': FC_MAX_EPOCHS,\n",
    "    'FC_MAX_LEN': FC_MAX_LEN,\n",
    "    'FC_WARMUP_RATIO': FC_WARMUP_RATIO,\n",
    "})\n",
    "print('Elastic (NeMo 2.0 SFT):')\n",
    "print({\n",
    "    'FC_ELASTIC': FC_ELASTIC,\n",
    "    'FC_ELASTIC_MIN': FC_ELASTIC_MIN,\n",
    "    'FC_ELASTIC_MAX': FC_ELASTIC_MAX,\n",
    "    'FC_ELASTIC_REPLICA_INCREMENT_STEP': FC_ELASTIC_REPLICA_INCREMENT_STEP,\n",
    "})\n",
    "print('NeMo 2.0 SFT:')\n",
    "print({\n",
    "    'FC_NEMO2_SEQ_LEN': FC_NEMO2_SEQ_LEN,\n",
    "    'FC_NEMO2_MICRO_BS': FC_NEMO2_MICRO_BS,\n",
    "    'FC_NEMO2_GLOBAL_BS': FC_NEMO2_GLOBAL_BS,\n",
    "    'FC_NEMO2_MAX_STEPS': FC_NEMO2_MAX_STEPS,\n",
    "    'FC_NEMO2_LR': FC_NEMO2_LR,\n",
    "    'FC_NEMO2_WARMUP_STEPS': FC_NEMO2_WARMUP_STEPS,\n",
    "    'FC_NEMO2_VAL_CHECK_INTERVAL': FC_NEMO2_VAL_CHECK_INTERVAL,\n",
    "    'FC_NEMO2_LOG_EVERY_N_STEPS': FC_NEMO2_LOG_EVERY_N_STEPS,\n",
    "    'FC_NEMO2_NUM_WORKERS': FC_NEMO2_NUM_WORKERS,\n",
    "    'FC_NEMO2_PEFT': FC_NEMO2_PEFT,\n",
    "    'FC_NEMO2_RECOMPUTE_GRANULARITY': FC_NEMO2_RECOMPUTE_GRANULARITY,\n",
    "    'FC_NEMO2_RECOMPUTE_METHOD': FC_NEMO2_RECOMPUTE_METHOD,\n",
    "    'FC_NEMO2_RECOMPUTE_NUM_LAYERS': FC_NEMO2_RECOMPUTE_NUM_LAYERS,\n",
    "    'FC_NEMO2_GRAD_ACCUM_FUSION': FC_NEMO2_GRAD_ACCUM_FUSION,\n",
    "})\n",
    "print('Paths:')\n",
    "print({\n",
    "    'FC_DATA_TRAIN_DIR': str(FC_DATA_TRAIN_DIR),\n",
    "    'FC_DATA_VAL_DIR': str(FC_DATA_VAL_DIR),\n",
    "    'FC_DATA_TEST_DIR': str(FC_DATA_TEST_DIR),\n",
    "    'FC_NEMO2_DATA_DIR': str(FC_NEMO2_DATA_DIR),\n",
    "    'FC_NEMO2_IMPORT_OUTPUT': str(FC_NEMO2_IMPORT_OUTPUT),\n",
    "    'POD_FC_DATA_TRAIN_DIR': POD_FC_DATA_TRAIN_DIR,\n",
    "    'POD_FC_NEMO2_IMPORT_OUTPUT': POD_FC_NEMO2_IMPORT_OUTPUT,\n",
    "    'POD_FC_RESULTS_DIR': POD_FC_RESULTS_DIR,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18161d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 8 data prep (XLAM function-calling)\n",
      "===========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: {'train': 54000, 'val': 3000, 'test': 3000}\n",
      "Wrote:\n",
      "  train: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/data/train/train.json rows: 54000\n",
      "  val  : /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/data/val/val.json rows: 3000\n",
      "  test : /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/data/test/test_raw.jsonl rows: 3000\n",
      "Raw splits: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/data/train/train_raw.jsonl /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/data/val/val_raw.jsonl /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/data/test/test_raw.jsonl\n",
      "Sample raw: {'query': \"What is 'December 31, 2022' in the 'day-month-year' format?\", 'tools': '[{\"name\": \"format_date\", \"description\": \"Converts a date string from one format to another.\", \"parameters\": {\"date\": {\"description\": \"The date string to convert.\", \"type\": \"str\"}, \"input_format\": {\"description\": \"The format of the input date string.\", \"type\": \"str\"}, \"output_format\": {\"description\": \"The desired format of the output date string.\", \"type\": \"str\"}}}, {\"name\": \"merge_dictionaries\", \"description\": \"Merges two dictionaries into a single dictionary.\", \"parameters\": {\"dict1\": {\"description\": \"The first dictionary.\", \"type\": \"Dict\"}, \"dict2\": {\"description\": \"The second dictionary.\", \"type\": \"Dict\"}}}]', 'answers': '[{\"name\": \"format_date\", \"arguments\": {\"date\": \"December 31, 2022\", \"input_format\": \"%B %d, %Y\", \"output_format\": \"%d-%m-%Y\"}}]'}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8b: Prepare XLAM function-calling dataset (train/val/test)\n",
    "# =============================================================================\n",
    "\n",
    "if not RUN_FC_PREP:\n",
    "    print('Skipping Section 8 data prep (RUN_FC_PREP=False).')\n",
    "else:\n",
    "    print_header('Section 8 data prep (XLAM function-calling)')\n",
    "\n",
    "    from datasets import load_dataset\n",
    "    from transformers import AutoTokenizer\n",
    "    import json as _json\n",
    "\n",
    "    HF_TOKEN = os.environ.get('HF_TOKEN') or None\n",
    "    tokenizer = AutoTokenizer.from_pretrained(FC_TOKENIZER_ID, token=HF_TOKEN)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'right'\n",
    "\n",
    "    def _apply_fc_chat_template(messages, add_generation_prompt):\n",
    "        if hasattr(tokenizer, 'apply_chat_template'):\n",
    "            return tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=add_generation_prompt,\n",
    "            )\n",
    "        parts = []\n",
    "        for msg in messages:\n",
    "            role = msg.get('role', 'user')\n",
    "            prefix = 'User: ' if role == 'user' else 'Assistant: '\n",
    "            parts.append(prefix + msg.get('content', ''))\n",
    "        if add_generation_prompt:\n",
    "            parts.append('Assistant: ')\n",
    "        return '\\n'.join(parts)\n",
    "\n",
    "    def _tokenize(text, max_len=None, truncation_side='left'):\n",
    "        tokenizer.truncation_side = truncation_side\n",
    "        if max_len:\n",
    "            return tokenizer(text, add_special_tokens=False, truncation=True, max_length=max_len).input_ids\n",
    "        return tokenizer(text, add_special_tokens=False).input_ids\n",
    "\n",
    "    def _format_example(example):\n",
    "        tools = example.get('tools', '')\n",
    "        query = example.get('query', '')\n",
    "        answer = example.get('answers', '')\n",
    "\n",
    "        system = (\n",
    "            \"You are a helpful assistant with access to the following functions. \"\n",
    "            \"Use them if required.\\n\" + str(tools)\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': system},\n",
    "            {'role': 'user', 'content': str(query)},\n",
    "            {'role': 'assistant', 'content': str(answer)},\n",
    "        ]\n",
    "\n",
    "        prompt_text = _apply_fc_chat_template(messages[:-1], add_generation_prompt=True)\n",
    "        full_text = _apply_fc_chat_template(messages, add_generation_prompt=False)\n",
    "\n",
    "        prompt_ids = _tokenize(prompt_text, max_len=FC_MAX_LEN, truncation_side='left')\n",
    "        full_ids = _tokenize(full_text, max_len=FC_MAX_LEN, truncation_side='left')\n",
    "\n",
    "        labels = list(full_ids)\n",
    "        prompt_len = min(len(prompt_ids), len(labels))\n",
    "        for i in range(prompt_len):\n",
    "            labels[i] = -100\n",
    "\n",
    "        rec = {\n",
    "            'input_ids': list(full_ids),\n",
    "            'attention_mask': [1] * len(full_ids),\n",
    "            'labels': labels,\n",
    "        }\n",
    "        raw = {\n",
    "            'query': query,\n",
    "            'tools': tools,\n",
    "            'answers': answer,\n",
    "        }\n",
    "        return rec, raw\n",
    "\n",
    "    # Load dataset and split\n",
    "    ds = load_dataset(FC_DATASET_ID, split='train')\n",
    "    ds = ds.shuffle(seed=FC_SEED)\n",
    "    if FC_TOTAL_LIMIT and FC_TOTAL_LIMIT > 0:\n",
    "        ds = ds.select(range(min(FC_TOTAL_LIMIT, len(ds))))\n",
    "\n",
    "    splits = ds.train_test_split(test_size=0.1, seed=FC_SEED)\n",
    "    train_set = splits['train']\n",
    "    temp = splits['test'].train_test_split(test_size=0.5, seed=FC_SEED)\n",
    "    val_set = temp['train']\n",
    "    test_set = temp['test']\n",
    "\n",
    "    # Optional size caps\n",
    "    if FC_TRAIN_SIZE > 0:\n",
    "        train_set = train_set.select(range(min(FC_TRAIN_SIZE, len(train_set))))\n",
    "    if FC_VAL_SIZE > 0:\n",
    "        val_set = val_set.select(range(min(FC_VAL_SIZE, len(val_set))))\n",
    "    if FC_TEST_SIZE > 0:\n",
    "        test_set = test_set.select(range(min(FC_TEST_SIZE, len(test_set))))\n",
    "\n",
    "    print('Split sizes:', {'train': len(train_set), 'val': len(val_set), 'test': len(test_set)})\n",
    "\n",
    "    # Write tokenized JSONL + raw JSONL\n",
    "    def _write_split(ds_split, token_path, raw_path):\n",
    "        count = 0\n",
    "        with open(token_path, 'w') as f_tok, open(raw_path, 'w') as f_raw:\n",
    "            for ex in ds_split:\n",
    "                rec, raw = _format_example(ex)\n",
    "                f_tok.write(_json.dumps(rec) + '\\n')\n",
    "                f_raw.write(_json.dumps(raw) + '\\n')\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    train_tok = FC_DATA_TRAIN_DIR / 'train.json'\n",
    "    val_tok = FC_DATA_VAL_DIR / 'val.json'\n",
    "    test_tok = FC_DATA_TEST_DIR / 'test.json'  # optional; not used by trainer\n",
    "\n",
    "    train_raw = FC_DATA_TRAIN_DIR / 'train_raw.jsonl'\n",
    "    val_raw = FC_DATA_VAL_DIR / 'val_raw.jsonl'\n",
    "    test_raw = FC_DATA_TEST_DIR / 'test_raw.jsonl'\n",
    "\n",
    "    n_train = _write_split(train_set, train_tok, train_raw)\n",
    "    n_val = _write_split(val_set, val_tok, val_raw)\n",
    "    n_test = _write_split(test_set, test_tok, test_raw)\n",
    "\n",
    "    print('Wrote:')\n",
    "    print('  train:', train_tok, 'rows:', n_train)\n",
    "    print('  val  :', val_tok, 'rows:', n_val)\n",
    "    print('  test :', test_raw, 'rows:', n_test)\n",
    "    print('Raw splits:', train_raw, val_raw, test_raw)\n",
    "\n",
    "    # Quick sanity sample\n",
    "    try:\n",
    "        with open(train_raw, 'r') as f:\n",
    "            sample = _json.loads(next(f))\n",
    "        print('Sample raw:', sample)\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8919f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 8 NeMo 2.0 data prep (input/output jsonl)\n",
      "=================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote NeMo 2.0 JSONL:\n",
      "  train: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/data_nemo2/training.jsonl rows: 54000\n",
      "  val  : /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/data_nemo2/validation.jsonl rows: 3000\n",
      "  test : /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/data_nemo2/test.jsonl rows: 3000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8b.2: Prepare NeMo 2.0 JSONL splits (input/output)\n",
    "# =============================================================================\n",
    "\n",
    "if not RUN_FC_NEMO2_PREP:\n",
    "    print('Skipping NeMo 2.0 data prep (RUN_FC_NEMO2_PREP=False).')\n",
    "else:\n",
    "    print_header('Section 8 NeMo 2.0 data prep (input/output jsonl)')\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "    import json as _json\n",
    "\n",
    "    HF_TOKEN = os.environ.get('HF_TOKEN') or None\n",
    "    tokenizer = AutoTokenizer.from_pretrained(FC_TOKENIZER_ID, token=HF_TOKEN)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'right'\n",
    "\n",
    "    raw_train = FC_DATA_TRAIN_DIR / 'train_raw.jsonl'\n",
    "    raw_val = FC_DATA_VAL_DIR / 'val_raw.jsonl'\n",
    "    raw_test = FC_DATA_TEST_DIR / 'test_raw.jsonl'\n",
    "    if not raw_train.exists() or not raw_val.exists() or not raw_test.exists():\n",
    "        raise FileNotFoundError('Raw splits not found. Run Section 8b (RUN_FC_PREP=True) first.')\n",
    "\n",
    "    def _apply_fc_chat_template(messages, add_generation_prompt):\n",
    "        if hasattr(tokenizer, 'apply_chat_template'):\n",
    "            return tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=add_generation_prompt,\n",
    "            )\n",
    "        parts = []\n",
    "        for msg in messages:\n",
    "            role = msg.get('role', 'user')\n",
    "            prefix = 'User: ' if role == 'user' else 'Assistant: '\n",
    "            parts.append(prefix + msg.get('content', ''))\n",
    "        if add_generation_prompt:\n",
    "            parts.append('Assistant: ')\n",
    "        return '\\n'.join(parts)\n",
    "\n",
    "    def _format_nemo(raw):\n",
    "        tools = raw.get('tools', '')\n",
    "        query = raw.get('query', '')\n",
    "        answer = raw.get('answers', '')\n",
    "\n",
    "        system = (\n",
    "            \"You are a helpful assistant with access to the following functions. \"\n",
    "            \"Use them if required.\\n\" + str(tools)\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': system},\n",
    "            {'role': 'user', 'content': str(query)},\n",
    "            {'role': 'assistant', 'content': str(answer)},\n",
    "        ]\n",
    "\n",
    "        prompt_text = _apply_fc_chat_template(messages[:-1], add_generation_prompt=True)\n",
    "        return {\n",
    "            'input': prompt_text,\n",
    "            'output': str(answer),\n",
    "            'query': query,\n",
    "            'tools': tools,\n",
    "        }\n",
    "\n",
    "    def _write_nemo(raw_path, out_path):\n",
    "        count = 0\n",
    "        with open(raw_path, 'r') as f_in, open(out_path, 'w') as f_out:\n",
    "            for line in f_in:\n",
    "                rec = _format_nemo(_json.loads(line))\n",
    "                f_out.write(_json.dumps(rec) + '\\n')\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    out_train = FC_NEMO2_DATA_DIR / 'training.jsonl'\n",
    "    out_val = FC_NEMO2_DATA_DIR / 'validation.jsonl'\n",
    "    out_test = FC_NEMO2_DATA_DIR / 'test.jsonl'\n",
    "\n",
    "    n_train = _write_nemo(raw_train, out_train)\n",
    "    n_val = _write_nemo(raw_val, out_val)\n",
    "    n_test = _write_nemo(raw_test, out_test)\n",
    "\n",
    "    print('Wrote NeMo 2.0 JSONL:')\n",
    "    print('  train:', out_train, 'rows:', n_train)\n",
    "    print('  val  :', out_val, 'rows:', n_val)\n",
    "    print('  test :', out_test, 'rows:', n_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "344410f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prefetch Qwen2.5-Coder base model to FSx cache\n",
      "==============================================\n",
      "Prefetching model: Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "Cache dir: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/hf-home/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d8bf3509d24ffba863c87fa70f0b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefetch complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8b.1: Prefetch Qwen2.5-Coder model to FSx cache\n",
    "# =============================================================================\n",
    "\n",
    "if not RUN_FC_PREFETCH:\n",
    "    print('Skipping model prefetch (RUN_FC_PREFETCH=False).')\n",
    "else:\n",
    "    print_header('Prefetch Qwen2.5-Coder base model to FSx cache')\n",
    "\n",
    "    try:\n",
    "        from huggingface_hub import snapshot_download\n",
    "    except Exception:\n",
    "        run('python -m pip install -q huggingface_hub')\n",
    "        from huggingface_hub import snapshot_download\n",
    "\n",
    "    model_id = FC_PREFETCH_MODEL_ID\n",
    "    revision = FC_PREFETCH_REVISION\n",
    "\n",
    "    cache_dir = os.environ.get('HF_HUB_CACHE', '') or str(HF_HUB_CACHE_DIR)\n",
    "    print('Prefetching model:', model_id)\n",
    "    print('Cache dir:', cache_dir)\n",
    "\n",
    "    try:\n",
    "        snapshot_download(\n",
    "            repo_id=model_id,\n",
    "            revision=revision,\n",
    "            cache_dir=cache_dir,\n",
    "            token=os.environ.get('HF_TOKEN') or None,\n",
    "            local_dir_use_symlinks=False,\n",
    "        )\n",
    "        print('Prefetch complete.')\n",
    "    except Exception as e:\n",
    "        print('WARNING: Prefetch failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8829b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submit NeMo 2.0 HF -> NeMo import (Qwen2.5-Coder-7B-Instruct)\n",
      "=============================================================\n",
      "Import artifact already exists: /mnt/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/imports/qwen25_7b_instruct.nemo (skipping)\n",
      "\n",
      "Submit NeMo 2.0 SFT (Qwen2.5-Coder-7B-Instruct)\n",
      "===============================================\n",
      "Recipe: fine-tuning/qwen/nemo2_qwen2_5_7b_instruct_seq4k_gpu_sft_fft\n",
      "Command: /home/sagemaker-user/bobber/notebooks/.venv/bin/python -m launcher recipes=fine-tuning/qwen/nemo2_qwen2_5_7b_instruct_seq4k_gpu_sft_fft base_results_dir=/fsx/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/results container=<ACCOUNT_ID>.dkr.ecr.us-east-1.amazonaws.com/nemo-framework-hyperpod:25.04-eks cluster=k8s cluster_type=k8s instance_type=p4d.24xlarge recipes.run.name=fc-sft-20260105-000949-d8a722 recipes.run.nodes=2 recipes.trainer.num_nodes=2 recipes.run.ntasks_per_node=8 +env_vars.HF_TOKEN=<HF_TOKEN> +env_vars.HF_HOME=/fsx/fs-03b1953d09801303c/smus-nemo-smoke/hf-home +env_vars.HF_HUB_CACHE=/fsx/fs-03b1953d09801303c/smus-nemo-smoke/hf-home/hub +env_vars.HF_DATASETS_CACHE=/fsx/fs-03b1953d09801303c/smus-nemo-smoke/hf-home/datasets +env_vars.HF_ASSETS_CACHE=/fsx/fs-03b1953d09801303c/smus-nemo-smoke/hf-home/assets +env_vars.TOKENIZERS_PARALLELISM=false env_vars.NCCL_DEBUG=INFO +env_vars.NCCL_DEBUG_SUBSYS=INIT\\\\,NET\\\\,ENV\\\\,COLL +env_vars.TORCH_NCCL_ASYNC_ERROR_HANDLING=1 +env_vars.TORCH_DISABLE_ADDR2LINE=1 +env_vars.CUDA_LAUNCH_BLOCKING=1 +env_vars.TORCH_CPP_LOG_LEVEL=INFO +env_vars.NCCL_DEBUG_FILE=/data/nccl_%h_%p.log +env_vars.TORCH_DISTRIBUTED_DEBUG=INFO +env_vars.TORCH_SHOW_CPP_STACKTRACES=1 +env_vars.FC_NEMO2_RECOMPUTE_GRANULARITY=selective recipes.elastic_policy.is_elastic=true recipes.elastic_policy.min_nodes=1 recipes.elastic_policy.max_nodes=4 recipes.elastic_policy.replica_increment_step=1 +recipes.pre_script='[\"JOB_DATA_DIR=\\\"/fsx/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/results/fc-sft-20260105-000949-d8a722/data\\\"\", \"mkdir -p \\\"$JOB_DATA_DIR\\\"\", \"ln -sfn \\\"$JOB_DATA_DIR\\\" /data\"]'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8c: Import HF -> NeMo + Submit SFT (NeMo 2.0, Qwen2.5-Coder-7B/14B-Instruct)\n",
    "# =============================================================================\n",
    "SFT_PRE_SCRIPT = [\n",
    "        f'JOB_DATA_DIR=\"{POD_FC_TRAIN_DIR}/data\"',\n",
    "        'mkdir -p \"$JOB_DATA_DIR\"',\n",
    "        'ln -sfn \"$JOB_DATA_DIR\" /data'\n",
    "    ]\n",
    "SFT_PRE_SCRIPT_ARG = \"+recipes.pre_script=\" + shlex.quote(json.dumps(SFT_PRE_SCRIPT))\n",
    "if not RUN_FC_IMPORT:\n",
    "    print('Skipping HF -> NeMo import (RUN_FC_IMPORT=False).')\n",
    "else:\n",
    "    print_header(f'Submit NeMo 2.0 HF -> NeMo import (Qwen2.5-Coder-{FC_QWEN_SIZE_LABEL}-Instruct)')\n",
    "\n",
    "    # Import output (shared across runs)\n",
    "    os.environ['FC_NEMO2_HF_MODEL_ID'] = FC_BASE_MODEL_ID\n",
    "    os.environ['FC_NEMO2_SEQ_LEN'] = str(FC_NEMO2_SEQ_LEN)\n",
    "    os.environ['FC_NEMO2_IMPORT_OUTPUT'] = str(POD_FC_NEMO2_IMPORT_OUTPUT)\n",
    "\n",
    "    import_output = Path(FC_NEMO2_IMPORT_OUTPUT).expanduser().resolve()\n",
    "    if import_output.exists():\n",
    "        print(f'Import artifact already exists: {import_output} (skipping)')\n",
    "    else:\n",
    "        cmd = (\n",
    "            f\"{VENV_PY} -m launcher recipes={FC_RECIPE_IMPORT_ID} \"\n",
    "            f\"base_results_dir={POD_FC_RESULTS_DIR} \"\n",
    "            f\"container={NEMO_IMAGE} \"\n",
    "            f\"cluster=k8s cluster_type=k8s instance_type={INSTANCE_TYPE} \"\n",
    "            f\"recipes.run.name={FC_IMPORT_RUN_NAME} \"\n",
    "            f\"+env_vars.HF_TOKEN={os.environ.get('HF_TOKEN','')} \"\n",
    "            f\"+env_vars.HF_HOME={POD_HF_HOME} \"\n",
    "            f\"+env_vars.HF_HUB_CACHE={POD_HF_HUB_CACHE} \"\n",
    "            f\"+env_vars.HF_DATASETS_CACHE={POD_HF_DATASETS_CACHE} \"\n",
    "            f\"+env_vars.HF_ASSETS_CACHE={POD_HF_ASSETS_CACHE} \"\n",
    "            f\"+env_vars.TOKENIZERS_PARALLELISM=false \"\n",
    "            f\"{SFT_PRE_SCRIPT_ARG}\"\n",
    "        )\n",
    "        print('Recipe:', FC_RECIPE_IMPORT_ID)\n",
    "        print('Command:', rdct_sens(cmd))\n",
    "        run(cmd)\n",
    "\n",
    "        # Wait for the imported checkpoint to appear on FSx\n",
    "        timeout_s = int(os.environ.get('FC_IMPORT_TIMEOUT_S', '7200'))\n",
    "        poll_s = int(os.environ.get('FC_IMPORT_POLL_S', '30'))\n",
    "        print(f'Waiting for import artifact (timeout={timeout_s}s): {import_output}')\n",
    "        start = time.time()\n",
    "        while not import_output.exists():\n",
    "            if time.time() - start > timeout_s:\n",
    "                raise TimeoutError(f'Timed out waiting for import artifact: {import_output}')\n",
    "            time.sleep(poll_s)\n",
    "        print(f'Import completed: {import_output}')\n",
    "\n",
    "if not RUN_FC_SFT:\n",
    "    print('Skipping Section 8 SFT (RUN_FC_SFT=False).')\n",
    "else:\n",
    "    print_header(f'Submit NeMo 2.0 SFT (Qwen2.5-Coder-{FC_QWEN_SIZE_LABEL}-Instruct)')\n",
    "\n",
    "    \n",
    "\n",
    "    # Populate env vars used by the NeMo 2.0 recipe\n",
    "    os.environ['FC_NEMO2_DATASET_ROOT'] = str(POD_FC_NEMO2_DATA_DIR)\n",
    "    os.environ['FC_NEMO2_HF_MODEL_ID'] = FC_BASE_MODEL_ID\n",
    "    os.environ['FC_NEMO2_IMPORT_OUTPUT'] = str(POD_FC_NEMO2_IMPORT_OUTPUT)\n",
    "    os.environ['FC_NEMO2_SEQ_LEN'] = str(FC_NEMO2_SEQ_LEN)\n",
    "    os.environ['FC_NEMO2_MICRO_BS'] = str(FC_NEMO2_MICRO_BS)\n",
    "    os.environ['FC_NEMO2_GLOBAL_BS'] = str(FC_NEMO2_GLOBAL_BS)\n",
    "    os.environ['FC_NEMO2_MAX_STEPS'] = str(FC_NEMO2_MAX_STEPS)\n",
    "    os.environ['FC_NEMO2_LR'] = str(FC_NEMO2_LR)\n",
    "    os.environ['FC_NEMO2_WARMUP_STEPS'] = str(FC_NEMO2_WARMUP_STEPS)\n",
    "    os.environ['FC_NEMO2_VAL_CHECK_INTERVAL'] = str(FC_NEMO2_VAL_CHECK_INTERVAL)\n",
    "    os.environ['FC_NEMO2_LOG_EVERY_N_STEPS'] = str(FC_NEMO2_LOG_EVERY_N_STEPS)\n",
    "    os.environ['FC_NEMO2_NUM_WORKERS'] = str(FC_NEMO2_NUM_WORKERS)\n",
    "    os.environ['FC_NEMO2_PEFT'] = FC_NEMO2_PEFT\n",
    "    os.environ['FC_NEMO2_SEED'] = str(FC_NEMO2_SEED)\n",
    "    os.environ['FC_NEMO2_RECOMPUTE_GRANULARITY'] = str(FC_NEMO2_RECOMPUTE_GRANULARITY)\n",
    "    os.environ['FC_NEMO2_RECOMPUTE_METHOD'] = str(FC_NEMO2_RECOMPUTE_METHOD)\n",
    "    os.environ['FC_NEMO2_RECOMPUTE_NUM_LAYERS'] = str(FC_NEMO2_RECOMPUTE_NUM_LAYERS)\n",
    "    os.environ['FC_NEMO2_GRAD_ACCUM_FUSION'] = 'true' if FC_NEMO2_GRAD_ACCUM_FUSION else 'false'\n",
    "\n",
    "    recompute_env_args = ''\n",
    "    if FC_NEMO2_RECOMPUTE_GRANULARITY:\n",
    "        recompute_env_args += f\"+env_vars.FC_NEMO2_RECOMPUTE_GRANULARITY={FC_NEMO2_RECOMPUTE_GRANULARITY} \"\n",
    "    if FC_NEMO2_RECOMPUTE_METHOD and FC_NEMO2_RECOMPUTE_METHOD.lower() != 'none':\n",
    "        recompute_env_args += f\"+env_vars.FC_NEMO2_RECOMPUTE_METHOD={FC_NEMO2_RECOMPUTE_METHOD} \"\n",
    "    if FC_NEMO2_RECOMPUTE_NUM_LAYERS > 0:\n",
    "        recompute_env_args += f\"+env_vars.FC_NEMO2_RECOMPUTE_NUM_LAYERS={FC_NEMO2_RECOMPUTE_NUM_LAYERS} \"\n",
    "\n",
    "    elastic_args = f\"recipes.elastic_policy.is_elastic={str(FC_ELASTIC).lower()} \"\n",
    "    if FC_ELASTIC:\n",
    "        elastic_args += f\"recipes.elastic_policy.min_nodes={FC_ELASTIC_MIN} \"\n",
    "        elastic_args += f\"recipes.elastic_policy.max_nodes={FC_ELASTIC_MAX} \"\n",
    "        elastic_args += f\"recipes.elastic_policy.replica_increment_step={FC_ELASTIC_REPLICA_INCREMENT_STEP} \"\n",
    "    # Guard: import artifact must exist when skip-import is enabled in the recipe\n",
    "    if not Path(FC_NEMO2_IMPORT_OUTPUT).exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing HF -> NeMo import artifact: {FC_NEMO2_IMPORT_OUTPUT}. \"\n",
    "            \"Run the import step first or set RUN_FC_IMPORT=true.\"\n",
    "        )\n",
    "\n",
    "    cmd = (\n",
    "        f\"{VENV_PY} -m launcher recipes={FC_RECIPE_SFT_ID} \"\n",
    "        f\"base_results_dir={POD_FC_RESULTS_DIR} \"\n",
    "        f\"container={NEMO_IMAGE} \"\n",
    "        f\"cluster=k8s cluster_type=k8s instance_type={INSTANCE_TYPE} \"\n",
    "        f\"recipes.run.name={FC_RUN_NAME} \"\n",
    "        f\"recipes.run.nodes={FC_TRAIN_NUM_NODES} recipes.trainer.num_nodes={FC_TRAIN_NUM_NODES} recipes.run.ntasks_per_node={FC_GPUS_PER_NODE} \"\n",
    "        f\"+env_vars.HF_TOKEN={os.environ.get('HF_TOKEN','')} \"\n",
    "        f\"+env_vars.HF_HOME={POD_HF_HOME} \"\n",
    "        f\"+env_vars.HF_HUB_CACHE={POD_HF_HUB_CACHE} \"\n",
    "        f\"+env_vars.HF_DATASETS_CACHE={POD_HF_DATASETS_CACHE} \"\n",
    "        f\"+env_vars.HF_ASSETS_CACHE={POD_HF_ASSETS_CACHE} \"\n",
    "        f\"+env_vars.TOKENIZERS_PARALLELISM=false \"\n",
    "        f\"env_vars.NCCL_DEBUG=INFO \"\n",
    "        rf\"+env_vars.NCCL_DEBUG_SUBSYS=INIT\\\\,NET\\\\,ENV\\\\,COLL \"\n",
    "        f\"+env_vars.TORCH_NCCL_ASYNC_ERROR_HANDLING=1 \"\n",
    "        f\"+env_vars.TORCH_DISABLE_ADDR2LINE=1 \"\n",
    "        f\"+env_vars.CUDA_LAUNCH_BLOCKING=1 \"\n",
    "        f\"+env_vars.TORCH_CPP_LOG_LEVEL=INFO \"\n",
    "        f\"+env_vars.NCCL_DEBUG_FILE=/data/nccl_%h_%p.log \"\n",
    "        f\"+env_vars.TORCH_DISTRIBUTED_DEBUG=INFO \"\n",
    "        f\"+env_vars.TORCH_SHOW_CPP_STACKTRACES=1 \"\n",
    "        f\"{recompute_env_args}\"\n",
    "        f\"{elastic_args}\"\n",
    "        f\"{SFT_PRE_SCRIPT_ARG}\"\n",
    "    )\n",
    "    print('Recipe:', FC_RECIPE_SFT_ID)\n",
    "    print('Command:', rdct_sens(cmd))\n",
    "    run(cmd)\n",
    "\n",
    "    # Reuse existing SFT env vars for downstream eval\n",
    "    os.environ['FC_SFT_RUN_NAME'] = FC_RUN_NAME\n",
    "    os.environ['FC_SFT_JOB_NAME'] = ''\n",
    "    os.environ['FC_SFT_TRAIN_DIR'] = f\"{POD_FC_RESULTS_DIR}/{FC_RUN_NAME}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad21a9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 8 SFT status (async)\n",
      "============================\n",
      "fc-sft-20260105-000949-d8a722-qgjkhhyperpod-ns-datascientist1Completed      9m             \n",
      "Job: fc-sft-20260105-000949-d8a722-qgjkh\n",
      "Phase: (unavailable)\n"
     ]
    }
   ],
   "source": [
    "# --- Section 8 SFT job status (async) ---\n",
    "print_header('Section 8 SFT status (async)')\n",
    "\n",
    "job_name = os.environ.get('FC_SFT_JOB_NAME', '').strip()\n",
    "job_prefix = os.environ.get('FC_SFT_RUN_NAME', '').strip()\n",
    "namespace = os.environ.get('HYPERPOD_NAMESPACE', '').strip() or 'hyperpod-ns-datascientist1'\n",
    "\n",
    "if not job_prefix and job_name:\n",
    "    job_prefix = job_name\n",
    "if not job_prefix:\n",
    "    try:\n",
    "        job_prefix = f\"fc-sft-{RUN_ID}\"\n",
    "    except Exception:\n",
    "        job_prefix = ''\n",
    "\n",
    "def _find_latest_hyp_job_by_prefix_with_hyp(prefix, namespace):\n",
    "    if not prefix:\n",
    "        return None\n",
    "    rc, out_txt = run(f\"hyp list hyp-pytorch-job -n {namespace}\", check=False)\n",
    "    if rc != 0 or not out_txt:\n",
    "        return None\n",
    "    matches = []\n",
    "    for line in out_txt.splitlines():\n",
    "        parsed = _parse_hyp_list_line(line, namespace)\n",
    "        if not parsed:\n",
    "            continue\n",
    "        name, status, age, raw = parsed\n",
    "        if name.startswith(prefix):\n",
    "            matches.append((name, age, line))\n",
    "    if not matches:\n",
    "        return None\n",
    "    # Pick most recent by age (smallest age wins when parsed)\n",
    "    def _age_to_seconds(age):\n",
    "        if not age:\n",
    "            return 10**12\n",
    "        total = 0\n",
    "        num = ''\n",
    "        for ch in age:\n",
    "            if ch.isdigit():\n",
    "                num += ch\n",
    "                continue\n",
    "            if not num:\n",
    "                continue\n",
    "            val = int(num)\n",
    "            num = ''\n",
    "            if ch == 's':\n",
    "                total += val\n",
    "            elif ch == 'm':\n",
    "                total += val * 60\n",
    "            elif ch == 'h':\n",
    "                total += val * 3600\n",
    "            elif ch == 'd':\n",
    "                total += val * 86400\n",
    "        return total if total > 0 else 10**12\n",
    "    matches.sort(key=lambda x: _age_to_seconds(x[1]))\n",
    "    return matches[0][0]\n",
    "\n",
    "if not job_name and job_prefix:\n",
    "    job_name = _find_latest_hyp_job_by_prefix_with_hyp(job_prefix, namespace)\n",
    "    if job_name:\n",
    "        os.environ['FC_SFT_JOB_NAME'] = job_name\n",
    "\n",
    "if not job_name:\n",
    "    print('FC_SFT_JOB_NAME not set. Run the submission cell first.')\n",
    "else:\n",
    "    !hyp list hyp-pytorch-job -n ${HYPERPOD_NAMESPACE} | grep -F -- \"${FC_SFT_JOB_NAME}\" || echo '(job not found)'\n",
    "    phase, conds = get_hyp_job_status(job_name, namespace)\n",
    "    print('Job:', job_name)\n",
    "    print('Phase:', phase if phase is not None else '(unavailable)')\n",
    "    if conds:\n",
    "        print('Conditions:', conds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51fb794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 8 eval script setup\n",
      "===========================\n",
      "Eval script: /home/sagemaker-user/custom-file-systems/fsx_lustre/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/eval/run_fc_eval.py\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8d: Evaluation script (baseline + SFT)\n",
    "# =============================================================================\n",
    "\n",
    "print_header('Section 8 eval script setup')\n",
    "\n",
    "# Eval params (reuse if already set)\n",
    "FC_EVAL_MODE = os.environ.get('FC_EVAL_MODE', 'quick').strip().lower()\n",
    "if FC_EVAL_MODE not in {'quick', 'full'}:\n",
    "    FC_EVAL_MODE = 'quick'\n",
    "\n",
    "_fc_eval_limit_env = os.environ.get('FC_EVAL_LIMIT', '').strip()\n",
    "if _fc_eval_limit_env:\n",
    "    FC_EVAL_LIMIT = int(_fc_eval_limit_env)\n",
    "else:\n",
    "    FC_EVAL_LIMIT = 10 if FC_EVAL_MODE == 'quick' else int(FC_TEST_SIZE)\n",
    "\n",
    "FC_EVAL_MAX_NEW_TOKENS = int(os.environ.get('FC_EVAL_MAX_NEW_TOKENS', '256'))\n",
    "FC_EVAL_LOG_EVERY = int(os.environ.get('FC_EVAL_LOG_EVERY', '200'))\n",
    "FC_EVAL_BATCH_SIZE = int(os.environ.get('FC_EVAL_BATCH_SIZE', '4'))\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def _resolve_fc_sft_run_dir():\n",
    "    run_dir_hint = os.environ.get('FC_SFT_TRAIN_DIR', '').strip()\n",
    "    if run_dir_hint:\n",
    "        p = Path(run_dir_hint)\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    results_dir = Path(FC_RESULTS_DIR)\n",
    "    if not results_dir.exists():\n",
    "        return None\n",
    "\n",
    "    candidates = []\n",
    "    prefixes = []\n",
    "    if os.environ.get('FC_SFT_RUN_NAME'):\n",
    "        prefixes.append(os.environ.get('FC_SFT_RUN_NAME'))\n",
    "    if FC_RUN_NAME:\n",
    "        prefixes.append(FC_RUN_NAME)\n",
    "    for prefix in prefixes:\n",
    "        if prefix:\n",
    "            candidates.extend([p for p in results_dir.glob(f\"{prefix}*\") if p.is_dir()])\n",
    "\n",
    "    if not candidates:\n",
    "        for pat in ('fc-sft-*', 'sft-*'):\n",
    "            candidates.extend([p for p in results_dir.glob(pat) if p.is_dir()])\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    def _score(p: Path):\n",
    "        has_nemo = (p / 'nemo_experiments').exists()\n",
    "        return (1 if has_nemo else 0, p.stat().st_mtime)\n",
    "\n",
    "    return max(candidates, key=_score)\n",
    "\n",
    "\n",
    "def _to_pod_path(local_path: Path) -> str:\n",
    "    try:\n",
    "        local = local_path.resolve()\n",
    "    except Exception:\n",
    "        local = local_path\n",
    "    fsx_root = Path(FSX_BASE_DIR).resolve()\n",
    "    try:\n",
    "        rel = local.relative_to(fsx_root)\n",
    "        return f\"{POD_FSX_ROOT}/{rel.as_posix()}\"\n",
    "    except Exception:\n",
    "        return str(local).replace(str(fsx_root), POD_FSX_ROOT, 1)\n",
    "\n",
    "\n",
    "_sft_run_dir = _resolve_fc_sft_run_dir()\n",
    "if _sft_run_dir:\n",
    "    os.environ['FC_SFT_TRAIN_DIR'] = str(_sft_run_dir)\n",
    "    os.environ['POD_FC_SFT_TRAIN_DIR'] = _to_pod_path(_sft_run_dir)\n",
    "    print('Resolved SFT run dir:', _sft_run_dir)\n",
    "else:\n",
    "    print('SFT run dir not found yet; will resolve later if needed.')\n",
    "\n",
    "FC_EVAL_SCRIPT = FC_EVAL_DIR / 'run_fc_eval.py'\n",
    "FC_EVAL_SCRIPT.write_text('import json, os\\nfrom pathlib import Path\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\n\\n\\ndef _find_adapter_dir(root: Path):\\n    for p in root.rglob(\\'adapter_config.json\\'):\\n        return p.parent\\n    return None\\n\\n\\ndef _find_merged_dir(root: Path):\\n    for p in root.rglob(\\'config.json\\'):\\n        parent = p.parent\\n        if any(parent.glob(\\'*.safetensors\\')) or (parent / \\'pytorch_model.bin\\').exists():\\n            return parent\\n    return None\\n\\n\\ndef _find_dist_ckpt_dir(root: Path):\\n    candidates = []\\n    for p in root.rglob(\\'checkpoints\\'):\\n        if not p.is_dir():\\n            continue\\n        for ckpt in p.rglob(\\'*\\'):\\n            if not ckpt.is_dir():\\n                continue\\n            if (ckpt / \\'weights\\').is_dir() and (ckpt / \\'context\\').is_dir():\\n                candidates.append(ckpt)\\n    if not candidates:\\n        return None\\n    return max(candidates, key=lambda p: p.stat().st_mtime)\\n\\n\\ndef _maybe_export_nemo_to_hf(root: Path):\\n    export_dir = root / \\'hf_export\\'\\n    if (export_dir / \\'config.json\\').exists():\\n        return export_dir\\n\\n    # Prefer .nemo if present\\n    nemo_ckpts = list(root.rglob(\\'*.nemo\\'))\\n    ckpt = max(nemo_ckpts, key=lambda p: p.stat().st_mtime) if nemo_ckpts else None\\n    if ckpt is None:\\n        ckpt = _find_dist_ckpt_dir(root)\\n\\n    if ckpt is None:\\n        return None\\n\\n    try:\\n        from nemo.collections import llm\\n        llm.export_ckpt(path=ckpt, target=\\'hf\\', output_path=export_dir)\\n    except Exception as e:\\n        print(f\\'WARNING: NeMo export failed: {e}\\')\\n        return None\\n    return export_dir if (export_dir / \\'config.json\\').exists() else None\\n\\n\\ndef _load_model(base_model, training_dir, require_sft=False):\\n    adapter_dir = None\\n    merged_dir = None\\n    used = \\'base\\'\\n\\n    if training_dir:\\n        root = Path(training_dir)\\n        if root.exists():\\n            adapter_dir = _find_adapter_dir(root)\\n            if adapter_dir is None:\\n                merged_dir = _find_merged_dir(root)\\n            if adapter_dir is None and merged_dir is None:\\n                export_dir = _maybe_export_nemo_to_hf(root)\\n                if export_dir is not None:\\n                    merged_dir = export_dir\\n\\n    if adapter_dir:\\n        from peft import PeftModel\\n        model = AutoModelForCausalLM.from_pretrained(base_model, torch_dtype=torch.bfloat16, device_map=\\'auto\\')\\n        model = PeftModel.from_pretrained(model, str(adapter_dir))\\n        tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\\n        used = \\'adapter\\'\\n    elif merged_dir:\\n        model = AutoModelForCausalLM.from_pretrained(str(merged_dir), torch_dtype=torch.bfloat16, device_map=\\'auto\\')\\n        tokenizer = AutoTokenizer.from_pretrained(str(merged_dir), use_fast=True)\\n        used = \\'merged\\'\\n    else:\\n        if require_sft:\\n            raise RuntimeError(\\n                \\'SFT eval requested but no fine-tuned weights were found. \\'\\n                \\'Export HF weights (hf_export/) or provide TRAINING_DIR with adapters/merged weights.\\'\\n            )\\n        model = AutoModelForCausalLM.from_pretrained(base_model, torch_dtype=torch.bfloat16, device_map=\\'auto\\')\\n        tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\\n\\n    if tokenizer.pad_token is None:\\n        tokenizer.pad_token = tokenizer.eos_token\\n    tokenizer.padding_side = \\'right\\'\\n    model.eval()\\n    return model, tokenizer, used\\n\\n\\ndef _apply_chat_template(tokenizer, messages, add_generation_prompt=True):\\n    if hasattr(tokenizer, \\'apply_chat_template\\'):\\n        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\\n    parts = []\\n    for msg in messages:\\n        role = msg.get(\\'role\\', \\'user\\')\\n        prefix = \\'User: \\' if role == \\'user\\' else \\'Assistant: \\'\\n        parts.append(prefix + msg.get(\\'content\\', \\'\\'))\\n    if add_generation_prompt:\\n        parts.append(\\'Assistant: \\')\\n    return \\'\\n\\n\\'.join(parts)\\n\\n\\ndef _safe_json_loads(text):\\n    try:\\n        return json.loads(text)\\n    except Exception:\\n        pass\\n    for start_char in [\\'[\\', \\'{\\']:\\n        start = text.find(start_char)\\n        if start == -1:\\n            continue\\n        for end_char in [\\']\\', \\'}\\']:\\n            end = text.rfind(end_char)\\n            if end > start:\\n                frag = text[start:end+1]\\n                try:\\n                    return json.loads(frag)\\n                except Exception:\\n                    continue\\n    return None\\n\\n\\ndef _get_name_and_args(obj):\\n    if isinstance(obj, list) and obj:\\n        obj = obj[0]\\n    if not isinstance(obj, dict):\\n        return None, None\\n    return obj.get(\\'name\\'), obj.get(\\'arguments\\')\\n\\n\\ndef evaluate(data_path, base_model, training_dir, out_path, max_new_tokens, limit, err_path=None, log_every=100, batch_size=4, require_sft=False):\\n    print(f\"Loading model: base_model={base_model} training_dir={training_dir}\")\\n    model, tokenizer, used = _load_model(base_model, training_dir, require_sft=require_sft)\\n    print(f\"Model loaded (source={used})\")\\n\\n    total = None\\n    if log_every and log_every > 0:\\n        try:\\n            with open(data_path, \\'r\\') as _f:\\n                total = sum(1 for _ in _f)\\n        except Exception as e:\\n            print(f\"WARNING: Could not count eval lines: {e}\")\\n        if limit and total is not None:\\n            total = min(total, limit)\\n        elif limit and total is None:\\n            total = limit\\n    print(f\"Starting eval: limit={limit} total={total} batch_size={batch_size}\")\\n\\n    results = {\"format_acc\": [], \"function_acc\": [], \"argument_acc\": [], \"exact_match\": [], \"hallucination\": []}\\n    errors = []\\n\\n    def _score_one(query, tools, answers, gen):\\n        expected = _safe_json_loads(str(answers))\\n        pred = _safe_json_loads(gen)\\n\\n        if pred is None:\\n            results[\\'format_acc\\'].append(0)\\n            results[\\'function_acc\\'].append(0)\\n            results[\\'argument_acc\\'].append(0)\\n            results[\\'exact_match\\'].append(0)\\n            results[\\'hallucination\\'].append(0)\\n            errors.append({\\'query\\': query, \\'error\\': \\'invalid_json\\', \\'output\\': gen})\\n            return\\n\\n        results[\\'format_acc\\'].append(1)\\n        pred_name, pred_args = _get_name_and_args(pred)\\n        exp_name, exp_args = _get_name_and_args(expected)\\n\\n        try:\\n            tool_names = [t.get(\\'name\\') for t in json.loads(tools)]\\n        except Exception:\\n            tool_names = []\\n\\n        is_hallucination = pred_name not in tool_names if tool_names else False\\n        results[\\'hallucination\\'].append(1 if is_hallucination else 0)\\n\\n        fn_correct = pred_name == exp_name\\n        results[\\'function_acc\\'].append(1 if fn_correct else 0)\\n\\n        args_correct = pred_args == exp_args\\n        results[\\'argument_acc\\'].append(1 if args_correct else 0)\\n\\n        results[\\'exact_match\\'].append(1 if (fn_correct and args_correct) else 0)\\n\\n        if not fn_correct or not args_correct:\\n            errors.append({\\n                \\'query\\': query,\\n                \\'expected\\': expected,\\n                \\'pred\\': pred,\\n                \\'output\\': gen,\\n            })\\n\\n    processed = 0\\n    batch_prompts = []\\n    batch_meta = []\\n\\n    with open(data_path, \\'r\\') as f:\\n        for idx, line in enumerate(f):\\n            if limit and idx >= limit:\\n                break\\n            ex = json.loads(line)\\n            tools = ex.get(\\'tools\\', \\'\\')\\n            query = ex.get(\\'query\\', \\'\\')\\n            answers = ex.get(\\'answers\\', \\'\\')\\n\\n            system = (\\n                \"You are a helpful assistant with access to the following functions. \"\\n                \"Use them if required.\\n\\n\" + str(tools)\\n            )\\n            messages = [\\n                {\\'role\\': \\'system\\', \\'content\\': system},\\n                {\\'role\\': \\'user\\', \\'content\\': str(query)},\\n            ]\\n            prompt = _apply_chat_template(tokenizer, messages, add_generation_prompt=True)\\n\\n            batch_prompts.append(prompt)\\n            batch_meta.append((query, tools, answers))\\n\\n            if len(batch_prompts) >= max(1, batch_size):\\n                enc = tokenizer(batch_prompts, return_tensors=\\'pt\\', padding=True)\\n                input_ids = enc[\\'input_ids\\'].to(model.device)\\n                attention_mask = enc[\\'attention_mask\\'].to(model.device)\\n                with torch.no_grad():\\n                    out = model.generate(\\n                        input_ids=input_ids,\\n                        attention_mask=attention_mask,\\n                        max_new_tokens=max_new_tokens,\\n                        do_sample=False,\\n                        temperature=0.0,\\n                    )\\n                for i in range(len(batch_prompts)):\\n                    prompt_len = int(attention_mask[i].sum().item())\\n                    gen = tokenizer.decode(out[i][prompt_len:], skip_special_tokens=True).strip()\\n                    q, t, a = batch_meta[i]\\n                    _score_one(q, t, a, gen)\\n                processed += len(batch_prompts)\\n                if log_every and log_every > 0 and processed % log_every == 0:\\n                    if total is not None:\\n                        print(f\"Processed {processed}/{total}\")\\n                    else:\\n                        print(f\"Processed {processed}\")\\n                batch_prompts = []\\n                batch_meta = []\\n\\n    if batch_prompts:\\n        enc = tokenizer(batch_prompts, return_tensors=\\'pt\\', padding=True)\\n        input_ids = enc[\\'input_ids\\'].to(model.device)\\n        attention_mask = enc[\\'attention_mask\\'].to(model.device)\\n        with torch.no_grad():\\n            out = model.generate(\\n                input_ids=input_ids,\\n                attention_mask=attention_mask,\\n                max_new_tokens=max_new_tokens,\\n                do_sample=False,\\n                temperature=0.0,\\n            )\\n        for i in range(len(batch_prompts)):\\n            prompt_len = int(attention_mask[i].sum().item())\\n            gen = tokenizer.decode(out[i][prompt_len:], skip_special_tokens=True).strip()\\n            q, t, a = batch_meta[i]\\n            _score_one(q, t, a, gen)\\n        processed += len(batch_prompts)\\n\\n    summary = {k: (sum(v) / len(v) if v else 0.0) for k, v in results.items()}\\n    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\\n    with open(out_path, \\'w\\') as f:\\n        json.dump(summary, f, indent=2)\\n\\n    if err_path:\\n        with open(err_path, \\'w\\') as f:\\n            json.dump(errors[:200], f, indent=2)\\n\\n    print(\\'Wrote metrics to\\', out_path)\\n    if err_path:\\n        print(\\'Wrote errors to\\', err_path)\\n\\n\\nif __name__ == \\'__main__\\':\\n    data_path = os.environ.get(\\'EVAL_DATA_PATH\\')\\n    base_model = os.environ.get(\\'BASE_MODEL\\')\\n    training_dir = os.environ.get(\\'TRAINING_DIR\\', \\'\\').strip()\\n    out_path = os.environ.get(\\'OUTPUT_PATH\\')\\n    err_path = os.environ.get(\\'ERRORS_PATH\\', \\'\\')\\n    max_new_tokens = int(os.environ.get(\\'MAX_NEW_TOKENS\\', \\'256\\'))\\n    limit = int(os.environ.get(\\'EVAL_LIMIT\\', \\'0\\'))\\n    log_every = int(os.environ.get(\\'EVAL_LOG_EVERY\\', \\'100\\'))\\n    batch_size = int(os.environ.get(\\'EVAL_BATCH_SIZE\\', \\'4\\'))\\n    require_sft = str(os.environ.get(\\'REQUIRE_SFT\\', \\'false\\')).lower() in {\\'1\\',\\'true\\',\\'yes\\'}\\n\\n    if not data_path or not base_model or not out_path:\\n        raise ValueError(\\'Missing EVAL_DATA_PATH, BASE_MODEL, or OUTPUT_PATH\\')\\n\\n    evaluate(\\n        data_path,\\n        base_model,\\n        training_dir,\\n        out_path,\\n        max_new_tokens,\\n        limit,\\n        err_path or None,\\n        log_every=log_every,\\n        batch_size=batch_size,\\n        require_sft=require_sft,\\n    )\\n')\n",
    "\n",
    "print('Eval script:', FC_EVAL_SCRIPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d148be85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline eval (Qwen2.5-Coder-7B-Instruct)\n",
      "=========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully submitted HyperPodPytorchJob 'fc-eval-base-20260105-000949-d8a722'!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted baseline eval job: fc-eval-base-20260105-000949-d8a722\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8e: Submit baseline evaluation (optional, Qwen2.5-Coder-7B/14B)\n",
    "# =============================================================================\n",
    "\n",
    "if not RUN_FC_EVAL_BASELINE:\n",
    "    print('Skipping baseline eval (RUN_FC_EVAL_BASELINE=False).')\n",
    "else:\n",
    "    print_header(f'Baseline eval (Qwen2.5-Coder-{FC_QWEN_SIZE_LABEL}-Instruct)')\n",
    "\n",
    "    eval_name = f\"fc-eval-base-{RUN_ID}\"\n",
    "    eval_out = f\"{POD_FC_EVAL_DIR}/baseline-metrics-{RUN_ID}.json\"\n",
    "    eval_err = f\"{POD_FC_EVAL_DIR}/baseline-errors-{RUN_ID}.json\"\n",
    "    data_path = f\"{POD_FC_DATA_TEST_DIR}/test_raw.jsonl\"\n",
    "\n",
    "    cmd = f\"python {POD_FC_EVAL_DIR}/run_fc_eval.py\"\n",
    "    job = submit_simple_job(\n",
    "        name=eval_name,\n",
    "        image=NEMO_IMAGE,\n",
    "        command=cmd,\n",
    "        gpu='1',\n",
    "        labels={\n",
    "            'kueue.x-k8s.io/queue-name': KUEUE_QUEUE_NAME,\n",
    "            'kueue.x-k8s.io/priority-class': KUEUE_PRIORITY_CLASS_INFER,\n",
    "        } if HAS_TASK_GOV else None,\n",
    "        annotations={\n",
    "            f'kueue.x-k8s.io/podset-{KUEUE_TOPOLOGY_MODE_INFER}-topology': KUEUE_TOPOLOGY_LABEL_INFER,\n",
    "        } if HAS_TASK_GOV else None,\n",
    "        env=HF_ENV + [\n",
    "            {'name': 'BASE_MODEL', 'value': FC_BASE_MODEL_ID},\n",
    "            {'name': 'TRAINING_DIR', 'value': ''},\n",
    "            {'name': 'EVAL_DATA_PATH', 'value': data_path},\n",
    "            {'name': 'OUTPUT_PATH', 'value': eval_out},\n",
    "            {'name': 'ERRORS_PATH', 'value': eval_err},\n",
    "            {'name': 'EVAL_LIMIT', 'value': str(FC_EVAL_LIMIT)},\n",
    "            {'name': 'MAX_NEW_TOKENS', 'value': str(FC_EVAL_MAX_NEW_TOKENS)},\n",
    "            {'name': 'EVAL_LOG_EVERY', 'value': str(FC_EVAL_LOG_EVERY)},\n",
    "            {'name': 'EVAL_BATCH_SIZE', 'value': str(FC_EVAL_BATCH_SIZE)},\n",
    "        ],\n",
    "    )\n",
    "    os.environ['FC_EVAL_BASE_JOB_NAME'] = eval_name\n",
    "    os.environ['FC_EVAL_BASE_OUTPUT_PATH'] = eval_out\n",
    "    os.environ['FC_BASE_METRICS_PATH'] = str(FC_EVAL_DIR / f'baseline-metrics-{RUN_ID}.json')\n",
    "    os.environ['FC_BASE_ERRORS_PATH'] = str(FC_EVAL_DIR / f'baseline-errors-{RUN_ID}.json')\n",
    "    print('Submitted baseline eval job:', eval_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8e83537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully submitted HyperPodPytorchJob 'fc-eval-sft-20260105-000949-d8a722'!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SFT eval (function-calling)\n",
      "===========================\n",
      "Submitted SFT eval job: fc-eval-sft-20260105-000949-d8a722\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8f: Submit SFT evaluation (optional)\n",
    "# =============================================================================\n",
    "\n",
    "if not RUN_FC_EVAL_SFT:\n",
    "    print('Skipping SFT eval (RUN_FC_EVAL_SFT=False).')\n",
    "else:\n",
    "    print_header('SFT eval (function-calling)')\n",
    "\n",
    "    eval_name = f\"fc-eval-sft-{RUN_ID}\"\n",
    "    eval_out = f\"{POD_FC_EVAL_DIR}/sft-metrics-{RUN_ID}.json\"\n",
    "    eval_err = f\"{POD_FC_EVAL_DIR}/sft-errors-{RUN_ID}.json\"\n",
    "    data_path = f\"{POD_FC_DATA_TEST_DIR}/test_raw.jsonl\"\n",
    "\n",
    "    train_dir = os.environ.get('POD_FC_SFT_TRAIN_DIR', '').strip()\n",
    "    if not train_dir:\n",
    "        local_dir = _resolve_fc_sft_run_dir()\n",
    "        if local_dir:\n",
    "            train_dir = _to_pod_path(local_dir)\n",
    "            os.environ['POD_FC_SFT_TRAIN_DIR'] = train_dir\n",
    "            os.environ['FC_SFT_TRAIN_DIR'] = str(local_dir)\n",
    "\n",
    "    cmd = f\"python {POD_FC_EVAL_DIR}/run_fc_eval.py\"\n",
    "    job = submit_simple_job(\n",
    "        name=eval_name,\n",
    "        image=NEMO_IMAGE,\n",
    "        command=cmd,\n",
    "        gpu='1',\n",
    "        labels={\n",
    "            'kueue.x-k8s.io/queue-name': KUEUE_QUEUE_NAME,\n",
    "            'kueue.x-k8s.io/priority-class': KUEUE_PRIORITY_CLASS_INFER,\n",
    "        } if HAS_TASK_GOV else None,\n",
    "        annotations={\n",
    "            f'kueue.x-k8s.io/podset-{KUEUE_TOPOLOGY_MODE_INFER}-topology': KUEUE_TOPOLOGY_LABEL_INFER,\n",
    "        } if HAS_TASK_GOV else None,\n",
    "        env=HF_ENV + [\n",
    "            {'name': 'BASE_MODEL', 'value': FC_BASE_MODEL_ID},\n",
    "            {'name': 'TRAINING_DIR', 'value': train_dir},\n",
    "            {'name': 'EVAL_DATA_PATH', 'value': data_path},\n",
    "            {'name': 'OUTPUT_PATH', 'value': eval_out},\n",
    "            {'name': 'ERRORS_PATH', 'value': eval_err},\n",
    "            {'name': 'EVAL_LIMIT', 'value': str(FC_EVAL_LIMIT)},\n",
    "            {'name': 'MAX_NEW_TOKENS', 'value': str(FC_EVAL_MAX_NEW_TOKENS)},\n",
    "            {'name': 'EVAL_LOG_EVERY', 'value': str(FC_EVAL_LOG_EVERY)},\n",
    "            {'name': 'EVAL_BATCH_SIZE', 'value': str(FC_EVAL_BATCH_SIZE)},\n",
    "            {'name': 'REQUIRE_SFT', 'value': 'true'},\n",
    "        ],\n",
    "    )\n",
    "    os.environ['FC_EVAL_SFT_JOB_NAME'] = eval_name\n",
    "    os.environ['FC_EVAL_SFT_OUTPUT_PATH'] = eval_out\n",
    "    os.environ['FC_SFT_METRICS_PATH'] = str(FC_EVAL_DIR / f'sft-metrics-{RUN_ID}.json')\n",
    "    os.environ['FC_SFT_ERRORS_PATH'] = str(FC_EVAL_DIR / f'sft-errors-{RUN_ID}.json')\n",
    "    os.environ['FC_ERRORS_PATH'] = os.environ['FC_SFT_ERRORS_PATH']\n",
    "    print('Submitted SFT eval job:', eval_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e669123",
   "metadata": {},
   "source": [
    "## Section 8g \u2014 Checkpoint + Training Curves + Eval Summary\n",
    "\n",
    "These cells surface the remaining Stage\u20111 deliverables:\n",
    "- checkpoint location and key files\n",
    "- training curves (train/val loss)\n",
    "- evaluation table (baseline vs SFT)\n",
    "- error analysis buckets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0c25b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 8 checkpoint discovery\n",
      "==============================\n",
      "Run dir: /fsx/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/results/fc-sft-20260105-000949-d8a722\n",
      "No checkpoint files found yet. Job may still be running.\n",
      "TensorBoard event files: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8g.1: Locate checkpoint + logs\n",
    "# =============================================================================\n",
    "\n",
    "print_header('Section 8 checkpoint discovery')\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "run_dir = _resolve_fc_sft_run_dir()\n",
    "if run_dir:\n",
    "    os.environ['FC_SFT_TRAIN_DIR'] = str(run_dir)\n",
    "    os.environ['POD_FC_SFT_TRAIN_DIR'] = _to_pod_path(run_dir)\n",
    "\n",
    "if not run_dir or not run_dir.exists():\n",
    "    print('No SFT run directory found yet under:', FC_RESULTS_DIR)\n",
    "else:\n",
    "    print('Run dir:', run_dir)\n",
    "\n",
    "    # HF export/adapter/merged checkpoints\n",
    "    hf_export = run_dir / 'hf_export'\n",
    "    if hf_export.exists() and (hf_export / 'config.json').exists():\n",
    "        print('HF export dir:', hf_export)\n",
    "\n",
    "    patterns = ['adapter_config.json', 'config.json', '*.safetensors', 'pytorch_model.bin']\n",
    "    found = []\n",
    "    for pat in patterns:\n",
    "        found.extend(run_dir.rglob(pat))\n",
    "    if not found:\n",
    "        print('No HF/adapter checkpoint files found yet.')\n",
    "    else:\n",
    "        for p in sorted(found)[:20]:\n",
    "            print('  ', p)\n",
    "        if len(found) > 20:\n",
    "            print('  ...')\n",
    "\n",
    "    # NeMo distributed checkpoints\n",
    "    ckpt_dirs = []\n",
    "    for p in run_dir.rglob('checkpoints'):\n",
    "        if not p.is_dir():\n",
    "            continue\n",
    "        for ckpt in p.rglob('*'):\n",
    "            if ckpt.is_dir() and (ckpt / 'weights').is_dir() and (ckpt / 'context').is_dir():\n",
    "                ckpt_dirs.append(ckpt)\n",
    "    if ckpt_dirs:\n",
    "        ckpt_dirs = sorted(ckpt_dirs, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        print('NeMo dist checkpoint dirs:', len(ckpt_dirs))\n",
    "        for p in ckpt_dirs[:5]:\n",
    "            print('  ', p)\n",
    "        if len(ckpt_dirs) > 5:\n",
    "            print('  ...')\n",
    "    else:\n",
    "        print('No NeMo dist checkpoints found yet.')\n",
    "\n",
    "    # TensorBoard logs\n",
    "    tb_files = list(run_dir.rglob('events.out.tfevents*'))\n",
    "    print('TensorBoard event files:', len(tb_files))\n",
    "    if tb_files:\n",
    "        for p in tb_files[:5]:\n",
    "            print('  ', p)\n",
    "        if len(tb_files) > 5:\n",
    "            print('  ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ce3212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 8 training curves\n",
      "=========================\n",
      "No TensorBoard event files found in: /fsx/fs-03b1953d09801303c/smus-nemo-smoke/section8_fc/results/fc-sft-20260105-000949-d8a722\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8g.2: Plot training curves (train/val loss)\n",
    "# =============================================================================\n",
    "\n",
    "RUN_FC_PLOT_LOSS = bool(str(os.environ.get('RUN_FC_PLOT_LOSS', 'true')).lower() in {'1','true','yes'})\n",
    "if not RUN_FC_PLOT_LOSS:\n",
    "    print('Skipping loss plot (RUN_FC_PLOT_LOSS=False).')\n",
    "else:\n",
    "    print_header('Section 8 training curves')\n",
    "\n",
    "    from pathlib import Path\n",
    "    from collections import defaultdict\n",
    "\n",
    "    try:\n",
    "        from tensorboard.backend.event_processing import event_accumulator\n",
    "    except Exception:\n",
    "        print('Installing tensorboard...')\n",
    "        run('python -m pip install -q tensorboard')\n",
    "        from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "    run_dir = _resolve_fc_sft_run_dir()\n",
    "    if not run_dir or not run_dir.exists():\n",
    "        print('No run directory found for plots.')\n",
    "    else:\n",
    "        tb_files = list(run_dir.rglob('events.out.tfevents*'))\n",
    "        if not tb_files:\n",
    "            print('No TensorBoard event files found in:', run_dir)\n",
    "        else:\n",
    "            series = defaultdict(list)\n",
    "            for f in tb_files:\n",
    "                ea = event_accumulator.EventAccumulator(str(f))\n",
    "                try:\n",
    "                    ea.Reload()\n",
    "                except Exception:\n",
    "                    continue\n",
    "                tags = ea.Tags().get('scalars', [])\n",
    "                for tag in tags:\n",
    "                    if 'loss' in tag.lower():\n",
    "                        for ev in ea.Scalars(tag):\n",
    "                            series[tag].append((ev.step, ev.value))\n",
    "\n",
    "            if not series:\n",
    "                print('No loss scalars found. Available scalar tags:')\n",
    "                try:\n",
    "                    tags = ea.Tags().get('scalars', [])\n",
    "                    print(tags)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            else:\n",
    "                import matplotlib.pyplot as plt\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                for tag, pts in series.items():\n",
    "                    pts = sorted(pts, key=lambda x: x[0])\n",
    "                    xs = [p[0] for p in pts]\n",
    "                    ys = [p[1] for p in pts]\n",
    "                    plt.plot(xs, ys, label=tag)\n",
    "                plt.xlabel('step')\n",
    "                plt.ylabel('loss')\n",
    "                plt.title('Training/Validation Loss')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52b8e0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 8 eval summary\n",
      "======================\n",
      "Baseline metrics file: (none)\n",
      "SFT metrics file: (none)\n",
      "Missing metrics files. Run Section 8e/8f to generate them.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8g.3: Evaluation summary table (baseline vs SFT)\n",
    "# =============================================================================\n",
    "\n",
    "print_header('Section 8 eval summary')\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Resolve latest metrics files if not explicitly set\n",
    "base_metrics = os.environ.get('FC_BASE_METRICS_PATH', '').strip()\n",
    "sft_metrics = os.environ.get('FC_SFT_METRICS_PATH', '').strip()\n",
    "\n",
    "if not base_metrics:\n",
    "    candidates = sorted(Path(FC_EVAL_DIR).glob('baseline-metrics-*.json'), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    base_metrics = str(candidates[0]) if candidates else ''\n",
    "if not sft_metrics:\n",
    "    candidates = sorted(Path(FC_EVAL_DIR).glob('sft-metrics-*.json'), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    sft_metrics = str(candidates[0]) if candidates else ''\n",
    "\n",
    "print('Baseline metrics file:', base_metrics or '(none)')\n",
    "print('SFT metrics file:', sft_metrics or '(none)')\n",
    "\n",
    "if not base_metrics or not sft_metrics:\n",
    "    print('Missing metrics files. Run Section 8e/8f to generate them.')\n",
    "else:\n",
    "    with open(base_metrics, 'r') as f:\n",
    "        base = json.load(f)\n",
    "    with open(sft_metrics, 'r') as f:\n",
    "        sft = json.load(f)\n",
    "\n",
    "    metrics = sorted(set(base.keys()) | set(sft.keys()))\n",
    "    rows = []\n",
    "    for m in metrics:\n",
    "        b = base.get(m, 0.0)\n",
    "        s = sft.get(m, 0.0)\n",
    "        rows.append((m, b, s, s - b))\n",
    "\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(rows, columns=['metric', 'baseline', 'sft', 'delta'])\n",
    "        display(df)\n",
    "    except Exception:\n",
    "        for m, b, s, d in rows:\n",
    "            print(f\"{m:16s} baseline={b:.4f} sft={s:.4f} delta={d:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "960c3bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 8 error analysis\n",
      "========================\n",
      "Errors file: (none)\n",
      "No error file found. Run Section 8e/8f first.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 8g.4: Error analysis buckets\n",
    "# =============================================================================\n",
    "\n",
    "print_header('Section 8 error analysis')\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "err_path = os.environ.get('FC_ERRORS_PATH', '').strip()\n",
    "if not err_path:\n",
    "    # Prefer SFT errors if available\n",
    "    sft_errs = sorted(Path(FC_EVAL_DIR).glob('sft-errors-*.json'), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    base_errs = sorted(Path(FC_EVAL_DIR).glob('baseline-errors-*.json'), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    err_path = str(sft_errs[0]) if sft_errs else (str(base_errs[0]) if base_errs else '')\n",
    "\n",
    "print('Errors file:', err_path or '(none)')\n",
    "\n",
    "if not err_path:\n",
    "    print('No error file found. Run Section 8e/8f first.')\n",
    "else:\n",
    "    # Build a lookup from query -> tools/answers\n",
    "    lookup = {}\n",
    "    test_raw = FC_DATA_TEST_DIR / 'test_raw.jsonl'\n",
    "    if test_raw.exists():\n",
    "        with open(test_raw, 'r') as f:\n",
    "            for line in f:\n",
    "                ex = json.loads(line)\n",
    "                q = ex.get('query')\n",
    "                if q not in lookup:\n",
    "                    lookup[q] = ex\n",
    "    else:\n",
    "        print('Missing test_raw.jsonl at', test_raw)\n",
    "\n",
    "    def _safe_json_loads(text):\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _get_name_and_args(obj):\n",
    "        if isinstance(obj, list) and obj:\n",
    "            obj = obj[0]\n",
    "        if not isinstance(obj, dict):\n",
    "            return None, None\n",
    "        return obj.get('name'), obj.get('arguments')\n",
    "\n",
    "    with open(err_path, 'r') as f:\n",
    "        errs = json.load(f)\n",
    "\n",
    "    buckets = {\n",
    "        'invalid_json': [],\n",
    "        'hallucination': [],\n",
    "        'wrong_function': [],\n",
    "        'wrong_arguments': [],\n",
    "        'other': [],\n",
    "    }\n",
    "\n",
    "    for ex in errs:\n",
    "        if ex.get('error') == 'invalid_json':\n",
    "            buckets['invalid_json'].append(ex)\n",
    "            continue\n",
    "\n",
    "        pred = ex.get('pred')\n",
    "        expected = ex.get('expected')\n",
    "        if pred is None and ex.get('output'):\n",
    "            pred = _safe_json_loads(ex['output'])\n",
    "        if expected is None and ex.get('answers'):\n",
    "            expected = _safe_json_loads(ex['answers'])\n",
    "\n",
    "        pred_name, pred_args = _get_name_and_args(pred)\n",
    "        exp_name, exp_args = _get_name_and_args(expected)\n",
    "\n",
    "        tools = []\n",
    "        if ex.get('query') in lookup:\n",
    "            tools_raw = lookup[ex['query']].get('tools')\n",
    "            try:\n",
    "                tools = json.loads(tools_raw)\n",
    "            except Exception:\n",
    "                tools = []\n",
    "\n",
    "        tool_names = [t.get('name') for t in tools] if tools else []\n",
    "\n",
    "        if tool_names and pred_name not in tool_names:\n",
    "            buckets['hallucination'].append(ex)\n",
    "        elif pred_name != exp_name:\n",
    "            buckets['wrong_function'].append(ex)\n",
    "        elif pred_args != exp_args:\n",
    "            buckets['wrong_arguments'].append(ex)\n",
    "        else:\n",
    "            buckets['other'].append(ex)\n",
    "\n",
    "    print('Bucket counts:')\n",
    "    for k, v in buckets.items():\n",
    "        print(f\"  {k:16s} {len(v)}\")\n",
    "\n",
    "    # Show a few examples per bucket\n",
    "    for k, v in buckets.items():\n",
    "        if not v:\n",
    "            continue\n",
    "        print('\\n', k)\n",
    "        for ex in v[:3]:\n",
    "            print('  query:', ex.get('query'))\n",
    "            if ex.get('output'):\n",
    "                print('  output:', ex.get('output'))\n",
    "            if ex.get('pred'):\n",
    "                print('  pred:', ex.get('pred'))\n",
    "            if ex.get('expected'):\n",
    "                print('  expected:', ex.get('expected'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea590de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}